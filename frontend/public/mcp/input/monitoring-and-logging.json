{
  "category": "monitoring-and-logging",
  "categoryDisplay": "Monitoring and Logging",
  "description": "",
  "totalRepositories": 51,
  "repositories": {
    "0xPratikPatil--NmapMCP": {
      "owner": "0xPratikPatil",
      "name": "NmapMCP",
      "url": "https://github.com/0xPratikPatil/NmapMCP",
      "imageUrl": "/freedevtools/mcp/pfp/0xPratikPatil.webp",
      "description": "NmapMCP is a tool that allows users to perform various types of network scans, helping to identify open ports and discover associated subdomains. It is integrated with the Model Context Protocol for easy use in different applications.",
      "stars": 4,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-11T02:35:31Z",
      "readme_content": "# NmapMCP \n[![smithery badge](https://smithery.ai/badge/@0xPratikPatil/nmapmcp)](https://smithery.ai/server/@0xPratikPatil/nmapmcp)\nNmapMCP  is a robust integration of the Nmap scanning tool with the Model Context Protocol (MCP), enabling seamless network scanning capabilities within MCP-compatible environments. This project allows users to perform various network scans, such as top ports scanning, DNS brute force, and more, directly through MCP interfaces.\n\n## Features\n\n-   **Top Ports Scanning:** Quickly identify the most commonly used ports on target hosts to assess potential entry points.\n    \n-   **DNS Brute Force:** Discover subdomains associated with a target domain, aiding in comprehensive domain mapping.\n    \n-   **List Scan:** Obtain a list of active hosts within a specified range without port scanning, useful for network inventory.\n    \n-   **OS Detection:** Determine the operating system of a target host by analyzing network responses, assisting in vulnerability assessment.\n    \n-   **Version Detection:** Identify service versions running on open ports to detect outdated or vulnerable services.\n    \n-   **FIN Scan:** Perform stealthy scans by sending FIN packets to detect open ports without establishing a full connection.\n    \n-   **Idle Scan:** Conduct highly stealthy scans by leveraging idle hosts to probe target systems, minimizing detection risks.\n    \n-   **Ping Scan:** Detect active hosts in a network by sending ICMP echo requests, useful for network mapping.\n    \n-   **SYN Scan:** Perform half-open TCP scans to identify open ports without completing the TCP handshake, reducing detection likelihood.\n    \n-   **TCP Connect Scan:** Establish full TCP connections to probe open ports, useful when SYN scans are not feasible.\n    \n-   **UDP Scan:** Identify open UDP ports on a target host to detect services that do not use TCP.\n    \n-   **Port Scan Only:** Focus solely on scanning ports without additional host discovery, streamlining the scanning process.\n    \n-   **No Port Scan:** Perform host discovery without scanning ports, useful for identifying live hosts without probing services.\n    \n-   **ARP Discovery:** Identify active devices within a local network segment using ARP requests, effective in LAN environments.\n    \n-   **Disable DNS Resolution:** Perform scans without resolving IP addresses to hostnames, enhancing scan speed and reducing DNS query traffic.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Nmap Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@0xPratikPatil/nmapmcp):\n\n```bash\nnpx -y @smithery/cli install @0xPratikPatil/nmapmcp --client claude\n```\n\n### Manual Installation\n1.  **Clone the Repository:**\n    \n    ```bash\n    git clone https://github.com/0xPratikPatil/NmapMCP.git\n    cd NmapMCP\n    ```\n2.  **Install `uv`:**\n    \n    ```bash\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    ```    \n\n3. **Create environment:**\n\t```bash\n\tuv venv\n\t```\n5.  **Install dependencies from `pyproject.toml`**\n    \n    ```bash\n    uv pip install\n    ```\n    or\n    ```bash\n    uv pip install -r pyproject.toml\n    ```\n\n## Configuration\n\nTo configure the Nmap MCP Server, edit the `claude_desktop_config.json` file located in the project root. This file allows you to set default scan arguments, define MCP tool behaviors, and adjust logging settings.\n\n**Example `claude_desktop_config.json`:**\n\n```json\n{\n  \"mcpServers\": {\n    \"NmapMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/NmapMCP\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\n## Contributing\n\nContributions are welcome! To contribute:\n\n1.  **Fork the Repository:** Click the \"Fork\" button at the top right of the repository page.\n    \n2.  **Clone Your Fork:**\n    \n    ```bash\n    git clone https://github.com/0xPratikPatil/NmapMCP.git\n    ```\n    \n3.  **Create a New Branch:**\n    \n    ```bash\n    git checkout -b feature/your-feature-name\n    ```\n4.  **Make Your Changes:** Implement your feature or fix.\n    \n5.  **Run Tests:** Ensure all tests pass.\n    \n6.  **Commit Changes:**\n    \n    ```bash\n    git commit -m \"Add feature: your feature name\"\n    ```\n7.  **Push to Your Fork:**\n    \n    ```bash\n    git push origin feature/your-feature-name\n    ```\n8.  **Submit a Pull Request:** Navigate to the original repository and click \"New Pull Request.\"\n    \n\n## License\n\nThis project is licensed under the MIT License.\n\n## Acknowledgments\n\nSpecial thanks to the Nmap and MCP communities for their invaluable tools and support.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nmapmcp",
        "scans",
        "monitoring",
        "nmapmcp tool",
        "nmapmcp nmapmcp",
        "0xpratikpatil nmapmcp"
      ],
      "category": "monitoring-and-logging"
    },
    "7gugu--whistle-mcp": {
      "owner": "7gugu",
      "name": "whistle-mcp",
      "url": "https://github.com/7gugu/whistle-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/7gugu.webp",
      "description": "The Whistle MCP Server is a tool that allows users to manage their Whistle proxy servers using AI. It simplifies tasks such as network debugging, API testing, and proxy rule management through easy, natural language interactions.",
      "stars": 20,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-17T09:43:13Z",
      "readme_content": "# Whistle MCP Server\n\nEnglish | [中文](README_CN.md)\n[![smithery badge](https://smithery.ai/badge/@7gugu/whistle-mcp)](https://smithery.ai/server/@7gugu/whistle-mcp)\n\n## Project Introduction\n\nWhistle MCP Server is a Whistle proxy management tool based on the Model Context Protocol (MCP), allowing AI assistants to directly operate and control local Whistle proxy servers. Through this tool, AI can help users manage rules, groups, values, monitor network requests, replay and modify requests, etc., without requiring manual operation of the Whistle interface. It greatly simplifies the process of network debugging, API testing, and proxy rule management, enabling users to complete complex network proxy configuration tasks through natural language interaction with AI.\n\n## Features\n\n- **Rule Management**: Create, update, rename, delete, and enable/disable Whistle rules\n- **Group Management**: Create, rename, delete groups, and associate operations between rules and groups\n- **Value Management**: Create, update, rename, and delete values, with support for value group management\n- **Proxy Control**: Enable/disable proxy, HTTP/HTTPS interception, HTTP/2 protocol, etc.\n- **Request Interception**: View intercepted network request information, with URL filtering support\n- **Request Replay**: Support for replaying captured requests with custom request parameters\n- **Multi-Rule Mode**: Support for enabling/disabling multi-rule mode\n\n## Installation\n\n### Installing via Smithery\n\nTo install Whistle MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@7gugu/whistle-mcp):\n\n```bash\nnpx -y @smithery/cli install @7gugu/whistle-mcp --client claude\n```\n\n### Manual Installation\nYou can install Whistle MCP Server globally via npm:\n\n```bash\nnpm install -g whistle-mcp-tool\n```\n\n## MCP Configuration\n\nAfter installation, you can configure Whistle MCP in your MCP JSON configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"whistle-mcp\": {\n      \"command\": \"whistle-mcp\",\n      \"args\": [\n        \"--host=<whistle server IP address>\",\n        \"--port=<whistle server port number>\"\n      ]\n    }\n  }\n}\n```\n\n### Configuration Details\n\n- host: Whistle server IP address, defaults to localhost if not configured\n- port: Whistle server port number, defaults to 8899 if not configured\n\n## Configuring MCP JSON in AI Clients\n\n- Claude Client: [https://modelcontextprotocol.io/quickstart/user](https://modelcontextprotocol.io/quickstart/user)\n- Raycast: Requires MCP plugin installation\n- Cursor: [https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\n## MCP Tools Description\n\nWhistle MCP Server provides the following tools, which can be called via the MCP protocol:\n\n### Rule Management\n\n| Tool Name | Description | Function |\n| ------- | --- | ---- |\n| getRules | Get all rules | List all created rules and their content |\n| createRule | Create new rule | Create a new rule with the specified name |\n| updateRule | Update rule content | Modify the content of a specified rule |\n| renameRule | Rename rule | Rename a rule to a new name |\n| deleteRule | Delete rule | Delete a rule with the specified name |\n| selectRule | Enable rule | Enable a rule with the specified name |\n| unselectRule | Disable rule | Disable a rule with the specified name |\n| disableAllRules | Disable all rules | Disable all created rules at once |\n\n### Group Management\n\n| Tool Name | Description | Function |\n| ------- | --- | ---- |\n| createGroup | Create group | Create a new rule group with the specified name |\n| renameGroup | Rename group | Rename a rule group to a new name |\n| deleteGroup | Delete group | Delete a rule group with the specified name |\n| moveRuleToGroup | Move rule to group | Move a specified rule to a specific group |\n| moveRuleOutOfGroup | Move rule out of group | Move a rule out of its group to the top level |\n\n### Value Management\n\n| Tool Name | Description | Function |\n| ------- | --- | ---- |\n| getAllValues | Get all values | List all created values and value groups |\n| createValue | Create new value | Create a new value with the specified name |\n| updateValue | Update value content | Modify the content of a specified value |\n| renameValue | Rename value | Rename a value to a new name |\n| deleteValue | Delete value | Delete a value with the specified name |\n| createValueGroup | Create value group | Create a new value group with the specified name |\n| renameValueGroup | Rename value group | Rename a value group to a new name |\n| deleteValueGroup | Delete value group | Delete a value group with the specified name |\n| moveValueToGroup | Move value to group | Move a specified value to a specific group |\n| moveValueOutOfGroup | Move value out of group | Move a value out of its group to the top level |\n\n### Proxy Control\n\n| Tool Name | Description | Function |\n| ------- | --- | ---- |\n| getStatus | Get server status | Get the current status information of the Whistle server |\n| toggleProxy | Enable/disable proxy | Toggle the enabled state of the Whistle proxy |\n| toggleHttpsInterception | Enable/disable HTTPS interception | Toggle the enabled state of HTTPS request interception |\n| toggleHttp2 | Enable/disable HTTP2 | Toggle the enabled state of HTTP/2 protocol support |\n| toggleMultiRuleMode | Enable/disable multi-rule mode | Toggle whether to allow multiple rules to be enabled simultaneously |\n\n### Request Management\n\n| Tool Name | Description | Function |\n| ------- | --- | ---- |\n| getInterceptInfo | Get interception information | Get network request information intercepted by Whistle, with filtering support |\n| replayRequest | Replay request | Resend a specified network request with customizable parameters |\n\n## Contact Information\n\n- Email: [gz7gugu@qq.com](mailto:gz7gugu@qq.com)\n- Blog: [https://7gugu.com](https://7gugu.com)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whistle",
        "logging",
        "monitoring",
        "whistle mcp",
        "mcp whistle",
        "whistle proxy"
      ],
      "category": "monitoring-and-logging"
    },
    "AVIMBU--uptime_agent_mcp": {
      "owner": "AVIMBU",
      "name": "uptime_agent_mcp",
      "url": "https://github.com/AVIMBU/uptime_agent_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/AVIMBU.webp",
      "description": "Connects Uptime Agent monitoring systems to AI assistants for real-time monitoring, incident management, and creation of monitoring tasks using natural language commands. Enables secure integration of uptime monitoring with AI workflows for improved operational insights.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-20T12:16:05Z",
      "readme_content": "# 🚀 Uptime Agent MCP Server\n\n[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](https://nodejs.org/)\n[![Model Context Protocol](https://img.shields.io/badge/MCP-Compliant-orange)](https://modelcontextprotocol.io/)\n[![smithery badge](https://smithery.ai/badge/@AVIMBU/uptime_agent_mcp)](https://smithery.ai/server/@AVIMBU/uptime_agent_mcp)\n\nConnect your [Uptime Agent](https://uptime-agent.io) monitoring system directly to AI assistants like Claude through the Model Context Protocol (MCP).\n\n## ✨ Features\n\n- **Real-time Monitoring Access**: Allow AI assistants to check your system's uptime status\n- **Incident Management**: View and analyze downtime incidents through natural conversation\n- **Monitor Creation**: Set up new monitoring endpoints with simple voice or text commands\n- **Secure Integration**: Enterprise-grade security for your monitoring infrastructure\n\n## 🔍 What is Uptime Agent?\n\n[Uptime Agent](https://uptime-agent.io) is a powerful monitoring solution that tracks your websites and API endpoints, alerting you when they go down. This MCP server extends Uptime Agent's capabilities by letting you interact with your monitoring system through AI assistants.\n\n## 🛠️ Installation\n\n### Prerequisites\n\n- Node.js 18 or higher\n- An active Uptime Agent account\n- Your Uptime Agent API key\n\nTo obtain your Uptime Agent API key:\n1. Log in to your [Uptime Agent Dashboard](https://uptime-agent.io/dashboard)\n2. Navigate to Account → API Keys\n3. Create a new API key with appropriate permissions\n4. Copy the generated key for use with the MCP server\n\n### Option 1: Quick Install via NPM (Recommended)\n\nThe fastest way to get started is with our setup command:\n\n```bash\nnpx uptime-agent-mcp setup\n```\n\nThis command will:\n- Install the MCP server\n- Configure it for use with Claude Desktop\n- Prompt you for your Uptime Agent API key\n- Set up all necessary configurations automatically\n\n### Option 2: Install via Smithery.ai\n\nTo install using Smithery.ai:\n\n1. Create an account at [smithery.ai](https://smithery.ai)\n2. Get your personal key from your Smithery account\n3. Run the following command:\n\n```bash\nnpx -y @smithery/cli@latest install @AVIMBU/uptime_agent_mcp --client claude --key <personal_key>\n```\n\nReplace `<personal_key>` with your actual Smithery personal key.\n\n### Option 3: Manual Local Installation\n\nFor advanced users who want more control:\n\n```bash\n# Clone the repository\ngit clone https://github.com/AVIMBU/uptime_agent_mcp.git\ncd uptime_agent_mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\nConfigure with your API key by creating a `.env` file:\n\n```\nUPTIME_API_KEY=your-api-key-here\nPORT=3000  # Optional, defaults to 3000\n```\n\nStart the server:\n\n```bash\nnpm start\n# or directly with\nnode dist/index.js\n```\n\n## 🤖 AI Assistant Integration\n\n### Setting Up with Claude Desktop\n\nAfter installing using one of the methods above, your MCP server is automatically configured for Claude Desktop.\n\nIf you installed manually, add the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"uptime-agent\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"uptime-agent-mcp\"\n      ],\n      \"env\": {\n        \"UPTIME_API_KEY\": \"<YOUR_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can use Docker:\n\n```json\n{\n  \"mcpServers\": {\n    \"uptime-agent\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"UPTIME_API_KEY\",\n        \"uptime-agent-mcp\"\n      ],\n      \"env\": {\n        \"UPTIME_API_KEY\": \"<YOUR_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n### Example Conversations\n\n**Checking Monitors:**\n> \"Claude, show me all my active uptime monitors.\"\n\n**Creating a New Monitor:**\n> \"Please create a new monitor for our API endpoint at https://api.mycompany.com/v2/health\"\n\n**Analyzing Incidents:**\n> \"What incidents happened on our production servers last week, and what was the average downtime?\"\n\n## 📊 Available Functions\n\n### Monitor Operations\n\n| Function | Description | Parameters |\n|----------|-------------|------------|\n| `listMonitors` | Get a complete list of all monitoring endpoints | None required |\n| `getMonitor` | Retrieve detailed information about a specific monitor | `id`: Monitor identifier |\n| `createMonitor` | Set up a new endpoint to monitor | `name`: Monitor name<br>`url`: URL to monitor<br>`tracking_type`: Type of monitoring (http, ping, etc.)<br>`check_frequency`: Check interval in seconds |\n\n### Incident Management\n\n| Function | Description | Parameters |\n|----------|-------------|------------|\n| `listIncidents` | View all detected downtime incidents | None required |\n| `getIncident` | Get detailed information about a specific incident | `id`: Incident identifier |\n| `listIncidentsByMonitor` | See all incidents for a particular endpoint | `monitor_id`: Monitor identifier |\n\n### Public Tracking\n\n| Function | Description | Parameters |\n|----------|-------------|------------|\n| `createAnonymousTracking` | Create public tracking without authentication | `url`: URL to monitor<br>`name`: (Optional) Name for the tracking |\n\n### Integration with Slack (Coming Soon)\n\n| Function | Description | Parameters |\n|----------|-------------|------------|\n| `slack_get_users` | List all users in connected Slack workspace | `limit`: Max number of users<br>`cursor`: Pagination cursor |\n| `slack_post_message` | Post notifications to Slack | `channel_id`: Channel to post to<br>`text`: Message content |\n\n## 🐳 Docker Deployment\n\nWe provide Docker support for easy deployment:\n\n```bash\n# Build the Docker image\ndocker build -t uptime-agent-mcp .\n\n# Run the container\ndocker run -p 3000:3000 -e UPTIME_API_KEY=your-api-key uptime-agent-mcp\n```\n\n## 📬 Support\n\nIf you have questions or need assistance:\n\n- [Open an issue](https://github.com/AVIMBU/uptime_agent_mcp/issues) on GitHub\n- Contact us through our website: [AVIMBU](https://avimbu.com)\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n---\n\n<p align=\"center\">Developed with ❤️ by <a href=\"https://avimbu.com\">AVIMBU</a></p>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "uptime_agent_mcp",
        "monitoring",
        "uptime",
        "avimbu uptime_agent_mcp",
        "uptime agent",
        "uptime monitoring"
      ],
      "category": "monitoring-and-logging"
    },
    "EdgeCloudX--ceph_exporter": {
      "owner": "EdgeCloudX",
      "name": "ceph_exporter",
      "url": "https://github.com/EdgeCloudX/ceph_exporter",
      "imageUrl": "/freedevtools/mcp/pfp/EdgeCloudX.webp",
      "description": "Collects and exposes detailed metrics from a Ceph cluster for Prometheus monitoring, providing real-time insights to optimize performance and reliability without requiring additional configuration.",
      "stars": 0,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-04-11T02:48:39Z",
      "readme_content": "# Ceph Exporter  [![GoDoc](https://godoc.org/github.com/digitalocean/ceph_exporter?status.svg)](https://godoc.org/github.com/digitalocean/ceph_exporter) ![build](https://github.com/digitalocean/ceph_exporter/actions/workflows/run_build.yml/badge.svg) ![tests](https://github.com/digitalocean/ceph_exporter/actions/workflows/run_tests.yml/badge.svg)  [![Go Report Card](https://goreportcard.com/badge/digitalocean/ceph_exporter)](https://goreportcard.com/report/digitalocean/ceph_exporter)\n\nA Prometheus exporter that scrapes meta information about a running Ceph\ncluster. All the information gathered from the cluster is done by interacting\nwith the monitors using an appropriate wrapper over\n`rados_mon_command()`. Hence, no additional setup is necessary other than\nhaving a working Ceph cluster.\n\nA List of all the metrics collected is available on [METRICS.md](./METRICS.md) page.\n\n## Dependencies\n\nYou should ideally run this exporter from the client that can talk to the Ceph\ncluster. Like any other Ceph client, it needs the following files to run\ncorrectly.\n\n* `ceph.conf` containing your Ceph configuration.\n* `ceph.client.<user>.keyring` in order to authenticate to your Ceph cluster.\n\nThe `ceph_exporter` will automatically pick those up if they are present in\nany of the [default\nlocations](http://docs.ceph.com/docs/master/rados/configuration/ceph-conf/#the-configuration-file). Otherwise\nyou will need to provide the configuration manually using environment\nvariables:\n\n* `CEPH_CLUSTER`: cluster's name (default `ceph`)\n* `CEPH_CONFIG`: configuration file that a Ceph client uses to connect to\n  the cluster (default `/etc/ceph/ceph.conf`)\n* `CEPH_USER`: a Ceph client user used to connect to the cluster (default\n  `admin`)\n\nWe use Ceph's [official Golang client](https://github.com/ceph/go-ceph) to run\ncommands on the cluster.\n\n`ceph_exporter` is currently in use and tested against Nautilus, Pacific, and Reef.\nIt might not work as expected with older or non-LTS versions of Ceph.\n\n## Environment Variables\n\n| Name                    | Description                                                                                    | Default                  |\n|-------------------------|------------------------------------------------------------------------------------------------|--------------------------|\n| `TELEMETRY_ADDR`        | Host:Port for ceph_exporter's metrics endpoint                                                 | `*:9128`                 |\n| `TELEMETRY_PATH`        | URL Path for surfacing metrics to Prometheus                                                   | `/metrics`               |\n| `EXPORTER_CONFIG`       | Path to ceph_exporter configuration file                                                       | `/etc/ceph/exporter.yml` |\n| `RGW_MODE`              | Enable collection of stats from RGW (0:disabled 1:enabled 2:background)                        | `0`                      |\n| `CEPH_CLUSTER`          | Ceph cluster name                                                                              | `ceph`                   |\n| `CEPH_CONFIG`           | Path to Ceph configuration file                                                                | `/etc/ceph/ceph.conf`    |\n| `CEPH_USER`             | Ceph user to connect to cluster                                                                | `admin`                  |\n| `CEPH_RADOS_OP_TIMEOUT` | Ceph rados_osd_op_timeout and rados_mon_op_timeout used to contact cluster (0s means no limit) | `30s`                    |\n| `LOG_LEVEL`             | Logging level. One of: [trace, debug, info, warn, error, fatal, panic]                         | `info`                   |\n| `TLS_CERT_FILE_PATH`    | Path to the x509 certificate file for enabling TLS (the key file path must also be specified)  |                          |\n| `TLS_KEY_FILE_PATH`     | Path to the x509 key file for enabling TLS (the cert file path must also be specified)         |                          |\n\n## Installation\n\nThe typical Go way of installing or building should work provided you have the [cgo dependencies](https://github.com/ceph/go-ceph#installation).\n\n```\n$ go install -tags nautilus\n```\n\n```\n$ go build -o ceph_exporter -tags nautilus\n```\n\nWe build the client with support for nautilus specifically but the binary will work for Octopus and Pacific as well.\n\n## Docker Image\n\n### Docker Hub\n\nThe official docker image is available at\n[digitalocean/ceph_exporter](https://hub.docker.com/r/digitalocean/ceph_exporter/).\n\n### Build From Source\n\nIt is also possible to build your own locally from the source. The port `9128`\nis exposed as a default port for `ceph_exporter`.\n\nThe exporter needs your Ceph configuration in order to establish communication\nwith the Ceph monitors. You can either pass it in as an additional command or\nmount the directory containing both your `ceph.conf` and your user's keyring\nunder the default `/etc/ceph` location that Ceph checks for.\n\nA sample build command would look like:\n\n```bash\n$ docker build -t digitalocean/ceph_exporter .\n```\n\nA `--build-args TEST=true` flag can be added to the build command above to\nalso run Golang's unit tests during build:\n\n```bash\ndocker build -t digitalocean/ceph_exporter . --build-arg TEST=true --no-cache\n```\n\nYou can start running your `ceph_exporter` container now.\n\n```bash\n$ docker run -v /etc/ceph:/etc/ceph -p=9128:9128 -it digitalocean/ceph_exporter\n```\n\nYou would have to ensure your image can talk over to the monitors. If it needs\naccess to your host's network stack you might need to add `--net=host` to the\nabove command. It makes the port mapping redundant so the `-p` flag can be\nremoved.\n\nPoint your Prometheus to scrape from `:9128` on your host now (or your port\nof choice if you decide to change it).\n\n## Contributing\n\nPlease refer to the [CONTRIBUTING](CONTRIBUTING.md) guide for more\ninformation on how to submit your changes to this repository.\n\n## Sample view\n\nSee `./examples` for a `docker-compose` file with Grafana if you'd like to\nquickly get a test environment up and running.\n\nLink to official documentation explaining `docker-compose`:\nhttps://docs.docker.com/compose/\n\nThe `docker-compose` file itself has comments on how to change it to adapt to\nyour environment. It does use volumes in order to persist data.  Docker\nvolumes documentation: https://docs.docker.com/engine/tutorials/dockervolumes/\n\nIf you have [promdash](https://github.com/prometheus/promdash) set up you\ncan generate views like:\n\n![](sample.png)\n\nCopyright @ 2016-2023 DigitalOcean™ Inc.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "edgecloudx",
        "ceph_exporter",
        "prometheus",
        "edgecloudx ceph_exporter",
        "prometheus monitoring",
        "cluster prometheus"
      ],
      "category": "monitoring-and-logging"
    },
    "GeLi2001--datadog-mcp-server": {
      "owner": "GeLi2001",
      "name": "datadog-mcp-server",
      "url": "https://github.com/GeLi2001/datadog-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/GeLi2001.webp",
      "description": "Interacts with the Datadog API to monitor systems, access monitor data, retrieve dashboards, query metrics, and manage incidents. Provides comprehensive error handling and advanced log searching capabilities.",
      "stars": 55,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-12T01:20:56Z",
      "readme_content": "# Datadog MCP Server\n\nA Model Context Protocol (MCP) server for interacting with the Datadog API.\n\n<a href=\"https://glama.ai/mcp/servers/@GeLi2001/datadog-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@GeLi2001/datadog-mcp-server/badge\" alt=\"Datadog MCP server\" />\n</a>\n\n## Features\n\n- **Monitoring**: Access monitor data and configurations\n- **Dashboards**: Retrieve and view dashboard definitions\n- **Metrics**: Query available metrics and their metadata\n- **Events**: Search and retrieve events within timeframes\n- **Logs**: Search logs with advanced filtering and sorting options\n- **Incidents**: Access incident management data\n- **API Integration**: Direct integration with Datadog's v1 and v2 APIs\n- **Comprehensive Error Handling**: Clear error messages for API and authentication issues\n- **Service-Specific Endpoints**: Support for different endpoints for logs and metrics\n\n## Prerequisites\n\n1. Node.js (version 16 or higher)\n2. Datadog account with:\n   - API key - Found in Organization Settings > API Keys\n   - Application key - Found in Organization Settings > Application Keys\n\n## Installation\n\n### Via npm (recommended)\n\n```bash\nnpm install -g datadog-mcp-server\n```\n\n### From Source\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Configuration\n\nYou can configure the Datadog MCP server using either environment variables or command-line arguments.\n\n### Environment Variables\n\nCreate a `.env` file with your Datadog credentials:\n\n```\nDD_API_KEY=your_api_key_here\nDD_APP_KEY=your_app_key_here\nDD_SITE=datadoghq.com\nDD_LOGS_SITE=datadoghq.com\nDD_METRICS_SITE=datadoghq.com\n```\n\n**Note**: `DD_LOGS_SITE` and `DD_METRICS_SITE` are optional and will default to the value of `DD_SITE` if not specified.\n\n### Command-line Arguments\n\nBasic usage with global site setting:\n\n```bash\ndatadog-mcp-server --apiKey=your_api_key --appKey=your_app_key --site=datadoghq.eu\n```\n\nAdvanced usage with service-specific endpoints:\n\n```bash\ndatadog-mcp-server --apiKey=your_api_key --appKey=your_app_key --site=datadoghq.com --logsSite=logs.datadoghq.com --metricsSite=metrics.datadoghq.com\n```\n\nNote: Site arguments don't need `https://` - it will be added automatically.\n\n### Regional Endpoints\n\nDifferent Datadog regions have different endpoints:\n\n- US (Default): `datadoghq.com`\n- EU: `datadoghq.eu`\n- US3 (GovCloud): `ddog-gov.com`\n- US5: `us5.datadoghq.com`\n- AP1: `ap1.datadoghq.com`\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"datadog\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"datadog-mcp-server\",\n        \"--apiKey\",\n        \"<YOUR_API_KEY>\",\n        \"--appKey\",\n        \"<YOUR_APP_KEY>\",\n        \"--site\",\n        \"<YOUR_DD_SITE>(e.g us5.datadoghq.com)\"\n      ]\n    }\n  }\n}\n```\n\nFor more advanced configurations with separate endpoints for logs and metrics:\n\n```json\n{\n  \"mcpServers\": {\n    \"datadog\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"datadog-mcp-server\",\n        \"--apiKey\",\n        \"<YOUR_API_KEY>\",\n        \"--appKey\",\n        \"<YOUR_APP_KEY>\",\n        \"--site\",\n        \"<YOUR_DD_SITE>\",\n        \"--logsSite\",\n        \"<YOUR_LOGS_SITE>\",\n        \"--metricsSite\",\n        \"<YOUR_METRICS_SITE>\"\n      ]\n    }\n  }\n}\n```\n\nLocations for the Claude Desktop config file:\n\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n## Usage with MCP Inspector\n\nTo use with the MCP Inspector tool:\n\n```bash\nnpx @modelcontextprotocol/inspector datadog-mcp-server --apiKey=your_api_key --appKey=your_app_key\n```\n\n## Available Tools\n\nThe server provides these MCP tools:\n\n- **get-monitors**: Fetch monitors with optional filtering\n- **get-monitor**: Get details of a specific monitor by ID\n- **get-dashboards**: List all dashboards\n- **get-dashboard**: Get a specific dashboard by ID\n- **get-metrics**: List available metrics\n- **get-metric-metadata**: Get metadata for a specific metric\n- **get-events**: Fetch events within a time range\n- **get-incidents**: List incidents with optional filtering\n- **search-logs**: Search logs with advanced query filtering\n- **aggregate-logs**: Perform analytics and aggregations on log data\n\n## Examples\n\n### Example: Get Monitors\n\n```javascript\n{\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"get-monitors\",\n    \"arguments\": {\n      \"groupStates\": [\"alert\", \"warn\"],\n      \"limit\": 5\n    }\n  }\n}\n```\n\n### Example: Get a Dashboard\n\n```javascript\n{\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"get-dashboard\",\n    \"arguments\": {\n      \"dashboardId\": \"abc-def-123\"\n    }\n  }\n}\n```\n\n### Example: Search Logs\n\n```javascript\n{\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"search-logs\",\n    \"arguments\": {\n      \"filter\": {\n        \"query\": \"service:web-app status:error\",\n        \"from\": \"now-15m\",\n        \"to\": \"now\"\n      },\n      \"sort\": \"-timestamp\",\n      \"limit\": 20\n    }\n  }\n}\n```\n\n### Example: Aggregate Logs\n\n```javascript\n{\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"aggregate-logs\",\n    \"arguments\": {\n      \"filter\": {\n        \"query\": \"service:web-app\",\n        \"from\": \"now-1h\",\n        \"to\": \"now\"\n      },\n      \"compute\": [\n        {\n          \"aggregation\": \"count\"\n        }\n      ],\n      \"groupBy\": [\n        {\n          \"facet\": \"status\",\n          \"limit\": 10,\n          \"sort\": {\n            \"aggregation\": \"count\",\n            \"order\": \"desc\"\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### Example: Get Incidents\n\n```javascript\n{\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"get-incidents\",\n    \"arguments\": {\n      \"includeArchived\": false,\n      \"query\": \"state:active\",\n      \"pageSize\": 10\n    }\n  }\n}\n```\n\n## Troubleshooting\n\nIf you encounter a 403 Forbidden error, verify that:\n\n1. Your API key and Application key are correct\n2. The keys have the necessary permissions to access the requested resources\n3. Your account has access to the requested data\n4. You're using the correct endpoint for your region (e.g., `datadoghq.eu` for EU customers)\n\n## Debugging\n\nIf you encounter issues, check Claude Desktop's MCP logs:\n\n```bash\n# On macOS\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n\n# On Windows\nGet-Content -Path \"$env:APPDATA\\Claude\\Logs\\mcp*.log\" -Tail 20 -Wait\n```\n\nCommon issues:\n\n- 403 Forbidden: Authentication issue with Datadog API keys\n- API key or App key format invalid: Ensure you're using the full key strings\n- Site configuration errors: Make sure you're using the correct Datadog domain\n- Endpoint mismatches: Verify that service-specific endpoints are correctly set if you're using separate domains for logs and metrics\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datadog",
        "logging",
        "monitoring",
        "datadog mcp",
        "datadog api",
        "interacts datadog"
      ],
      "category": "monitoring-and-logging"
    },
    "GetSherlog--mcp-grafana": {
      "owner": "GetSherlog",
      "name": "mcp-grafana",
      "url": "https://github.com/GetSherlog/mcp-grafana",
      "imageUrl": "/freedevtools/mcp/pfp/GetSherlog.webp",
      "description": "Provides access to Grafana instances and their ecosystem, enabling dashboard search, datasource querying, and incident management. Integrates Prometheus and Loki functionalities, including log and metric queries, along with alert rules and metadata fetching.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-21T06:20:19Z",
      "readme_content": "# Grafana MCP server\n\nA [Model Context Protocol][mcp] (MCP) server for Grafana.\n\nThis provides access to your Grafana instance and the surrounding ecosystem.\n\n## Features\n\n- [x] Search for dashboards\n- [x] Get dashboard by UID\n- [x] List and fetch datasource information\n- [ ] Query datasources\n  - [x] Prometheus\n  - [x] Loki\n    - [x] Log queries\n    - [x] Metric queries\n  - [ ] Tempo\n  - [ ] Pyroscope\n- [x] Query Prometheus metadata\n  - [x] Metric metadata\n  - [x] Metric names\n  - [x] Label names\n  - [x] Label values\n- [x] Query Loki metadata\n  - [x] Label names\n  - [x] Label values\n  - [x] Stats\n- [x] Search, create, update and close incidents\n- [ ] Start Sift investigations and view the results\n- [ ] Alerting\n  - [x] List and fetch alert rule information\n  - [x] Get alert rule statuses (firing/normal/error/etc.)\n  - [ ] Create and change alert rules\n  - [x] List contact points\n  - [ ] Create and change contact points\n- [x] Access Grafana OnCall functionality\n  - [x] List and manage schedules\n  - [x] Get shift details\n  - [x] Get current on-call users\n  - [x] List teams and users\n  - [ ] List alert groups\n\nThe list of tools is configurable, so you can choose which tools you want to make available to the MCP client.\nThis is useful if you don't use certain functionality or if you don't want to take up too much of the context window.\nTo disable a category of tools, use the `--disable-<category>` flag when starting the server. For example, to disable\nthe OnCall tools, use `--disable-oncall`.\n\n### Tools\n\n| Tool                              | Category    | Description                                                        |\n|-----------------------------------|-------------|--------------------------------------------------------------------|\n| `search_dashboards`               | Search      | Search for dashboards                                              |\n| `get_dashboard_by_uid`            | Dashboard   | Get a dashboard by uid                                             |\n| `list_datasources`                | Datasources | List datasources                                                   |\n| `get_datasource_by_uid`           | Datasources | Get a datasource by uid                                            |\n| `get_datasource_by_name`          | Datasources | Get a datasource by name                                           |\n| `query_prometheus`                | Prometheus  | Execute a query against a Prometheus datasource                    |\n| `list_prometheus_metric_metadata` | Prometheus  | List metric metadata                                               |\n| `list_prometheus_metric_names`    | Prometheus  | List available metric names                                        |\n| `list_prometheus_label_names`     | Prometheus  | List label names matching a selector                               |\n| `list_prometheus_label_values`    | Prometheus  | List values for a specific label                                   |\n| `list_incidents`                  | Incident    | List incidents in Grafana Incident                                 |\n| `create_incident`                 | Incident    | Create an incident in Grafana Incident                             |\n| `add_activity_to_incident`        | Incident    | Add an activity item to an incident in Grafana Incident            |\n| `resolve_incident`                | Incident    | Resolve an incident in Grafana Incident                            |\n| `query_loki_logs`                 | Loki        | Query and retrieve logs using LogQL (either log or metric queries) |\n| `list_loki_label_names`           | Loki        | List all available label names in logs                             |\n| `list_loki_label_values`          | Loki        | List values for a specific log label                               |\n| `query_loki_stats`                | Loki        | Get statistics about log streams                                   |\n| `list_alert_rules`                | Alerting    | List alert rules                                                   |\n| `get_alert_rule_by_uid`           | Alerting    | Get alert rule by UID                                              |\n| `list_oncall_schedules`           | OnCall      | List schedules from Grafana OnCall                                 |\n| `get_oncall_shift`                | OnCall      | Get details for a specific OnCall shift                           |\n| `get_current_oncall_users`        | OnCall      | Get users currently on-call for a specific schedule                |\n| `list_oncall_teams`               | OnCall      | List teams from Grafana OnCall                                     |\n| `list_oncall_users`               | OnCall      | List users from Grafana OnCall                                     |\n\n## Usage\n\n1. Create a service account in Grafana with enough permissions to use the tools you want to use,\n   generate a service account token, and copy it to the clipboard for use in the configuration file.\n   Follow the [Grafana documentation][service-account] for details.\n\n2. Download the latest release of `mcp-grafana` from the [releases page](https://github.com/grafana/mcp-grafana/releases) and place it in your `$PATH`.\n\n   If you have a Go toolchain installed you can also build and install it from source, using the `GOBIN` environment variable\n   to specify the directory where the binary should be installed. This should also be in your `PATH`.\n\n   ```bash\n   GOBIN=\"$HOME/go/bin\" go install github.com/grafana/mcp-grafana/cmd/mcp-grafana@latest\n   ```\n\n3. Add the server configuration to your client configuration file. For example, for Claude Desktop:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"grafana\": {\n         \"command\": \"mcp-grafana\",\n         \"args\": [],\n         \"env\": {\n           \"GRAFANA_URL\": \"http://localhost:3000\",\n           \"GRAFANA_API_KEY\": \"<your service account token>\"\n         }\n       }\n     }\n   }\n   ```\n\n> Note: if you see `Error: spawn mcp-grafana ENOENT` in Claude Desktop, you need to specify the full path to `mcp-grafana`.\n\n### Debug Mode\n\nYou can enable debug mode for the Grafana transport by adding the `-debug` flag to the command. This will provide detailed logging of HTTP requests and responses between the MCP server and the Grafana API, which can be helpful for troubleshooting.\n\nTo use debug mode with the Claude Desktop configuration, update your config as follows:\n\n```json\n{\n  \"mcpServers\": {\n    \"grafana\": {\n      \"command\": \"mcp-grafana\",\n      \"args\": [\"-debug\"],\n      \"env\": {\n        \"GRAFANA_URL\": \"http://localhost:3000\",\n        \"GRAFANA_API_KEY\": \"<your service account token>\"\n      }\n    }\n  }\n}\n```\n\n## Development\n\nContributions are welcome! Please open an issue or submit a pull request if you have any suggestions or improvements.\n\nThis project is written in Go. Install Go following the instructions for your platform.\n\nTo run the server, use:\n\n```bash\nmake run\n```\n\nYou can also run the server using the SSE transport inside Docker. To build the image, use\n\n```\nmake build-image\n```\n\nAnd to run the image, use:\n\n```\ndocker run -it --rm -p 8000:8000 mcp-grafana:latest\n```\n\n### Testing\n\nThere are three types of tests available:\n\n1. Unit Tests (no external dependencies required):\n```bash\nmake test-unit\n```\n\nYou can also run unit tests with:\n```bash\nmake test\n```\n\n2. Integration Tests (requires docker containers to be up and running):\n```bash\nmake test-integration\n```\n\n3. Cloud Tests (requires cloud Grafana instance and credentials):\n```bash\nmake test-cloud\n```\n> Note: Cloud tests are automatically configured in CI. For local development, you'll need to set up your own Grafana Cloud instance and credentials.\n\nMore comprehensive integration tests will require a Grafana instance to be running locally on port 3000; you can start one with Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\nThe integration tests can be run with:\n\n```bash\nmake test-all\n```\n\nIf you're adding more tools, please add integration tests for them. The existing tests should be a good starting point.\n\n### Linting\n\nTo lint the code, run:\n\n```bash\nmake lint\n```\n\nThis includes a custom linter that checks for unescaped commas in `jsonschema` struct tags. The commas in `description` fields must be escaped with `\\\\,` to prevent silent truncation. You can run just this linter with:\n\n```bash\nmake lint-jsonschema\n```\n\nSee the [JSONSchema Linter documentation](internal/linter/jsonschema/README.md) for more details.\n\n## License\n\nThis project is licensed under the [Apache License, Version 2.0](LICENSE).\n\n[mcp]: https://modelcontextprotocol.io/\n[service-account]: https://grafana.com/docs/grafana/latest/administration/service-accounts/\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "grafana",
        "logging",
        "monitoring",
        "grafana instances",
        "grafana provides",
        "access grafana"
      ],
      "category": "monitoring-and-logging"
    },
    "Heht571--ops-mcp-server": {
      "owner": "Heht571",
      "name": "ops-mcp-server",
      "url": "https://github.com/Heht571/ops-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Heht571.webp",
      "description": "A server toolset for monitoring and managing remote servers, providing functionalities for system checks, service status updates, network diagnostics, and security audits. It includes features for memory information retrieval, system load monitoring, and process management.",
      "stars": 38,
      "forks": 13,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T10:03:18Z",
      "readme_content": "\n---\n\n# ops-mcp-server\n\n[![中文](https://img.shields.io/badge/Language-中文-blue.svg)](README_zh.md)\n\n`ops-mcp-server`: an AI-driven IT operations platform that fuses LLMs and MCP architecture to enable intelligent monitoring, anomaly detection, and natural human-infrastructure interaction with enterprise-grade security and scalability.\n\n---\n\n## 📖 Table of Contents\n\n- [Project Overview](#project-overview)\n- [Key Features](#key-features)\n- [Demo Videos](#demo-videos)\n- [Installation](#installation)\n- [Deployment](#deployment)\n- [Local MCP Server Configuration](#local-mcp-server-configuration)\n- [Interactive Client Usage](#interactive-client-usage)\n- [License](#license)\n- [Notes](#notes)\n\n---\n\n## 🚀 Project Overview\n\n`ops-mcp-server` is an IT operations management solution for the AI era. It achieves intelligent IT operations through the seamless integration of the Model Context Protocol (MCP) and Large Language Models (LLMs). By leveraging the power of LLMs and MCP's distributed architecture, it transforms traditional IT operations into an AI-driven experience, enabling automated server monitoring, intelligent anomaly detection, and context-aware troubleshooting. The system acts as a bridge between human operators and complex IT infrastructure, providing natural language interaction for tasks ranging from routine maintenance to complex problem diagnosis, while maintaining enterprise-grade security and scalability.\n\n---\n\n## 🌟 Key Features\n\n### 🖥️ Server Monitoring\n\n- Real-time CPU, memory, disk inspections.\n- System load and process monitoring.\n- Service and network interface checks.\n- Log analysis and configuration backup.\n- Security vulnerability scans (SSH login, firewall status).\n- Detailed OS information retrieval.\n\n### 📦 Container Management (Docker)\n\n- Container, image, and volume management.\n- Container resource usage monitoring.\n- Log retrieval and health checks.\n\n### 🌐 Network Device Management\n\n- Multi-vendor support (Cisco, Huawei, H3C).\n- Switch port, VLAN, and router route checks.\n- ACL security configuration analysis.\n- Optical module and device performance monitoring.\n\n### ➕ Additional Capabilities\n\n- Extensible plugin architecture.\n- Batch operations across multiple devices.\n- Tool listing and descriptive commands.\n\n---\n\n## 🎬 Demo Videos\n\n### 📌 Project Demo\n\n_On Cherry Studio_\n\n![Demo Animation](assets/demo.gif)\n\n### 📌 Interactive Client Demo\n\n_On Terminal_\n\n![Client Demo Animation](assets/client.gif)\n\n---\n\n## ⚙️ Installation\n\nEnsure you have **Python 3.10+** installed. This project uses [`uv`](https://github.com/astral-sh/uv) for dependency and environment management.\n\n### 1. Install UV\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n### 2. Set Up Virtual Environment\n\n```bash\nuv venv .venv\n\n# Activate the environment\nsource .venv/bin/activate      # Linux/macOS\n.\\.venv\\Scripts\\activate       # Windows\n```\n\n### 3. Install Dependencies\n\n```bash\nuv pip install -r requirements.txt\n```\n\n> Dependencies are managed via `pyproject.toml`.\n\n---\n\n## 🚧 Deployment\n\n### 📡 SSE Remote Deployment (UV)\n\n```bash\ncd server_monitor_sse\n\n# Install dependencies\npip install -r requirements.txt\n\n# Start service\ncd ..\nuv run server_monitor_sse --transport sse --port 8000\n```\n\n### 🐳 SSE Remote Deployment (Docker Compose)\n\nEnsure Docker and Docker Compose are installed.\n\n```bash\ncd server_monitor_sse\ndocker compose up -d\n\n# Check status\ndocker compose ps\n\n# Logs monitoring\ndocker compose logs -f\n```\n\n---\n\n## 🛠️ Local MCP Server Configuration (Stdio)\n\nAdd this configuration to your MCP settings:\n\n```json\n{\n  \"ops-mcp-server\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\", \"YOUR_PROJECT_PATH_HERE\",\n      \"run\", \"server_monitor.py\"\n    ],\n    \"env\": {},\n    \"disabled\": true,\n    \"autoApprove\": [\"list_available_tools\"]\n  },\n  \"network_tools\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\", \"YOUR_PROJECT_PATH_HERE\",\n      \"run\", \"network_tools.py\"\n    ],\n    \"env\": {},\n    \"disabled\": false,\n    \"autoApprove\": []\n  },\n}\n```\n\n> **Note**: Replace `YOUR_PROJECT_PATH_HERE` with your project's actual path.\n\n---\n\n## 💬 Interactive Client Usage\n\nAn interactive client (`client.py`) allows you to interact with MCP services using natural language.\n\n### 1. Install Client Dependencies\n\n```bash\nuv pip install openai rich\n```\n\n### 2. Configure Client\n\nEdit these configurations within `client.py`:\n\n```python\n# Initialize OpenAI client\nself.client = AsyncOpenAI(\n    base_url=\"https://your-api-endpoint\",\n    api_key=\"YOUR_API_KEY\"\n)\n\n# Set model\nself.model = \"your-preferred-model\"\n```\n\n### 3. Run the Client\n\n```bash\nuv run client.py [path/to/server.py]\n```\n\nExample:\n\n```bash\nuv run client.py ./server_monitor.py\n```\n\n### Client Commands\n\n- `help` - Display help.\n- `quit` - Exit client.\n- `clear` - Clear conversation history.\n- `model <name>` - Switch models.\n\n---\n\n## 📄 License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n---\n\n## 📌 Notes\n\n- Ensure remote SSH access is properly configured.\n- Adjust tool parameters based on actual deployment conditions.\n- This project is under active development; feedback and contributions are welcome.\n\n---",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "servers",
        "logging",
        "server toolset",
        "mcp server",
        "toolset monitoring"
      ],
      "category": "monitoring-and-logging"
    },
    "Ivlad003--mcp_newrelic": {
      "owner": "Ivlad003",
      "name": "mcp_newrelic",
      "url": "https://github.com/Ivlad003/mcp_newrelic",
      "imageUrl": "/freedevtools/mcp/pfp/Ivlad003.webp",
      "description": "Query New Relic logs and metrics using NRQL queries for insights and monitoring. Provides detailed error logging and outputs that are formatted for readability.",
      "stars": 26,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T13:48:00Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ivlad003-mcp-newrelic-badge.png)](https://mseep.ai/app/ivlad003-mcp-newrelic)\n\n# New Relic MCP Server\n\nA simple Model Context Protocol (MCP) server for querying New Relic logs using NRQL queries. This server enables Large Language Models (LLMs) like Claude to interact with your New Relic data.\n\n## Features\n\n- Query New Relic logs and metrics using NRQL\n- Detailed error logging\n- Easy integration with Claude Desktop\n- Human-readable output formatting\n- Configurable New Relic account ID\n\n## Setup Instructions\n\n### Prerequisites\n\n- Python 3.10 or higher\n- New Relic account and API key\n- Claude Desktop application\n\n### Installation Steps\n\n1. Install `uv` package manager:\n```bash\n# On macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows (PowerShell)\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n2. Create and setup project:\n```bash\n# Create directory\nmkdir newrelic-mcp\ncd newrelic-mcp\n\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # On Unix/macOS\n.venv\\Scripts\\activate     # On Windows\n\n# Install dependencies\nuv pip install \"mcp[cli]\" httpx\n```\n\n3. Create server file `newrelic_logs_server.py` with the provided code.\n\n4. Configure your environment variables:\n```bash\n# On Unix/macOS\nexport NEW_RELIC_API_KEY=\"your-api-key-here\"\nexport NEW_RELIC_ACCOUNT_ID=\"your-account-id-here\"\n\n# On Windows (CMD)\nset NEW_RELIC_API_KEY=your-api-key-here\nset NEW_RELIC_ACCOUNT_ID=your-account-id-here\n\n# On Windows (PowerShell)\n$env:NEW_RELIC_API_KEY = \"your-api-key-here\"\n$env:NEW_RELIC_ACCOUNT_ID = \"your-account-id-here\"\n```\n\n### Claude Desktop Integration\n\nConfigure Claude Desktop by editing your configuration file:\n\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd the following configuration:\n```json\n{\n    \"mcpServers\": {\n        \"newrelic\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/absolute/path/to/newrelic-mcp\",\n                \"run\",\n                \"newrelic_logs_server.py\"\n            ],\n            \"env\": {\n                \"NEW_RELIC_API_KEY\": \"your-api-key-here\",\n                \"NEW_RELIC_ACCOUNT_ID\": \"your-account-id-here\"\n            }\n        }\n    }\n}\n```\n\n## Usage\n\n### Example NRQL Queries\n\n1. Basic Transaction Query:\n```sql\nSELECT * FROM Transaction SINCE 1 hour ago\n```\n\n2. Error Analysis:\n```sql\nSELECT * FROM Transaction WHERE error IS TRUE SINCE 1 hour ago LIMIT 10\n```\n\n3. Performance Analysis:\n```sql\nSELECT average(duration) FROM Transaction FACET name ORDER BY average(duration) DESC LIMIT 5\n```\n\n### Example Claude Prompts\n\nYou can ask Claude questions like:\n- \"Show me all transactions from the last hour\"\n- \"Are there any errors in our application?\"\n- \"What are our slowest endpoints?\"\n\n## Debugging\n\n### Viewing Logs\n\n```bash\n# On macOS/Linux\ntail -f ~/Library/Logs/Claude/mcp-server-newrelic.log\n\n# On Windows\ntype %APPDATA%\\Claude\\logs\\mcp-server-newrelic.log\n```\n\n### Testing with MCP Inspector\n\nTest your server functionality using:\n```bash\nnpx @modelcontextprotocol/inspector uv run newrelic_logs_server.py\n```\n\n### Common Issues\n\n1. Authentication Errors:\n- Check if NEW_RELIC_API_KEY is set correctly\n- Verify API key has correct permissions\n- Ensure API key is valid\n\n2. Query Errors:\n- Verify NRQL syntax\n- Check account ID in code matches your account\n- Ensure queried data exists in the time range\n\n3. Connection Issues:\n- Check network connectivity\n- Verify GraphQL endpoint is accessible\n- Ensure no firewalls are blocking connections\n\n## Security Notes\n\n- Never commit API keys to version control\n- Use environment variables for sensitive data\n- Keep dependencies updated\n- Monitor query patterns and access logs\n\n## Development\n\n### Local Testing\n\n1. Set environment variables:\n```bash\nexport NEW_RELIC_API_KEY=\"your-api-key-here\"\nexport NEW_RELIC_ACCOUNT_ID=\"your-account-id-here\"\n```\n\n2. Run the server:\n```bash\nuv run newrelic_logs_server.py\n```\n\n### Code Structure\n\nThe server implements:\n- Single NRQL query tool\n- Configurable New Relic account ID\n- Comprehensive error handling\n- Detailed logging\n- Response formatting\n\n### Testing Changes\n\n1. Modify code as needed\n2. Test with MCP Inspector\n3. Restart Claude Desktop to apply changes\n\n## Troubleshooting Guide\n\n1. Server Not Starting:\n- Check Python version\n- Verify all dependencies are installed\n- Ensure virtual environment is activated\n\n2. Query Not Working:\n- Check logs for detailed error messages\n- Verify NRQL syntax\n- Ensure data exists in queried time range\n\n3. Claude Not Connecting:\n- Verify configuration file syntax\n- Check paths are absolute\n- Restart Claude Desktop\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Submit a pull request\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Support\n\nIf you encounter issues:\n1. Check the logs\n2. Review common issues section\n3. Test with MCP Inspector\n4. File an issue on GitHub",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nrql",
        "logging",
        "logs",
        "relic logs",
        "monitoring logging",
        "logs metrics"
      ],
      "category": "monitoring-and-logging"
    },
    "JackXuyi--env-mcp": {
      "owner": "JackXuyi",
      "name": "env-mcp",
      "url": "https://github.com/JackXuyi/env-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/JackXuyi.webp",
      "description": "Retrieve detailed system information such as platform, memory, CPU, network, and user data for use in applications. Supports cross-platform operation and is easily integratable with MCP-enabled tools.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-21T15:32:03Z",
      "readme_content": "# env-mcp\n\n一个用于获取当前环境系统信息的 MCP 工具包。\n\n## 功能特性\n\n- 获取详细的系统信息\n- 支持跨平台使用\n- 易于集成到支持 MCP 的应用程序中\n- 提供 TypeScript 类型支持\n\n## 安装\n\n```bash\nnpm install @zhijianren/env-mcp -g\n```\n\n## 在支持 MCP 的应用中使用\n\n### 1. 配置 MCP 服务\n\n在应用的 MCP 配置中添加以下内容：\n\n```json\n{\n  \"mcpServers\": {\n    \"env-mcp\": {\n      \"name\": \"env-mcp\",\n      \"type\": \"command\",\n      \"command\": \"node\",\n      \"args\": [\n        \"/usr/local/lib/node_modules/@zhijianren/env-mcp/dist/index.js\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n### 2. 调用服务\n\n```typescript\n// 获取平台信息\nconst platformInfo = await mcp.env.getPlatformInfo();\n\n// 获取内存信息\nconst memoryInfo = await mcp.env.getMemoryInfo();\n\n// 获取 CPU 信息\nconst cpuInfo = await mcp.env.getCpuInfo();\n\n// 获取网络信息\nconst networkInfo = await mcp.env.getNetworkInfo();\n\n// 获取用户信息\nconst userInfo = await mcp.env.getUserInfo();\n\n// 获取 CPU 使用率\nconst cpuUsage = await mcp.env.getCpuUsage();\n\n// 获取硬盘使用率\nconst diskUsage = await mcp.env.getDiskUsage();\n\n// 获取终端类型\nconst terminalTypes = await mcp.env.getTerminalTypes();\n\n// 获取 IPv4 信息\nconst ipv4Info = await mcp.env.getIpv4Info();\n\n// 获取 IPv6 信息\nconst ipv6Info = await mcp.env.getIpv6Info();\n\n// 获取代理信息\nconst proxyInfo = await mcp.env.getProxyInfo();\n\n// 获取 Docker 信息\nconst dockerInfo = await mcp.env.getDockerInfo();\n\n// 获取 Node.js 版本信息\nconst nodeInfo = await mcp.env.getNodeInfo();\n```\n\n### 支持的工具列表\n\n| 工具名称         | 描述                           | 返回数据结构示例                                                                 |\n|------------------|--------------------------------|----------------------------------------------------------------------------------|\n| `getBatteryInfo` | 获取当前设备的电池信息         | `{ hasBattery: boolean, isCharging: boolean, maxCapacity: number, currentCapacity: number }` |\n| `getGraphicsInfo` | 获取当前设备的显卡信息        | `{ controllers: Array<{ model: string, vendor: string, bus: string, vram: number }>, displays: Array<{ model: string, resolutionX: number, resolutionY: number }> }` |\n| `getProcesses`   | 获取当前设备的进程信息         | `{ all: number, running: number, blocked: number, sleeping: number, list: Array<{ pid: number, name: string, cpu: number, memory: number }> }` |\n| `getBluetoothInfo` | 获取当前设备的蓝牙信息       | `Array<{ name: string, address: string, connected: boolean }>`                   |\n| `getAudioInfo`   | 获取当前设备的音频设备信息     | `Array<{ name: string, manufacturer: string, default: boolean }>`                |\n| `getAvailableNetworks` | 获取当前设备可用的网络信息 | `{ networkInterfaces: { [key: string]: Array<{ address: string, netmask: string, family: string, internal: boolean }> }, wifiNetworks: Array<{ ssid: string, bssid: string, mode: string, channel: number, frequency: number, signalLevel: number, quality: number, security: string[] }> }` |\n| `getTimezone`    | 获取当前设备的时区信息         | `{ timezone: string }`                                                           |\n| `getAppSchemas`  | 获取当前设备所有注册唤醒的 App Schema 信息 | `{ [bundle: string]: string[] }`                                                 |\n| `getWifiInfo`    | 获取当前设备的 Wi-Fi 信息      | `Array<{ ssid: string, bssid: string, mode: string, channel: number, frequency: number, signalLevel: number, quality: number, security: string[] }>` |\n| `getInstalledApps` | 获取当前设备已安装的应用信息   | `{ installedApps: string[] }`                                                    |\n| `getVpnInfo`     | 获取当前设备的 VPN 信息        | `{ [key: string]: Array<{ address: string, netmask: string, family: string, internal: boolean }> }` |\n| `getHardwareInfo` | 获取当前设备的硬件信息，包括生产日期等 | `{ manufacturer: string, model: string, version: string, serial: string, uuid: string, sku: string, virtual: boolean }` |\n| `getPlatformInfo` | 获取当前系统的平台信息         | `{ platform: string, arch: string, hostname: string, type: string, release: string, version: string }` |\n| `getMemoryInfo`   | 获取当前系统的内存信息         | `{ totalMemory: number, freeMemory: number, usedMemory: number }`                |\n| `getCpuInfo`      | 获取当前系统的 CPU 信息        | `{ cpus: Array<{ model: string, speed: number, times: { user: number, nice: number, sys: number, idle: number, irq: number } }> }` |\n| `getNetworkInfo`  | 获取当前系统的网络信息         | `{ networkInterfaces: { [key: string]: Array<{ address: string, netmask: string, family: string, mac: string, internal: boolean }> } }` |\n| `getUserInfo`     | 获取当前系统的用户信息         | `{ userInfo: { uid: number, gid: number, username: string, homedir: string, shell: string }, tmpdir: string, homedir: string }` |\n| `getCpuUsage`     | 获取当前平台的 CPU 占用率      | `{ cpuUsage: string }`                                                           |\n| `getDiskUsage`    | 获取当前平台的硬盘使用率       | `string`（`df -h` 命令的输出）                                                   |\n| `getTerminalTypes`| 获取系统上支持的所有终端类型   | `{ terminalTypes: string[] }`                                                    |\n| `getIpv4Info`     | 获取当前设备的 IPv4 信息       | `{ [key: string]: Array<{ address: string, netmask: string, family: string, internal: boolean }> }` |\n| `getIpv6Info`     | 获取当前设备的 IPv6 信息       | `{ [key: string]: Array<{ address: string, netmask: string, family: string, internal: boolean }> }` |\n| `getProxyInfo`    | 获取当前网络的所有代理信息     | `{ httpProxy: string, httpsProxy: string, noProxy: string }`                     |\n| `getUsbInfo`     | 获取当前设备的 USB 设备信息    | `Array<{ bus: number, device: number, vendor: string, product: string, serial: string, type: string }>` |\n| `getPrinterInfo` | 获取当前设备的打印机信息       | `Array<{ name: string, status: string, type: string, driver: string }>`        |\n| `getSshPublicKey` | 获取当前用户的 SSH 公钥       | `Array<string>`（包含所有找到的公钥）                                           |\n| `getDockerInfo`  | 获取当前设备的 Docker 信息     | `{ version: object, images: Array<object>, containers: Array<object> }` 或 `{}` （未安装时）|\n| `getNodeInfo`  | 获取当前设备安装的 Node.js 版本信息 | `{ version: string, fullVersion: string, npmVersion: string, platform: string, arch: string, globalPackages: object, execPath: string, features: object, modules: object }` |\n## 开发指南\n\n```bash\n# 安装依赖\nnpm install\n\n# 开发模式（监听文件变化）\nnpm run dev\n\n# 构建项目\nnpm run build\n\n# 运行项目\nnpm start\n```\n\n## 环境变量配置\n\n通过 `.env` 文件配置环境变量：\n\n- `PORT`：服务器端口号（默认：3000）\n- `NODE_ENV`：运行环境（development/production）\n\n## 许可证\n\n本项目采用 ISC 许可证\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jackxuyi",
        "monitoring",
        "logging",
        "logging jackxuyi",
        "jackxuyi env",
        "mcp retrieve"
      ],
      "category": "monitoring-and-logging"
    },
    "MCP-100--mcp-sentry": {
      "owner": "MCP-100",
      "name": "mcp-sentry",
      "url": "https://github.com/MCP-100/mcp-sentry",
      "imageUrl": "/freedevtools/mcp/pfp/MCP-100.webp",
      "description": "Retrieve and analyze Sentry issues to facilitate debugging by inspecting error reports and stack traces. Integrate insights directly from Sentry into applications for enhanced workflow efficiency.",
      "stars": 21,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-06T09:38:51Z",
      "readme_content": "\n# mcp-sentry: A Sentry MCP server\n\n[![smithery badge](https://smithery.ai/badge/@qianniuspace/mcp-sentry)](https://smithery.ai/server/@qianniuspace/mcp-sentry)\n\n## Overview\n\nA Model Context Protocol server for retrieving and analyzing issues from Sentry.io. This server provides tools to inspect error reports, stacktraces, and other debugging information from your Sentry account.\n\n### Tools\n\n1. `get_sentry_issue`\n   - Retrieve and analyze a Sentry issue by ID or URL\n   - Input:\n     - `issue_id_or_url` (string): Sentry issue ID or URL to analyze\n   - Returns: Issue details including:\n     - Title\n     - Issue ID\n     - Status\n     - Level\n     - First seen timestamp\n     - Last seen timestamp\n     - Event count\n     - Full stacktrace\n2. `get_list_issues`\n   - Retrieve and analyze Sentry issues by project slug\n   - Input:\n     - `project_slug` (string): Sentry project slug to analyze\n     - `organization_slug` (string): Sentry organization slug to analyze\n   - Returns: List of issues with details including:\n     - Title\n     - Issue ID\n     - Status\n     - Level\n     - First seen timestamp\n     - Last seen timestamp\n     - Event count\n     - Basic issue information\n\n### Prompts\n\n1. `sentry-issue`\n   - Retrieve issue details from Sentry\n   - Input:\n     - `issue_id_or_url` (string): Sentry issue ID or URL\n   - Returns: Formatted issue details as conversation context\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-sentry for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@qianniuspace/mcp-sentry):\n\n```bash\nnpx -y @smithery/cli install @qianniuspace/mcp-sentry --client claude\n```\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *mcp-sentry*.\n\n### Using PIP\n\nAlternatively you can install `mcp-sentry` via pip:\n\n```\npip install mcp-sentry\n```\n\nor use uv\n```\nuv pip install -e .\n```\n\nAfter installation, you can run it as a script using:\n\n```\npython -m mcp_sentry\n```\n\n## Configuration\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"sentry\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-sentry\", \"--auth-token\", \"YOUR_SENTRY_TOKEN\",\"--project-slug\" ,\"YOUR_PROJECT_SLUG\", \"--organization-slug\",\"YOUR_ORGANIZATION_SLUG\"]\n  }\n}\n```\n</details>\n\n\n<details>\n<summary>Using docker</summary>\n\n```json\n\"mcpServers\": {\n  \"sentry\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"-i\", \"--rm\", \"mcp/sentry\", \"--auth-token\", \"YOUR_SENTRY_TOKEN\",\"--project-slug\" ,\"YOUR_PROJECT_SLUG\", \"--organization-slug\",\"YOUR_ORGANIZATION_SLUG\"]\n  }\n}\n```\n</details>\n\n<details>\n\n<summary>Using pip installation</summary>\n\n```json\n\"mcpServers\": {\n  \"sentry\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_sentry\", \"--auth-token\", \"YOUR_SENTRY_TOKEN\",\"--project-slug\" ,\"YOUR_PROJECT_SLUG\", \"--organization-slug\",\"YOUR_ORGANIZATION_SLUG\"]\n  }\n}\n```\n</details>\n\n### Usage with [Zed](https://github.com/zed-industries/zed)\n\nAdd to your Zed settings.json:\n\n<details>\n<summary>Using uvx</summary>\n\nFor Example Curson ![mcp.json](.cursor/mcp.json) \n\n```json\n\"context_servers\": [\n  \"mcp-sentry\": {\n    \"command\": {\n      \"path\": \"uvx\",\n      \"args\": [\"mcp-sentry\", \"--auth-token\", \"YOUR_SENTRY_TOKEN\",\"--project-slug\" ,\"YOUR_PROJECT_SLUG\", \"--organization-slug\",\"YOUR_ORGANIZATION_SLUG\"]\n    }\n  }\n],\n```\n</details>\n\n<details>\n<summary>Using pip installation</summary>\n\n```json\n\"context_servers\": {\n  \"mcp-sentry\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_sentry\", \"--auth-token\", \"YOUR_SENTRY_TOKEN\",\"--project-slug\" ,\"YOUR_PROJECT_SLUG\", \"--organization-slug\",\"YOUR_ORGANIZATION_SLUG\"]\n  }\n},\n```\n</details>\n\n<details>\n<summary>Using pip installation with custom path</summary>\n\n```json\n\"context_servers\": {\n  \"sentry\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"mcp_sentry\",\n        \"--auth-token\",\n        \"YOUR_SENTRY_TOKEN\",\n        \"--project-slug\",\n        \"YOUR_PROJECT_SLUG\",\n        \"--organization-slug\",\n        \"YOUR_ORGANIZATION_SLUG\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"path/to/mcp-sentry/src\"\n      }\n    }\n},\n```\n\n\n</details>\n\n\n\n\n\n\n\n## Debugging\n\nYou can use the MCP inspector to debug the server. For uvx installations:\n\n```\nnpx @modelcontextprotocol/inspector uvx mcp-sentry --auth-token YOUR_SENTRY_TOKEN --project-slug YOUR_PROJECT_SLUG --organization-slug YOUR_ORGANIZATION_SLUG\n```\n\nOr if you've installed the package in a specific directory or are developing on it:\n\n```\ncd path/to/servers/src/sentry\nnpx @modelcontextprotocol/inspector uv run mcp-sentry --auth-token YOUR_SENTRY_TOKEN --project-slug YOUR_PROJECT_SLUG --organization-slug YOUR_ORGANIZATION_SLUG  \n```\nor in term\n```\nnpx @modelcontextprotocol/inspector uv --directory /Volumes/ExtremeSSD/MCP/mcp-sentry/src run mcp_sentry --auth-token YOUR_SENTRY_TOKEN\n--project-slug YOUR_PROJECT_SLUG --organization-slug YOUR_ORGANIZATION_SLUG\n```\n![Inspector-tools](./images/Inspector-tools.png)\n\n## Fork From\n- [https://github.com/modelcontextprotocol/servers/tree/main/src/sentr](https://github.com/modelcontextprotocol/servers/tree/main/src/sentry)\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sentry",
        "debugging",
        "logging",
        "mcp sentry",
        "sentry issues",
        "sentry applications"
      ],
      "category": "monitoring-and-logging"
    },
    "MindscapeHQ--mcp-server-raygun": {
      "owner": "MindscapeHQ",
      "name": "mcp-server-raygun",
      "url": "https://github.com/MindscapeHQ/mcp-server-raygun",
      "imageUrl": "/freedevtools/mcp/pfp/MindscapeHQ.webp",
      "description": "Access Raygun's error tracking and monitoring data, enabling users to view errors, crashes, and performance issues in applications. Provides comprehensive access to Raygun's API for managing applications and errors.",
      "stars": 17,
      "forks": 11,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T22:50:36Z",
      "readme_content": "# Raygun MCP Server\n\nMCP Server for Raygun's API V3 endpoints for interacting with your Crash Reporting and Real User Monitoring applications. This server provides comprehensive access to Raygun's API features through the Model Context Protocol.\n\n## Features\n\n### Tools\n\n#### Applications\n- `list_applications` - List all applications under your account\n- `get_application` - Get application details by identifier\n- `get_application_by_api_key` - Get application details by API key\n- `regenerate_application_api_key` - Generate a new API key for an application\n\n#### Error Management\n- `list_error_groups` - List error groups for an application\n- `get_error_group` - Get detailed information about an error group\n- `resolve_error_group` - Set error group status to resolved\n- `activate_error_group` - Set error group status to active\n- `ignore_error_group` - Set error group status to ignored\n- `permanently_ignore_error_group` - Set error group status to permanently ignored\n\n#### Deployment Management\n- `list_deployments` - List deployments for an application\n- `get_deployment` - Get deployment details by identifier\n- `delete_deployment` - Remove a deployment\n- `update_deployment` - Update deployment information\n- `reprocess_deployment_commits` - Reprocess deployment commit data\n\n#### User & Session Management\n- `list_customers` - List customers for an application\n- `list_sessions` - List user sessions for an application\n- `get_session` - Get detailed session information\n\n#### Performance Monitoring\n- `list_pages` - List monitored pages for an application\n- `get_page_metrics_time_series` - Get time-series performance metrics\n- `get_page_metrics_histogram` - Get histogram of performance metrics\n- `get_error_metrics_time_series` - Get time-series error metrics\n\n#### Source Maps\n- `list_source_maps` - List source maps for an application\n- `get_source_map` - Get source map details\n- `update_source_map` - Update source map information\n- `delete_source_map` - Remove a source map\n- `upload_source_map` - Upload a new source map\n- `delete_all_source_maps` - Remove all source maps\n\n#### Team Management\n- `list_invitations` - List pending team invitations\n- `send_invitation` - Send a new team invitation\n- `get_invitation` - Get invitation details\n- `revoke_invitation` - Revoke a pending invitation\n\n## Configuration\n\nThe server requires the following environment variables:\n\n- `RAYGUN_PAT_TOKEN` (required): Your [Raygun PAT token](https://raygun.com/documentation/product-guides/raygun-api/)\n- `SOURCEMAP_ALLOWED_DIRS` (optional): Comma-separated list of directories allowed for source map operations\n\n## Usage with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"raygun\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@raygun.io/mcp-server-raygun\"],\n      \"env\": {\n        \"RAYGUN_PAT_TOKEN\": \"your-pat-token-here\"\n      }\n    }\n  }\n}\n```\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"raygun\": {\n      \"command\": \"/path/to/server-raygun/build/index.js\",\n      \"env\": {\n        \"RAYGUN_PAT_TOKEN\": \"your-pat-token-ken\"\n      }\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "raygun",
        "mindscapehq",
        "logging",
        "server raygun",
        "raygun api",
        "logging mindscapehq"
      ],
      "category": "monitoring-and-logging"
    },
    "Nozomuts--datadog-mcp": {
      "owner": "Nozomuts",
      "name": "datadog-mcp",
      "url": "https://github.com/Nozomuts/datadog-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Nozomuts.webp",
      "description": "Enables powerful search and aggregation of logs and trace spans from Datadog, allowing for flexible queries and detailed data retrieval. Facilitates the integration of Datadog monitoring data into applications for insightful analysis.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-28T14:00:49Z",
      "readme_content": "# Datadog MCP Server\n\n[English (This Document)](/README.md) | [日本語](/README-ja.md)\n\nMCP Server for Datadog API, enabling log search, trace span search, and trace span aggregation functionalities.\n\n<a href=\"https://glama.ai/mcp/servers/@Nozomuts/datadog-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Nozomuts/datadog-mcp/badge\" alt=\"Datadog Server MCP server\" />\n</a>\n\n## Features\n\n- **Log Search**: Search and retrieve logs from Datadog with flexible query options\n- **Trace Span Search**: Search for distributed trace spans with various filtering options\n- **Trace Span Aggregation**: Aggregate trace spans by different dimensions for analysis\n\n## Tools\n\n1. `search_logs`\n   - Search for logs in Datadog\n   - Inputs:\n     - `filterQuery` (optional string): Query string to search logs (default: \"*\")\n     - `filterFrom` (optional number): Search start time as UNIX timestamp in seconds (default: 15 minutes ago)\n     - `filterTo` (optional number): Search end time as UNIX timestamp in seconds (default: current time)\n     - `pageLimit` (optional number): Maximum number of logs to retrieve (default: 25, max: 1000)\n     - `pageCursor` (optional string): Pagination cursor for retrieving additional results\n   - Returns: Formatted text containing:\n     - Search conditions (query and time range)\n     - Number of logs found\n     - Next page cursor (if available)\n     - Log details including:\n       - Service name\n       - Tags\n       - Timestamp\n       - Status\n       - Message (truncated to 300 characters)\n       - Host\n       - Important attributes (http.method, http.url, http.status_code, error)\n\n2. `search_spans`\n   - Search for trace spans in Datadog\n   - Inputs:\n     - `filterQuery` (optional string): Query string to search spans (default: \"*\")\n     - `filterFrom` (optional number): Search start time as UNIX timestamp in seconds (default: 15 minutes ago)\n     - `filterTo` (optional number): Search end time as UNIX timestamp in seconds (default: current time)\n     - `pageLimit` (optional number): Maximum number of spans to retrieve (default: 25, max: 1000)\n     - `pageCursor` (optional string): Pagination cursor for retrieving additional results\n   - Returns: Formatted text containing:\n     - Search conditions (query and time range)\n     - Number of spans found\n     - Next page cursor (if available)\n     - Span details including:\n       - Service name\n       - Timestamp\n       - Resource name\n       - Duration (in seconds)\n       - Host\n       - Environment\n       - Type\n       - Important attributes (http.method, http.url, http.status_code, error)\n\n3. `aggregate_spans`\n   - Aggregate trace spans in Datadog by specified dimensions\n   - Inputs:\n     - `filterQuery` (optional string): Query string to filter spans for aggregation (default: \"*\")\n     - `filterFrom` (optional number): Start time as UNIX timestamp in seconds (default: 15 minutes ago)\n     - `filterTo` (optional number): End time as UNIX timestamp in seconds (default: current time)\n     - `groupBy` (optional string[]): Dimensions to group by (e.g., [\"service\", \"resource_name\", \"status\"])\n     - `aggregation` (optional string): Aggregation method - \"count\", \"avg\", \"sum\", \"min\", \"max\", \"pct\" (default: \"count\")\n     - `interval` (optional string): Time interval for time series data (only when type is \"timeseries\")\n     - `type` (optional string): Result type, either \"timeseries\" or \"total\" (default: \"timeseries\")\n   - Returns: Formatted text containing:\n     - Aggregation results in buckets, each including:\n       - Bucket ID\n       - Group by values (if groupBy is specified)\n       - Computed values based on the aggregation method\n     - Additional metadata:\n       - Processing time (elapsed)\n       - Request ID\n       - Status\n       - Warnings (if any)\n\n## Setup\nYou need to set up Datadog API and application keys:\n\n1. Get your API key and application key from the [Datadog API Keys page](https://app.datadoghq.com/organization-settings/api-keys)\n2. Install dependencies in the datadog-mcp project:\n   ```bash\n   npm install\n   # or\n   pnpm install\n   ```\n3. Build the TypeScript project:\n   ```bash\n   npm run build\n   # or\n   pnpm run build\n   ```\n\n### Docker Setup\nYou can build using Docker with the following command:\n\n```bash\ndocker build -t datadog-mcp .\n```\n\n### Usage with Claude Desktop\nTo use this with Claude Desktop, add the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"datadog\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/datadog-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"DD_API_KEY\": \"<YOUR_DATADOG_API_KEY>\",\n        \"DD_APP_KEY\": \"<YOUR_DATADOG_APP_KEY>\"\n      }\n    }\n  }\n}\n```\n\nIf you're using Docker, you can configure it like this:\n\n```json\n{\n  \"mcpServers\": {\n    \"datadog\": {\n      \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"DD_API_KEY\",\n          \"-e\",\n          \"DD_APP_KEY\",\n          \"datadog-mcp\"\n        ],\n      \"env\": {\n        \"DD_API_KEY\": \"<YOUR_DATADOG_API_KEY>\",\n        \"DD_APP_KEY\": \"<YOUR_DATADOG_APP_KEY>\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation in VS Code, configure your settings:\n\n1. Open User Settings (JSON) in VS Code (`Ctrl+Shift+P` → `Preferences: Open User Settings (JSON)`)\n2. Add the following configuration:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"datadog\": {\n        \"command\": \"node\",\n        \"args\": [\n          \"/path/to/datadog-mcp/build/index.js\"\n        ],\n        \"env\": {\n          \"DD_API_KEY\": \"<YOUR_DATADOG_API_KEY>\",\n          \"DD_APP_KEY\": \"<YOUR_DATADOG_APP_KEY>\"\n        }\n      }\n    }\n  }\n}\n```\n\nIf you're using Docker, you can configure it like this:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"datadog\": {\n        \"command\": \"docker\",\n          \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"DD_API_KEY\",\n          \"-e\",\n          \"DD_APP_KEY\",\n          \"datadog-mcp\"\n        ],\n        \"env\": {\n          \"DD_API_KEY\": \"<YOUR_DATADOG_API_KEY>\",\n          \"DD_APP_KEY\": \"<YOUR_DATADOG_APP_KEY>\"\n        }\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can add this to a `.vscode/mcp.json` file in your workspace (without the `mcp` key):\n\n```json\n{\n  \"servers\": {\n    \"datadog\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/datadog-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"DD_API_KEY\": \"<YOUR_DATADOG_API_KEY>\",\n        \"DD_APP_KEY\": \"<YOUR_DATADOG_APP_KEY>\"\n      }\n    }\n  }\n}\n```\n\nIf you're using Docker, you can configure it like this:\n\n```json\n{\n  \"servers\": {\n    \"datadog\": {\n      \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"DD_API_KEY\",\n          \"-e\",\n          \"DD_APP_KEY\",\n          \"datadog-mcp\"\n        ],\n      \"env\": {\n        \"DD_API_KEY\": \"<YOUR_DATADOG_API_KEY>\",\n        \"DD_APP_KEY\": \"<YOUR_DATADOG_APP_KEY>\"\n      }\n    }\n  }\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datadog",
        "logging",
        "logs",
        "datadog monitoring",
        "monitoring logging",
        "nozomuts datadog"
      ],
      "category": "monitoring-and-logging"
    },
    "Operative-Sh--playwright-consolelogs-mcp": {
      "owner": "Operative-Sh",
      "name": "playwright-consolelogs-mcp",
      "url": "https://github.com/Operative-Sh/playwright-consolelogs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Open a browser to monitor console logs and track network requests for improved debugging and analysis. Retrieve structured log data and network activity while maintaining a clean environment post-session.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "consolelogs",
        "logging",
        "debugging",
        "consolelogs mcp",
        "playwright consolelogs",
        "console logs"
      ],
      "category": "monitoring-and-logging"
    },
    "QAInsights--k6-mcp-server": {
      "owner": "QAInsights",
      "name": "k6-mcp-server",
      "url": "https://github.com/QAInsights/k6-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/QAInsights.webp",
      "description": "Run load tests using k6 with custom configurations and obtain real-time execution output. Integrates with the Model Context Protocol to streamline load testing processes and enhance performance insights.",
      "stars": 15,
      "forks": 8,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-11T16:28:07Z",
      "readme_content": "# 🚀 ⚡️ k6-mcp-server\n\nA Model Context Protocol (MCP) server implementation for running k6 load tests.\n\n## ✨ Features\n\n- Simple integration with Model Context Protocol framework\n- Support for custom test durations and virtual users (VUs)\n- Easy-to-use API for running k6 load tests\n- Configurable through environment variables\n- Real-time test execution output\n\n## 🔧 Prerequisites\n\nBefore you begin, ensure you have the following installed:\n\n- Python 3.12 or higher\n- k6 load testing tool ([Installation guide](https://grafana.com/docs/k6/latest/set-up/install-k6/))\n- uv package manager ([Installation guide](https://github.com/astral-sh/uv))\n\n## 📦 Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/qainsights/k6-mcp-server.git\n```\n\n2. Install the required dependencies:\n\n```bash\nuv pip install -r requirements.txt\n```\n\n3. Set up environment variables (optional):\n   Create a `.env` file in the project root:\n\n```bash\nK6_BIN=/path/to/k6  # Optional: defaults to 'k6' in system PATH\n```\n\n## 🚀 Getting Started\n\n1. Create a k6 test script (e.g., `test.js`):\n\n```javascript\nimport http from \"k6/http\";\nimport { sleep } from \"k6\";\n\nexport default function () {\n  http.get(\"http://test.k6.io\");\n  sleep(1);\n}\n```\n\n2. Configure the MCP server using the below specs in your favorite MCP client (Claude Desktop, Cursor, Windsurf and more):\n\n```json\n{\n  \"mcpServers\": {\n    \"k6\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/k6-mcp-server\",\n        \"run\",\n        \"k6_server.py\"\n      ]\n    }\n  }\n}\n\n```\n3. Now ask the LLM to run the test e.g. `run k6 test for hello.js`. The k6 mcp server will leverage either one of the below tools to start the test.\n\n- `execute_k6_test`: Run a test with default options (30s duration, 10 VUs)\n- `execute_k6_test_with_options`: Run a test with custom duration and VUs\n\n![k6-MCP](./images/k6-mcp.png)\n\n\n## 📝 API Reference\n\n### Execute K6 Test\n\n```python\nexecute_k6_test(\n    script_file: str,\n    duration: str = \"30s\",  # Optional\n    vus: int = 10          # Optional\n)\n```\n\n### Execute K6 Test with Custom Options\n\n```python\nexecute_k6_test_with_options(\n    script_file: str,\n    duration: str,\n    vus: int\n)\n```\n\n## ✨ Use cases\n\n- LLM powered results analysis\n- Effective debugging of load tests\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "k6",
        "monitoring",
        "testing",
        "load testing",
        "load tests",
        "using k6"
      ],
      "category": "monitoring-and-logging"
    },
    "Yaswanth-ampolu--smithery-mcp-server": {
      "owner": "Yaswanth-ampolu",
      "name": "smithery-mcp-server",
      "url": "https://github.com/Yaswanth-ampolu/smithery-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Yaswanth-ampolu.webp",
      "description": "Provides terminal access through a web interface for executing shell commands, browsing directories, and managing server tasks without requiring root access. Supports real-time updates of command outputs for immediate feedback.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "HTML",
      "updated_at": "2025-06-22T17:46:47Z",
      "readme_content": "# MCP Terminal Server\n\nMCP Terminal Server provides terminal access and system tools via a clean web interface. It enables remote command execution, directory listing, and other terminal operations through a simple HTTP server.\n\n![MCP Terminal Server](https://raw.githubusercontent.com/Yaswanth-ampolu/smithery-mcp-server/main/screenshots/terminal-server.png)\n\n## Features\n\n- 🚀 **Remote Terminal Access**: Execute shell commands from a web browser\n- 📁 **Directory Listing**: Browse and list files in any directory\n- 🔒 **Secure Local Installation**: Runs without requiring root access\n- 🌐 **Web Interface**: Clean, modern UI for easy interaction\n- 🔄 **Real-time Updates**: See command outputs as they happen\n- 🛠️ **Easy Management**: Simple commands to start, stop, and manage the server\n\n## Prerequisites\n\nBefore installing the MCP Terminal Server, ensure your system meets these requirements:\n\n- **Node.js** (version 14 or higher)\n- **curl** (for downloading the installation script)\n- **tar** (for extracting the package)\n\n## Quick Installation\n\nInstall the MCP Terminal Server with a single command:\n\n```bash\ncurl -o- https://github.com/Yaswanth-ampolu/smithery-mcp-server/raw/main/main/install-mcp.sh | bash\n```\n\nThis will:\n1. Download the installation script\n2. Check for dependencies\n3. Download and extract the server files\n4. Set up the necessary directories\n5. Add the command to your PATH\n\n## Manual Installation\n\nIf you prefer to inspect the script before running it:\n\n1. Download the installation script:\n   ```bash\n   curl -o install-mcp.sh https://github.com/Yaswanth-ampolu/smithery-mcp-server/raw/main/main/install-mcp.sh\n   ```\n\n2. Review the script content:\n   ```bash\n   less install-mcp.sh\n   ```\n\n3. Run the installation:\n   ```bash\n   bash install-mcp.sh\n   ```\n\n## Usage\n\nAfter installation, the MCP Terminal Server can be managed with the following commands:\n\n### Starting the Server\n\n```bash\nmcp-terminal start\n```\n\nBy default, the server starts on port 8080. To use a different port:\n\n```bash\nmcp-terminal start --port 9000\n```\n\n### Stopping the Server\n\n```bash\nmcp-terminal stop\n```\n\n### Checking Server Status\n\n```bash\nmcp-terminal status\n```\n\nThis shows:\n- Whether the server is running\n- The PID of the running server\n- Memory usage\n- The URL to access the web interface\n\n### Restarting the Server\n\n```bash\nmcp-terminal restart\n```\n\n### Uninstalling\n\nTo completely remove the MCP Terminal Server:\n\n```bash\nmcp-terminal uninstall\n```\n\n## Web Interface\n\nOnce the server is running, access the web interface at:\n\n```\nhttp://localhost:8080\n```\n\n(or the custom port you specified)\n\nThe web interface provides:\n- A command execution tool\n- A directory listing tool\n- Real-time output display\n\n## Configuration\n\nThe MCP Terminal Server stores its files in:\n- `~/mcp-terminal` - Main installation directory\n- `~/bin/mcp-terminal` - Command script\n\nLog files and PID information are stored in:\n- `~/mcp-terminal/mcp.log` - Server log file\n- `~/mcp-terminal/mcp.pid` - Server PID file\n\n## Troubleshooting\n\n### Server Won't Start\n\nIf the server fails to start:\n\n1. Check if Node.js is installed and version 14+:\n   ```bash\n   node -v\n   ```\n\n2. Check the log file for errors:\n   ```bash\n   tail -n 50 ~/mcp-terminal/mcp.log\n   ```\n\n3. Verify the installation directory exists:\n   ```bash\n   ls -la ~/mcp-terminal\n   ```\n\n### Port Already in Use\n\nIf the default port (8080) is already in use:\n\n```bash\nmcp-terminal start --port 9000\n```\n\n### Missing Command\n\nIf the `mcp-terminal` command is not found:\n\n1. Ensure `~/bin` is in your PATH:\n   ```bash\n   echo $PATH\n   ```\n\n2. If not, add it manually:\n   ```bash\n   export PATH=\"$HOME/bin:$PATH\"\n   ```\n\n3. For permanent addition, add to your shell configuration:\n   ```bash\n   echo 'export PATH=\"$HOME/bin:$PATH\"' >> ~/.bashrc\n   source ~/.bashrc\n   ```\n\n## Security Considerations\n\n- The MCP Terminal Server executes commands with the same permissions as the user who started it\n- It's recommended to run the server on a local network or behind a firewall\n- Consider using a reverse proxy with authentication for public-facing deployments\n\n## License\n\n[MIT License](LICENSE)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Support\n\nFor issues, questions, or feedback, please open an issue on the [GitHub repository](https://github.com/Yaswanth-ampolu/smithery-mcp-server/issues). ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "yaswanth",
        "terminal",
        "logging yaswanth",
        "provides terminal",
        "mcp server"
      ],
      "category": "monitoring-and-logging"
    },
    "Zwe1--error-monitor-frontend": {
      "owner": "Zwe1",
      "name": "error-monitor-frontend",
      "url": "https://github.com/Zwe1/error-monitor-frontend",
      "imageUrl": "/freedevtools/mcp/pfp/Zwe1.webp",
      "description": "构建一个错误监控系统，集中收集、存储和展示 B 端应用的远程错误信息，旨在提升错误处理效率和用户体验。该系统支持前端错误上报和展示处理后的错误信息。",
      "stars": 10,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2024-11-25T05:56:28Z",
      "readme_content": "## 错误监控系统 (client)\n\n### 目标\n\n针对 B 端应用难以分析远程错误，设想构建一个错误收集监控系统，以便错误记录与收集并展示。\n\n### 架构\n\n整个系统包含前端系统，source-map [插件](https://github.com/Zwe1/error-monitor-webpack-plugin) 和 [后端系统](https://github.com/Zwe1/error-monitor-node-server)，及数据库服务。前端收集上报发送错误信息到服务端，服务端处理收集存储错误信息到数据库，并支持前端获取处理后的错误信息，在前端进行集中展示。\n\n<img src='./src/imgs/structure.jpg'/>\n\n### 项目表述\n\n该项目由 create-react-app 搭建，进行 override webpack config 操作来扩展打包配置。进行错误生产，作为整个系统的前端实验室。\n\n### 基本功能\n\n1. 错误上报\n2. 错误展示\n3. ci 系统 (hard)\n\n### 难点\n\n1. 分层拦截错误\n\n2. 如何在本地搭建一个生产环境？\n\n为了能够验证 production 模式打包后的 sourcemap。我们需要在本地搭建一个前端生产环境。这里考虑用 nginx 作为前端服务器，放置在 docker 容器当中，这种方式十分方便和高效，一台物理机就可以完成所有工作。\n\n````md\n1. 首先我们需要安装 [docker](https://www.docker.com/)\n\n2. 下来拉取 nginx 镜像。\n\n   docker pull nginx\n\n3. 创建 nginx 相关目录\n\nmkdir -p /data/nginx/{conf, conf.d,logs}\n\n这里我们在宿主机的 /data/nginx 目录放置 nginx 相关的文件，这个目录是可自定义的，但后续的目录映射一定要保证和这个目录相同。\n\n4. 新建 nginx 配置文件\n\n   touch /data/nginx/conf/nginx.conf\n   vim /data/nginx/conf/nginx.conf\n\n   ```conf\n   user nginx;\n   worker_processes  1;\n   error_log  /var/log/nginx/error.log warn;\n   pid        /var/run/nginx.pid;\n   events {\n       worker_connections  1024;\n   }\n\n   http {\n       include      /etc/nginx/mime.types;\n       default_type  application/octet-stream;\n       log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n\n                       '$status $body_bytes_sent \"$http_referer\" '\n\n                       '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n       access_log  /var/log/nginx/access.log  main;\n       sendfile        on;\n       #tcp_nopush    on;\n       keepalive_timeout  65;\n       #gzip  on;\n       include /etc/nginx/conf.d/*.conf;\n   }\n   ```\n\n   5. 新建 default.conf\n\n   touch /data/nginx/conf.d/default.conf\n   vim /data/nginx/conf.d/default.conf\n\n   ```conf\n\n    server {\n    listen      80;\n    server_name  localhost;\n    location / {\n        root  /usr/share/nginx/html;\n        index  index.html index.htm;\n        autoindex  on;\n    }\n\n    error_page  500 502 503 504  /50x.html;\n\n    location = /50x.html {\n\n        root  /usr/share/nginx/html;\n\n    }\n   ```\n\n   到这里，有关于 nginx 配置的处理就完成了，下来我们要做的就是进行 docker 容器与宿主机的目录映射\n\n   6. 将 nginx 内容挂载到宿主机\n\n   docker run -p 80:80 -d -v /Users/xxx/Documents/lab/error-monitor/react-repo/build:/usr/share/nginx/html -v /data/nginx/logs:/var/log/nginx -v /data/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /data/nginx/conf.d:/etc/nginx/conf.d docker.io/nginx\n\n   这里可以看到我们映射了两个目录和两个配置文件，包括了前端 html 文件目录，log 目录以及两个 nginx 配置文件。这里我直接将我们前端项目的打包目录映射到了容器中的 html 目录中，这样会比较方便一些。\n\n   这里我们选择宿主机的 80 端口映射 nginx 容器的 80 端口，我们直接打开本机的浏览器访问 localhost ，就可以看到打包完后的前端项目运行起来了。如果 80 端口有其他用途 ，可以自行切换到其他端口。\n````\n\n<img src='./public/nginx-container.png'>\n\n<img src='./public/fe-release.png'>\n\n2. source-map 解析\n\n使用 Mozilla source-map 解析 .map 文件生成源代码\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zwe1",
        "logging",
        "monitoring",
        "logging zwe1",
        "error monitor",
        "zwe1 error"
      ],
      "category": "monitoring-and-logging"
    },
    "Zzzccs123--mcp-sentry": {
      "owner": "Zzzccs123",
      "name": "mcp-sentry",
      "url": "https://github.com/Zzzccs123/mcp-sentry",
      "imageUrl": "/freedevtools/mcp/pfp/Zzzccs123.webp",
      "description": "Connect to Sentry's error tracking service to retrieve and analyze error reports and gain insights into application issues with detailed information such as status and stack traces.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-22T20:30:16Z",
      "readme_content": "# MCP Server Sentry - TypeScript Implementation\n\nThis is a Model Context Protocol (MCP) server implemented in TypeScript for connecting to the Sentry error tracking service. This server allows AI models to query and analyze error reports and events on Sentry.\n\n## Features\n\n1. `get_sentry_issue` Tool\n   * Retrieves and analyzes Sentry issues by ID or URL\n   * Input:\n     * `issue_id_or_url` (string): Sentry issue ID or URL to analyze\n   * Returns: Issue details including:\n     * Title\n     * Issue ID\n     * Status\n     * Level\n     * First seen timestamp\n     * Last seen timestamp\n     * Event count\n     * Complete stack trace\n\n2. `sentry-issue` Prompt Template\n   * Retrieves issue details from Sentry\n   * Input:\n     * `issue_id_or_url` (string): Sentry issue ID or URL\n   * Returns: Formatted issue details as conversation context\n\n## Installation\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Configuration\n\nThe server is configured using environment variables. Create a `.env` file in the project root directory:\n\n```\n# Required: Sentry authentication token\nSENTRY_AUTH_TOKEN=your_sentry_auth_token\n\n# Optional: Sentry organization name\nSENTRY_ORGANIZATION_SLUG=your_organization_slug\n\n# Optional: Sentry project name\nSENTRY_PROJECT_SLUG=your_project_slug\n\n# Optional: Sentry base url\nSENTRY_BASE_URL=https://sentry.com/api/0\n```\n\nAlternatively, you can set these environment variables at runtime.\n\n## Running\n\nRun the server via standard IO:\n\n```bash\nnode dist/index.js\n```\n\nDebug with MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## Environment Variables Description\n\n- `SENTRY_AUTH_TOKEN` (required): Your Sentry API access token\n- `SENTRY_PROJECT_SLUG` (optional): The slug of your Sentry project\n- `SENTRY_ORGANIZATION_SLUG` (optional): The slug of your Sentry organization\n\nThe latter two variables can be omitted if project and organization information are provided in the URL.\n\n## License\n\nThis project is licensed under the MIT License. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sentry",
        "monitoring",
        "logging",
        "sentry connect",
        "connect sentry",
        "mcp sentry"
      ],
      "category": "monitoring-and-logging"
    },
    "abhinav7895--system-mcp": {
      "owner": "abhinav7895",
      "name": "system-mcp",
      "url": "https://github.com/abhinav7895/system-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/abhinav7895.webp",
      "description": "Provides real-time system monitoring capabilities for CPU, memory, disk, network, battery, and internet speed metrics. Enables querying of detailed system resource usage and internet performance for enhanced diagnostics and management.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-10T06:30:43Z",
      "readme_content": "# System Resource Monitor MCP Server\n\nAn MCP server that provides Claude with real-time system monitoring capabilities, including CPU, memory, disk, network, battery, and internet speed metrics using `systeminformation` and multi-source speed tests.\n\n![Demo](assets/example-mcp.png)  \n\n## Tools\n\n- **get_cpu_usage**\n  - Retrieves the current CPU load as a percentage, including overall and per-core usage.\n  - **Inputs**: None\n  - **Output**: Text (e.g., `CPU Load: 12.34% (Cores: 10.50, 15.20, 8.90, 14.60%)`)\n\n- **get_memory_usage**\n  - Reports total, used, and free memory in GB, plus percentage used.\n  - **Inputs**: None\n  - **Output**: Text (e.g., `Memory: 65.43% used (7.82GB / 16.00GB)`)\n\n- **get_disk_space**\n  - Shows disk usage for the largest drive in GB and percentage.\n  - **Inputs**: None\n  - **Output**: Text (e.g., `Disk (/): 78.90% used (189.50GB / 250.00GB)`)\n\n- **get_network_usage**\n  - Returns real-time network RX/TX rates (KB/s) and total data since boot (MB).\n  - **Inputs**: None\n  - **Output**: Text (e.g., `Network (eth0): RX: 25.50KB/s, TX: 10.20KB/s (Total: RX 150.34MB, TX 75.89MB)`)\n\n- **get_battery_status**\n  - Provides battery charge percentage, charging status, and time remaining (if applicable).\n  - **Inputs**: None\n  - **Output**: Text (e.g., `Battery: 85% (charging), 120 min remaining` or `No battery detected`)\n\n- **get_internet_speed**\n  - Measures internet speed using multiple download sources (including a user-uploaded file) and upload tests, returning median speeds in Mbps.\n  - **Inputs**: None\n  - **Output**: Text (e.g., `Internet Speed: Download 45.67Mbps, Upload 8.45Mbps`)\n\n## Configuration\n\n### Step 1: Clone and Install\n\nClone this repository:\n\n```bash\ngit clone git@github.com:abhinav7895/system-mcp.git\n```\n\nNavigate to the directory and install dependencies:\n\n```bash\ncd system-resource-monitor && npm install\n```\n\n### Step 2: Build the Project\n\nCompile the TypeScript code:\n\n```bash\nnpm run build\n```\n\nThis generates the `dist/index.js` file, ready to run as an MCP server.\n\n### Step 3: Configure Claude Desktop\n\n1. Download Claude Desktop [here](https://claude.ai/download).\n2. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"system-resource-monitor\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/dist/index.js\"]\n    }\n  }\n}\n```\n\nAccess the config file:\n\n```bash\nvim ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n*(Adjust the path to `dist/index.js` based on your project location.)*\n\n\n\n### Step 4: Testing\n\nEnsure Claude Desktop recognizes the tools by checking for the hammer icon:\n\n![Claude Visual Tools](assets/hammer-mcp.png)\n\nClick the hammer icon to see available tools:\n\n![Available Integration](assets/tools-mcp.png)\n\nIf all six tools (`get_cpu_usage`, `get_memory_usage`, etc.) appear, the integration is active. You can now ask questions like:\n- \"What’s my CPU usage?\"\n- \"How fast is my internet?\"\n\n### Step 5: Advanced Customization\n\n- **Internet Speed Test**: Modify `testUrls` in `index.ts` to use different download sources or adjust `uploadSizeBytes` (default 80KB) for upload tests.\n- **Logging**: Console logs provide detailed test output; disable them in production by removing `console.log` statements.\n\n### Troubleshooting\n\n- **Tool Not Showing**: Verify the server is running (`node dist/index.js`) and the config path is correct.\n- **Internet Speed Errors**: Ensure network connectivity and test URLs are accessible. Check console logs for specific failures.\n- Refer to the [MCP troubleshooting guide](https://modelcontextprotocol.io/docs/tools/debugging)\n\n## License\n\nThis MCP server is licensed under the MIT License. You are free to use, modify, and distribute the software under the terms of the MIT License. See the `LICENSE` file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "logging",
        "mcp",
        "monitoring capabilities",
        "monitoring logging",
        "logging abhinav7895"
      ],
      "category": "monitoring-and-logging"
    },
    "ahmad2x4--mcp-server-seq": {
      "owner": "ahmad2x4",
      "name": "mcp-server-seq",
      "url": "https://github.com/ahmad2x4/mcp-server-seq",
      "imageUrl": "/freedevtools/mcp/pfp/ahmad2x4.webp",
      "description": "Interact with Seq's logging and monitoring system via its API endpoints, providing access to various features for managing signals and events.",
      "stars": 7,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-19T03:06:37Z",
      "readme_content": "# Seq MCP Server\n\nMCP Server for Seq's API endpoints for interacting with your logging and monitoring system. This server provides comprehensive access to Seq's API features through the Model Context Protocol.\n\n<a href=\"https://glama.ai/mcp/servers/yljb00fc2g\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/yljb00fc2g/badge\" alt=\"Seq Server MCP server\" /></a>\n\n## Features\n\n### Tools\n\n#### Signals Management\n- `get-signals` - Fetch signals with filtering options\n  - Filter by owner ID\n  - Filter shared/private signals\n  - Support for partial matches\n\n#### Event Management\n- `get-events` - Retrieve events with extensive filtering options\n  - Filter by signal IDs\n  - Custom filter expressions\n  - Configurable event count (max 100)\n  - Flexible time range options\n  - Date range filtering\n\n#### Alert Management\n- `get-alertstate` - Retrieve the current state of alerts\n\n### Resources\n\n#### Signals Listing\n- `signals` - List all shared signals with detailed information\n  - Signal ID\n  - Title\n  - Description\n  - Sharing status\n  - Owner information\n\n## Configuration\n\nThe server requires the following environment variables:\n\n- `SEQ_BASE_URL` (optional): Your Seq server URL (defaults to 'http://localhost:8080')\n- `SEQ_API_KEY` (required): Your Seq API key\n\n## Usage with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"seq\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-seq\"],\n      \"env\": {\n        \"SEQ_BASE_URL\": \"your-seq-url\",\n        \"SEQ_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run dev\n```\n\nRun tests:\n```bash\nnpm run test-script\n```\n\n## Time Range Options\n\nThe `get-events` tool supports the following time range options:\n- `1m` - Last minute\n- `15m` - Last 15 minutes\n- `30m` - Last 30 minutes\n- `1h` - Last hour\n- `2h` - Last 2 hours\n- `6h` - Last 6 hours\n- `12h` - Last 12 hours\n- `1d` - Last day\n- `7d` - Last 7 days\n- `14d` - Last 14 days\n- `30d` - Last 30 days\n\n## Installation\n\nThis tool is still in development and we havn't pushed to the npm repository. You need to clone this repository on your local then build `npm run build`\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"seq\": {\n      \"command\":\"node\",\n      \"args\": [\"/Users/ahmadreza/source/ahmad2x4/mcp-server-seq/build/seq-server.js\"],\n      \"env\": {\n        \"SEQ_BASE_URL\": \"your-seq-url\",\n        \"SEQ_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. The server implements proper error handling and logging for all operations. You can run the test script to verify functionality:\n\n```bash\nnpm run test-script\n```\n## Type Safety\n\nThe server implements comprehensive type safety using:\n- TypeScript for static type checking\n- Zod schema validation for runtime type checking\n- Proper error handling and response formatting\n=======\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "seq",
        "monitoring",
        "logging",
        "seq logging",
        "server seq",
        "logging ahmad2x4"
      ],
      "category": "monitoring-and-logging"
    },
    "amitdeshmukh--stdout-mcp-server": {
      "owner": "amitdeshmukh",
      "name": "stdout-mcp-server",
      "url": "https://github.com/amitdeshmukh/stdout-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/amitdeshmukh.webp",
      "description": "Captures and manages stdout logs from multiple processes, providing real-time monitoring and a standardized interface for querying, filtering, and analyzing logs. Facilitates debugging by enabling easy log retrieval through named pipes.",
      "stars": 6,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-21T17:17:55Z",
      "readme_content": "# stdout-mcp-server\n\nA Model Context Protocol (MCP) server that captures and manages stdout logs through a named pipe system. This server is particularly useful for:\n- Capturing logs from multiple processes or applications and making them available for debugging in Cursor IDE.\n- Monitoring application output in real-time and providing a MCP interface to query, filter, and analyze logs\n\n## How It Works\n\n1. The server creates a named pipe at a specific location (`/tmp/stdout_pipe` on Unix/MacOS or `\\\\.\\pipe\\stdout_pipe` on Windows)\n\n2. Any application can write logs to this pipe using standard output redirection. For example:\n```bash\nyour_application | tee /tmp/stdout_pipe # or\nyour_application > /tmp/stdout_pipe\n```\n3. The server monitors the pipe, captures all incoming logs, and maintains a history of the last 100 entries\n\n4. Through MCP tools, you can query, filter, and analyze these logs\n\n## System Requirements\n\nBefore installing, please ensure you have:\n\n* Node.js v18 or newer\n\n## Installation Options\n\n### Option 1: Installation in Cursor\n\n1. Open Cursor and navigate to `Cursor > Settings > MCP Servers`\n2. Click on \"Add new MCP Server\"\n3. Update your MCP settings file with the following configuration:\n\n```sh\nname: stdout-mcp-server\ntype: command\ncommand: npx stdout-mcp-server\n```\n\n### Option 2: Installation in other MCP clients\n\n## Installation in other MCP clients\nFor macOS/Linux:\n```json\n{\n  \"mcpServers\": {\n    \"stdio-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"stdio-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\nFor Windows:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-installer\": {\n      \"command\": \"cmd.exe\",\n      \"args\": [\"/c\", \"npx\", \"stdio-mcp-server\"]\n    }\n  }\n}\n```\n\n## Usage Examples\n\n### Redirecting Application Logs\n\nTo send your application's output to the pipe:\n\n```bash\n# Unix/MacOS\nyour_application > /tmp/stdout_pipe\n\n# Windows (PowerShell)\nyour_application > \\\\.\\pipe\\stdout_pipe\n```\n\n### Monitoring Multiple Applications\n\nYou can redirect logs from multiple sources:\n\n```bash\n# Application 1\napp1 > /tmp/stdout_pipe &\n\n# Application 2\napp2 > /tmp/stdout_pipe &\n```\n\n### Querying Logs\n\nYour AI will use the `get-logs` tool in your MCP client to retrieve and filter logs:\n\n```typescript\n// Get last 50 logs\nget-logs()\n\n// Get last 100 logs containing \"error\"\nget-logs({ lines: 100, filter: \"error\" })\n\n// Get logs since a specific timestamp\nget-logs({ since: 1648675200000 }) // Unix timestamp in milliseconds\n```\n\n## Features\n\n- Named pipe creation and monitoring\n- Real-time log capture and storage\n- Log filtering and retrieval through MCP tools\n- Configurable log history (default: 100 entries)\n- Cross-platform support (Windows and Unix-based systems)\n\n## Named Pipe Locations\n\n- Windows: `\\\\.\\pipe\\stdout_pipe`\n- Unix/MacOS: `/tmp/stdout_pipe`\n\n## Available Tools\n\n### get-logs\n\nRetrieve logs from the named pipe with optional filtering:\n\nParameters:\n- `lines` (optional, default: 50): Number of log lines to return\n- `filter` (optional): Text to filter logs by\n- `since` (optional): Timestamp to get logs after\n\nExample responses:\n```typescript\n// Response format\n{\n  content: [{\n    type: \"text\",\n    text: \"[2024-03-20T10:15:30.123Z] Application started\\n[2024-03-20T10:15:31.456Z] Connected to database\"\n  }]\n}\n```\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "stdout",
        "logging",
        "logs",
        "stdout logs",
        "stdout mcp",
        "manages stdout"
      ],
      "category": "monitoring-and-logging"
    },
    "awslabs--Log-Analyzer-with-MCP": {
      "owner": "awslabs",
      "name": "Log-Analyzer-with-MCP",
      "url": "https://github.com/awslabs/Log-Analyzer-with-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/awslabs.webp",
      "description": "Access AWS CloudWatch Logs for efficient analysis, searching, and correlation. Provides functionalities for browsing log groups, executing queries, generating summaries, and identifying error patterns across AWS services.",
      "stars": 129,
      "forks": 19,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-28T03:42:30Z",
      "readme_content": "# Log Analyzer with MCP\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server that provides AI assistants access to AWS CloudWatch Logs for analysis, searching, and correlation.\n\n## 🏗️ Architecture\n![Architecture Diagram](./docs/assets/Log-Analyzer-with-MCP-arch.png)\n\n## 🔌 Model Context Protocol (MCP)\n\nAs outlined by Anthropic:\n> MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n\nThis repository is an example client and server that allows an AI assistant like Claude to interact with CloudWatch logs in an AWS account. To learn more about MCP, read through the [introduction](https://modelcontextprotocol.io/introduction). \n\n## ✨ Features\n\n- Browse and search CloudWatch Log Groups\n- Search logs using CloudWatch Logs Insights query syntax\n- Generate log summaries and identify error patterns\n- Correlate logs across multiple AWS services\n- AI-optimized tools for assistants like Claude\n\n[Detailed feature list](./docs/features.md)\n\n## 🚀 Installation\n\n### Prerequisites\n\n- The [uv](https://github.com/astral-sh/uv) Python package and project manager\n- An AWS account with CloudWatch Logs\n- Configured [AWS credentials](./docs/aws-config.md)\n\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/awslabs/Log-Analyzer-with-MCP.git\ncd Log-Analyzer-with-MCP\n\n# Create a virtual environment and install dependencies\nuv sync\nsource .venv/bin/activate  # On Windows, use `.venv\\Scripts\\activate`\n```\n\n## 🚦 Quick Start\n\n1. Make sure to have configured your AWS credentials as [described here](./docs/aws-config.md)\n\n2. Update your `claude_desktop_config.json` file with the proper configuration outlined in the [AI integration guide](./docs/ai-integration.md)\n\n3. Open Claude for Desktop and start chatting!\n\nFor more examples and advanced usage, see the [detailed usage guide](./docs/usage.md).\n\n## 🤖 AI Integration\n\nThis project can be easily integrated with AI assistants like Claude for Desktop. See the [AI integration guide](./docs/ai-integration.md) for details.\n\n## 📚 Documentation\n\n- [Detailed Features](./docs/features.md)\n- [Usage Guide](./docs/usage.md)\n- [AWS Configuration](./docs/aws-config.md)\n- [Architecture Details](./docs/architecture.md)\n- [AI Integration](./docs/ai-integration.md)\n- [Troubleshooting](./docs/troubleshooting.md)\n\n## 🔒 Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## 📄 License\n\nThis project is licensed under the Apache-2.0 License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "awslabs",
        "cloudwatch",
        "aws",
        "logging awslabs",
        "cloudwatch logs",
        "aws cloudwatch"
      ],
      "category": "monitoring-and-logging"
    },
    "codyde--mcp-sentry-ts": {
      "owner": "codyde",
      "name": "mcp-sentry-ts",
      "url": "https://github.com/codyde/mcp-sentry-ts",
      "imageUrl": "/freedevtools/mcp/pfp/codyde.webp",
      "description": "Interact with Sentry's API to retrieve and analyze error data, manage projects, and monitor application performance.",
      "stars": 20,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-03T09:13:52Z",
      "readme_content": "# Sentry MCP Server\n\nA Model Context Protocol (MCP) server for interacting with Sentry. This MCP server provides tools to interact with the Sentry API, allowing AI assistants to retrieve and analyze error data, manage projects, and monitor application performance.\n\n## Requirements\n\n- Node.js (v14 or higher)\n- npm or yarn\n- Sentry account with API access\n- Sentry authentication token with appropriate permissions\n\n## Setup\n\n1. Install dependencies:\n   ```\n   npm install\n   ```\n\n## Using this within an IDE \n\nThis MCP has been verified to work against Codeium Windsurf.\n\nCursor is currently having issues with its MCP implementation; and this tool is not yet fully functional.\n\n## Using with Claude\n\nTo use this MCP server with Claude, add the following configuration to your Claude settings:\n\n```json\n{\n    \"mcpServers\": {\n        \"sentry\": {\n            \"command\": \"npx\",\n            \"args\": [\"ts-node\", \"/Users/<your-user-directory>/mcp-sentry-ts/index.ts\"],\n            \"env\": {\n                \"SENTRY_AUTH\": \"<YOUR_AUTH_TOKEN>\"\n            }\n        }\n    }\n}\n```\n\n* Update with your directory path in the `args` field.\n* Replace `<YOUR_AUTH_TOKEN>` with your Sentry authentication token.\n\n## Available Tools\n\n### list_projects\n\nLists all accessible Sentry projects for a given organization.\n\n**Parameters:**\n- `organization_slug` (string, required): The slug of the organization to list projects from\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### resolve_short_id\n\nRetrieves details about an issue using its short ID.\n\n**Parameters:**\n- `organization_slug` (string, required): The slug of the organization the issue belongs to\n- `short_id` (string, required): The short ID of the issue to resolve (e.g., PROJECT-123)\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### get_sentry_event\n\nRetrieves and analyzes a specific Sentry event from an issue.\n\n**Parameters:**\n- `issue_id_or_url` (string, required): Either a full Sentry issue URL or just the numeric issue ID\n- `event_id` (string, required): The specific event ID to retrieve\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### list_error_events_in_project\n\nLists error events from a specific Sentry project.\n\n**Parameters:**\n- `organization_slug` (string, required): The slug of the organization the project belongs to\n- `project_slug` (string, required): The slug of the project to list events from\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### create_project\n\nCreates a new project in Sentry and retrieves its client keys.\n\n**Parameters:**\n- `organization_slug` (string, required): The slug of the organization to create the project in\n- `team_slug` (string, required): The slug of the team to assign the project to\n- `name` (string, required): The name of the new project\n- `platform` (string, optional): The platform for the new project\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### list_project_issues\n\nLists issues from a specific Sentry project.\n\n**Parameters:**\n- `organization_slug` (string, required): The slug of the organization the project belongs to\n- `project_slug` (string, required): The slug of the project to list issues from\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### list_issue_events\n\nLists events for a specific Sentry issue.\n\n**Parameters:**\n- `organization_slug` (string, required): The slug of the organization the issue belongs to\n- `issue_id` (string, required): The ID of the issue to list events from\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### get_sentry_issue\n\nRetrieves and analyzes a Sentry issue.\n\n**Parameters:**\n- `issue_id_or_url` (string, required): Either a full Sentry issue URL or just the numeric issue ID\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n### list_organization_replays\n\nLists replays from a specific Sentry organization.\n\n**Parameters:**\n- `organization_slug` (string, required): The slug of the organization to list replays from\n- `project_ids` (string[], optional): List of project IDs to filter replays by\n- `environment` (string, optional): Environment to filter replays by\n- `stats_period` (string, optional): Time period for stats (e.g., \"24h\", \"7d\")\n- `start` (string, optional): Start date for filtering replays\n- `end` (string, optional): End date for filtering replays\n- `sort` (string, optional): Field to sort replays by\n- `query` (string, optional): Search query to filter replays\n- `per_page` (number, optional): Number of replays per page\n- `cursor` (string, optional): Cursor for pagination\n- `view` (string, optional): View type, either \"summary\" or \"detailed\" (default: \"detailed\")\n- `format` (string, optional): Output format, either \"plain\" or \"markdown\" (default: \"markdown\")\n\n## Running the Server\n\n```\nnpx ts-node index.ts\n```\n\n## Authentication\n\nThis tool requires a Sentry authentication token with appropriate permissions to access the Sentry API. You can generate a token in your Sentry account settings under \"API Keys\".\n\n## Error Handling\n\nThe server includes comprehensive error handling for:\n- Missing authentication token\n- API request failures\n- Invalid parameters\n- Network errors\n\nAll errors are logged to the console for debugging.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sentry",
        "monitoring",
        "logging",
        "sentry api",
        "mcp sentry",
        "interact sentry"
      ],
      "category": "monitoring-and-logging"
    },
    "didlawowo--mcp-collection": {
      "owner": "didlawowo",
      "name": "mcp-collection",
      "url": "https://github.com/didlawowo/mcp-collection",
      "imageUrl": "/freedevtools/mcp/pfp/didlawowo.webp",
      "description": "Interact with Datadog API to fetch monitoring data and analyze specific monitor states and Kubernetes logs from infrastructure.",
      "stars": 8,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-08T09:11:18Z",
      "readme_content": "# Datadog Model Context Protocol (MCP) 🔍\n\n[![smithery badge](https://smithery.ai/badge/@didlawowo/mcp-collection)](https://smithery.ai/server/@didlawowo/mcp-collection)\n\nA Python-based tool to interact with Datadog API and fetch monitoring data from your infrastructure. This MCP provides easy access to monitor states and Kubernetes logs through a simple interface.\n\n## Datadog Features 🌟\n\n- **Monitor State Tracking**: Fetch and analyze specific monitor states\n- **Kubernetes Log Analysis**: Extract and format error logs from Kubernetes clusters\n\n## Prerequisites 📋\n\n- Python 3.11+\n- Datadog API and Application keys (with correct permissions)\n- Access to Datadog site\n\n## Installation 🔧\n\n### Installing via Smithery\n\nTo install Datadog for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@didlawowo/mcp-collection):\n\n```bash\nnpx -y @smithery/cli install @didlawowo/mcp-collection --client claude\n```\n\nRequired packages:\n\n```text\ndatadog-api-client\nfastmcp\nloguru\nicecream\npython-dotenv\nuv\n```\n\n## Environment Setup 🔑\n\nCreate a `.env` file with your Datadog credentials:\n\n```env\nDD_API_KEY=your_api_key\nDD_APP_KEY=your_app_key\n```\n\n## Setup Claude Desktop Setup for MCP 🖥️\n\n1. Install Claude Desktop\n\n```bash\n# Assuming you're on macOS\nbrew install claude-desktop\n\n# Or download from official website\nhttps://claude.ai/desktop\n```\n\n2. Set up Datadog MCP config:\n\n```bash\n# on mac is \n~/Library/Application\\ Support/Claude/claude_desktop_config.json\n\n\n# Add this to your claude config json\n```json\n    \"Datadog-MCP-Server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"datadog-api-client\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"icecream\",\n        \"--with\",\n        \"loguru\",\n        \"--with\",\n        \"python-dotenv\",\n        \"fastmcp\",\n        \"run\",\n        \"/your-path/mcp-collection/datadog/main.py\"\n      ],\n      \"env\": {\n        \"DD_API_KEY\": \"xxxx\",\n        \"DD_APP_KEY\": \"xxx\"\n      }\n    },\n```\n\n## Usage 💻\n\n![get logs](assets/logs.gif)\n\n![get monitor](assets/monitor.gif)\n\n## Architecture 🏗\n\n- **FastMCP Base**: Utilizes FastMCP framework for tool management\n- **Modular Design**: Separate functions for monitors and logs\n- **Type Safety**: Full typing support with Python type hints\n- **API Abstraction**: Wrapped Datadog API calls with error handling\n\nI'll add a section about MCP and Claude Desktop setup:\n\n# Model Context Protocol (MCP) Introduction 🤖\n\n## What is MCP?\n\nModel Context Protocol (MCP) is a framework allowing AI models to interact with external tools and APIs in a standardized way. It enables models like Claude to:\n\n- Access external data\n- Execute commands\n- Interact with APIs\n- Maintain context across conversations\n\n## some examples of MCP servers\n\n<https://github.com/punkpeye/awesome-mcp-servers?tab=readme-ov-file>\n\n## Tutorial for setup MCP\n\n<https://medium.com/@pedro.aquino.se/how-to-use-mcp-tools-on-claude-desktop-app-and-automate-your-daily-tasks-1c38e22bc4b0>\n\n## How it works - Available Functions 🛠️\n\nthe LLM use provided function to get the data and use it\n\n### 1. Get Monitor States\n\n```python\nget_monitor_states(\n    name: str,           # Monitor name to search\n    timeframe: int = 1   # Hours to look back\n)\n```\n\nExample:\n\n```python\n\nresponse = get_monitor_states(name=\"traefik\")\n\n# Sample Output\n{\n    \"id\": \"12345678\",\n    \"name\": \"traefik\",\n    \"status\": \"OK\",\n    \"query\": \"avg(last_5m):avg:traefik.response_time{*} > 1000\",\n    \"message\": \"Response time is too high\",\n    \"type\": \"metric alert\",\n    \"created\": \"2024-01-14T10:00:00Z\",\n    \"modified\": \"2024-01-14T15:30:00Z\"\n}\n```\n\n### 2. Get Kubernetes Logs\n\n```python\nget_k8s_logs(\n    cluster: str,            # Kubernetes cluster name\n    timeframe: int = 5,      # Hours to look back\n    namespace: str = None    # Optional namespace filter\n)\n```\n\nExample:\n\n```python\nlogs = get_k8s_logs(\n    cluster=\"prod-cluster\",\n    timeframe=3,\n    namespace=\"default\"\n)\n\n# Sample Output\n{\n    \"timestamp\": \"2024-01-14T22:00:00Z\",\n    \"host\": \"worker-1\",\n    \"service\": \"nginx-ingress\",\n    \"pod_name\": \"nginx-ingress-controller-abc123\",\n    \"namespace\": \"default\",\n    \"container_name\": \"controller\",\n    \"message\": \"Connection refused\",\n    \"status\": \"error\"\n}\n```\n\n```bash\n# Install as MCP extension\ncd datadog\ntask install-mcp\n```\n\n## 4. Verify Installation\n\n### In Claude chat desktop\n\n check datadog connection in claude\n\n![setup claude](assets/config.png)\n\n## 5. Use Datadog MCP Tools\n\n## Security Considerations 🔒\n\n- Store API keys in `.env`\n- MCP runs in isolated environment\n- Each tool has defined permissions\n- Rate limiting is implemented\n\n## Troubleshooting 🔧\n\n### Using MCP Inspector\n\n```bash\n# Launch MCP Inspector for debugging\ntask run-mcp-inspector\n```\n\nThe MCP Inspector provides:\n\n- Real-time view of MCP server status\n- Function call logs\n- Error tracing\n- API response monitoring\n\n### Common issues and solutions\n\n1. **API Authentication Errors**\n\n   ```bash\n   Error: (403) Forbidden\n   ```\n\n   ➡️ Check your DD_API_KEY and DD_APP_KEY in .env\n\n2. **MCP Connection Issues**\n\n   ```bash\n   Error: Failed to connect to MCP server\n   ```\n\n   ➡️ Verify your claude_desktop_config.json path and content\n\n3. **Monitor Not Found**\n\n   ```bash\n   Error: No monitor found with name 'xxx'\n   ```\n\n   ➡️ Check monitor name spelling and case sensitivity\n\n4. **logs can be found here**\n\n![alt text](assets/logs.png)\n\n## Contributing 🤝\n\nFeel free to:\n\n1. Open issues for bugs\n2. Submit PRs for improvements\n3. Add new features\n\n## Notes 📝\n\n- API calls are made to Datadog EU site\n- Default timeframe is 1 hour for monitor states\n- Page size limits are set to handle most use cases\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datadog",
        "kubernetes",
        "monitoring",
        "kubernetes logs",
        "datadog api",
        "monitoring logging"
      ],
      "category": "monitoring-and-logging"
    },
    "dynatrace-oss--dynatrace-mcp": {
      "owner": "dynatrace-oss",
      "name": "dynatrace-mcp",
      "url": "https://github.com/dynatrace-oss/dynatrace-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/dynatrace-oss.webp",
      "description": "Connect to the Dynatrace observability platform to retrieve real-time observability data, including production problems, security vulnerabilities, logs, and events. Utilize natural language queries to automate notifications and integrate monitoring within development workflows.",
      "stars": 148,
      "forks": 41,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T13:49:09Z",
      "readme_content": "# Dynatrace MCP Server\n\n<h4 align=\"center\">\n  <a href=\"https://github.com/dynatrace-oss/dynatrace-mcp/releases\">\n    <img src=\"https://img.shields.io/github/release/dynatrace-oss/dynatrace-mcp\" />\n  </a>\n  <a href=\"https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-mit-blue.svg\" alt=\"Dynatrace MCP Server is released under the MIT License\" />\n  </a>\n  <a href=\"https://www.npmjs.com/package/@dynatrace-oss/dynatrace-mcp-server\">\n    <img src=\"https://img.shields.io/npm/dm/@dynatrace-oss/dynatrace-mcp-server?logo=npm&style=flat&color=red\" alt=\"npm\" />\n  </a>\n  <a href=\"https://github.com/dynatrace-oss/dynatrace-mcp\">\n    <img src=\"https://img.shields.io/github/stars/dynatrace-oss/dynatrace-mcp\" alt=\"Dynatrace MCP Server Stars on GitHub\" />\n  </a>\n  <a href=\"https://github.com/dynatrace-oss/dynatrace-mcp\">\n    <img src=\"https://img.shields.io/github/contributors/dynatrace-oss/dynatrace-mcp?color=green\" alt=\"Dynatrace MCP Server Contributors on GitHub\" />\n  </a>\n</h4>\n\nThe local _Dynatrace MCP server_ allows AI Assistants to interact with the [Dynatrace](https://www.dynatrace.com/) observability platform,\nbringing real-time observability data directly into your development workflow.\n\n> Note: This product is not officially supported by Dynatrace.\n\nIf you need help, please contact us via [GitHub Issues](https://github.com/dynatrace-oss/dynatrace-mcp/issues) if you have feature requests, questions, or need help.\n\nhttps://github.com/user-attachments/assets/25c05db1-8e09-4a7f-add2-ed486ffd4b5a\n\n## Quickstart\n\nYou can add this MCP server to your MCP Client like VSCode, Claude, Cursor, Amazon Q, Windsurf, ChatGPT, or Github Copilot via the npmjs package `@dynatrace-oss/dynatrace-mcp-server`, and type `stdio`.\nYou can find more details about the configuration for different AI Assistants, Agents and MCP Clients in the [Configuration section below](#configuration).\n\nFurthermore, you need your Dynatrace environment URL, e.g., `https://abc12345.apps.dynatrace.com`, as well as a [Platform Token](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/platform-tokens), e.g., `dt0s16.SAMPLE.abcd1234`, with [required scopes](#scopes-for-authentication).\n\nDepending on your MCP Client, you need to configure these as environment variables or as settings in the UI:\n\n- `DT_ENVIRONMENT` (string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n- `DT_PLATFORM_TOKEN` (string, e.g., `dt0s16.SAMPLE.abcd1234`) - **Recommended**: Dynatrace Platform Token\n\nOnce you are done, we recommend looking into [example prompts](#-example-prompts-), like `Get all details of the entity 'my-service'` or `Show me error logs`. Please mind that these prompts lead to executing DQL statements which may incur [costs](#costs) in accordance to your licence.\n\n## Architecture\n\n![Architecture](https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/assets/dynatrace-mcp-arch.png?raw=true)\n\n## Use cases\n\n- **Real-time observability** - Fetch production-level data for early detection and proactive monitoring\n- **Contextual debugging** - Fix issues with full context from monitored exceptions, logs, and anomalies\n- **Security insights** - Get detailed vulnerability analysis and security problem tracking\n- **Natural language queries** - Use AI-powered DQL generation and explanation\n- **Multi-phase incident investigation** - Systematic 4-phase approach with automated impact assessment\n- **Advanced transaction analysis** - Precise root cause identification with file/line-level accuracy\n- **Cross-data source correlation** - Connect problems → spans → logs with trace ID correlation\n- **DevOps automation** - Deployment health gates with automated promotion/rollback logic\n- **Security compliance monitoring** - Multi-cloud compliance assessment with evidence-based investigation\n\n## Capabilities\n\n- List and get [problem](https://www.dynatrace.com/hub/detail/problems/) details from your services (for example Kubernetes)\n- List and get security problems / [vulnerability](https://www.dynatrace.com/hub/detail/vulnerabilities/) details\n- Execute DQL (Dynatrace Query Language) and retrieve logs, events, spans and metrics\n- Send Slack messages (via Slack Connector)\n- Set up notification Workflow (via Dynatrace [AutomationEngine](https://docs.dynatrace.com/docs/discover-dynatrace/platform/automationengine))\n- Get more information about a monitored entity\n- Get Ownership of an entity\n\n### Costs\n\n**Important:** While this local MCP server is provided for free, using certain capabilities to access data in Dynatrace Grail may incur additional costs based\non your Dynatrace consumption model. This affects `execute_dql` tool and other capabilities that **query** Dynatrace Grail storage, and costs\ndepend on the volume (GB scanned).\n\n**Before using this MCP server extensively, please:**\n\n1. Review your current Dynatrace consumption model and pricing\n2. Understand the cost implications of the specific data you plan to query (logs, events, metrics) - see [Dynatrace Pricing and Rate Card](https://www.dynatrace.com/pricing/)\n3. Start with smaller timeframes (e.g., 12h-24h) and make use of [buckets](https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model#built-in-grail-buckets) to reduce the cost impact\n4. Set an appropriate `DT_GRAIL_QUERY_BUDGET_GB` environment variable (default: 1000 GB) to control and monitor your Grail query consumption\n\n**Grail Budget Tracking:**\n\nThe MCP server includes built-in budget tracking for Grail queries to help you monitor and control costs:\n\n- Set `DT_GRAIL_QUERY_BUDGET_GB` (default: 1000 GB) to define your session budget limit\n- The server tracks bytes scanned across all Grail queries in the current session\n- You'll receive warnings when approaching 80% of your budget\n- Budget exceeded alerts help prevent unexpected high consumption\n- Budget resets when you restart the MCP server session\n\n**To understand costs that occured:**\n\nExecute the following DQL statement in a notebook to see how much bytes have been queried from Grail (Logs, Events, etc...):\n\n```\nfetch dt.system.events\n| filter event.kind == \"QUERY_EXECUTION_EVENT\" and contains(client.client_context, \"dynatrace-mcp\")\n| sort timestamp desc\n| fields timestamp, query_id, query_string, scanned_bytes, table, bucket, user.id, user.email, client.client_context\n| maketimeSeries sum(scanned_bytes), by: { user.email, user.id, table }\n```\n\n### AI-Powered Assistance (Preview)\n\n- **Natural Language to DQL** - Convert plain English queries to Dynatrace Query Language\n- **DQL Explanation** - Get plain English explanations of complex DQL queries\n- **AI Chat Assistant** - Get contextual help and guidance for Dynatrace questions\n- **Feedback System** - Provide feedback to improve AI responses over time\n\n> **Note:** While Davis CoPilot AI is generally available (GA), the Davis CoPilot APIs are currently in preview. For more information, visit the [Davis CoPilot Preview Community](https://dt-url.net/copilot-community).\n\n## 🎯 AI-Powered Observability Workshop Rules\n\nEnhance your AI assistant with comprehensive Dynatrace observability analysis capabilities through our streamlined workshop rules. These rules provide hierarchical workflows for security, compliance, incident response, and distributed systems investigation.\n\n### **🚀 Quick Setup for AI Assistants**\n\nCopy the comprehensive rule files from the [`dynatrace-agent-rules/rules/`](./dynatrace-agent-rules/rules/) directory to your AI assistant's rules directory:\n\n**IDE-Specific Locations:**\n\n- **Amazon Q**: `.amazonq/rules/` (project) or `~/.aws/amazonq/rules/` (global)\n- **Cursor**: `.cursor/rules/` (project) or via Settings → Rules (global)\n- **Windsurf**: `.windsurfrules/` (project) or via Customizations → Rules (global)\n- **Cline**: `.clinerules/` (project) or `~/Documents/Cline/Rules/` (global)\n- **GitHub Copilot**: `.github/copilot-instructions.md` (project only)\n\nThen initialize the agent in your AI chat:\n\n```\nload dynatrace mcp\n```\n\n### **🏗️ Enhanced Analysis Capabilities**\n\nThe workshop rules unlock advanced observability analysis modes:\n\n#### **🚨 Incident Response & Problem Investigation**\n\n- **4-phase structured investigation** workflow (Detection → Impact → Root Cause → Resolution)\n- **Cross-data source correlation** (problems → logs → spans → metrics)\n- **Kubernetes-aware incident analysis** with namespace and pod context\n- **User impact assessment** with Davis AI integration\n\n#### **📊 Comprehensive Data Investigation**\n\n- **Unified log-service-process analysis** in single workflow\n- **Business logic error detection** patterns\n- **Deployment correlation analysis** with ArgoCD/GitOps integration\n- **Golden signals monitoring** (Rate, Errors, Duration, Saturation)\n\n#### **🔗 Advanced Transaction Analysis**\n\n- **Precise root cause identification** with file/line numbers\n- **Exception stack trace analysis** with business context\n- **Multi-service cascade failure analysis**\n- **Performance impact correlation** across distributed systems\n\n#### **🛡️ Enhanced Security & Compliance**\n\n- **Latest-scan analysis** prevents outdated data aggregation\n- **Multi-cloud compliance** (AWS, Azure, GCP, Kubernetes)\n- **Evidence-based investigation** with detailed remediation paths\n- **Risk-based scoring** with team-specific guidance\n\n#### **⚡ DevOps Automation & SRE**\n\n- **Deployment health gates** with automated promotion/rollback\n- **SLO/SLI automation** with error budget calculations\n- **Infrastructure as Code remediation** with auto-generated templates\n- **Alert optimization workflows** with pattern recognition\n\n### **📁 Hierarchical Rule Architecture**\n\nThe rules are organized in a context-window optimized structure:\n\n```\nrules/\n├── DynatraceMcpIntegration.md                    # 🎯 MAIN ORCHESTRATOR\n├── workflows/                                    # 🔧 ANALYSIS WORKFLOWS\n│   ├── incidentResponse.md                       # Core incident investigation\n│   ├── DynatraceSecurityCompliance.md           # Security & compliance analysis\n│   ├── DynatraceDevOpsIntegration.md            # CI/CD automation\n│   └── dataSourceGuides/                        # 📊 DATA ANALYSIS GUIDES\n│       ├── dataInvestigation.md                 # Logs, services, processes\n│       └── DynatraceSpanAnalysis.md             # Transaction tracing\n└── reference/                                   # 📚 TECHNICAL DOCUMENTATION\n    ├── DynatraceQueryLanguage.md                # DQL syntax foundation\n    ├── DynatraceExplore.md                      # Field discovery patterns\n    ├── DynatraceSecurityEvents.md               # Security events schema\n    └── DynatraceProblemsSpec.md                 # Problems schema reference\n```\n\n**Key Architectural Benefits:**\n\n- **All files under 6,500 tokens** - Compatible with most LLM context limits\n- **Hierarchical organization** - Clear entry points and specialized guides\n- **Eliminated circular references** - No more confusing cross-referencing webs\n- **DQL-first approach** - Prefer flexible queries over rigid MCP calls\n\nFor detailed information about the workshop rules, see the [Rules README](./dynatrace-agent-rules/rules/README.md).\n\n## Configuration\n\nYou can add this MCP server (using STDIO) to your MCP Client like VS Code, Claude, Cursor, Amazon Q Developer CLI, Windsurf Github Copilot via the package `@dynatrace-oss/dynatrace-mcp-server`.\n\nWe recommend to always set it up for your current workspace instead of using it globally.\n\n**VS Code**\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"cwd\": \"${workspaceFolder}\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"envFile\": \"${workspaceFolder}/.env\"\n    }\n  }\n}\n```\n\nPlease note: In this config, [the `${workspaceFolder}` variable](https://code.visualstudio.com/docs/reference/variables-reference#_predefined-variables) is used.\nThis only works if the config is stored in the current workspaces, e.g., `<your-repo>/.vscode/mcp.json`. Alternatively, this can also be stored in user-settings, and you can define `env` as follows:\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_PLATFORM_TOKEN\": \"\",\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Claude Desktop**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_PLATFORM_TOKEN\": \"\",\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Amazon Q Developer CLI**\n\nThe [Amazon Q Developer CLI](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) provides an interactive chat experience directly in your terminal. You can ask questions, get help with AWS services, troubleshoot issues, and generate code snippets without leaving your command line environment.\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_PLATFORM_TOKEN\": \"\",\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\nThis configuration should be stored in `<your-repo>/.amazonq/mcp.json`.\n\n**Google Gemini CLI**\n\nThe [Google Gemini CLI](https://github.com/google-gemini/gemini-cli) is Google's official command-line AI assistant that supports MCP server integration. You can add the Dynatrace MCP server using either the built-in management commands or manual configuration.\n\nUsing `gemini` CLI directly (recommended):\n\n```bash\ngemini extensions install https://github.com/dynatrace-oss/dynatrace-mcp\nexport DT_PLATFORM_TOKEN=...\nexport DT_ENVIRONMENT=https://...\n```\n\nand verify that the server is running via\n\n```bash\ngemini mcp list\n```\n\nOr manually in your `~/.gemini/settings.json` or `.gemini/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace\": {\n      \"command\": \"npx\",\n      \"args\": [\"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_PLATFORM_TOKEN\": \"\",\n        \"DT_ENVIRONMENT\": \"\"\n      },\n      \"timeout\": 30000,\n      \"trust\": false\n    }\n  }\n}\n```\n\n### HTTP Server Mode (Alternative)\n\nFor scenarios where you need to run the MCP server as an HTTP service instead of using stdio (e.g., for stateful sessions, load balancing, or integration with web clients), you can use the HTTP server mode:\n\n**Running as HTTP server:**\n\n```bash\n# Get help and see all available options\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --help\n\n# Run with HTTP server on default port 3000\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http\n\n# Run with custom port (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --server -p 8080\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --port 3001\n\n# Run with custom host/IP (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --host 127.0.0.1\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http -H 192.168.0.1\n\n# Check version\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --version\n```\n\n**Configuration for MCP clients that support HTTP transport:**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-http\": {\n      \"url\": \"http://localhost:3000\",\n      \"transport\": \"http\"\n    }\n  }\n}\n```\n\n### Rule File\n\nFor efficient result retrieval from Dynatrace, please consider creating a rule file (e.g., [.github/copilot-instructions.md](https://docs.github.com/en/copilot/how-tos/configure-custom-instructions/add-repository-instructions), [.amazonq/rules/](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/context-project-rules.html)), instructing coding agents on how to get more details for your component/app/service. Here is an example for [easytrade](https://github.com/Dynatrace/easytrade), please adapt the names and filters to fit your use-cases and components:\n\n```\n# Observability\n\nWe use Dynatrace as an Observability solution. This document provides instructions on how to get data for easytrade from Dynatrace using DQL.\n\n## How to get any data for my App\n\nDepending on the query and tool used, the following filters can be applied to narrow down results:\n\n* `contains(entity.name, \"easytrade\")`\n* `contains(affected_entity.name, \"easytrade\")`\n* `contains(container.name, \"easytrade\")`\n\nFor best results, you can combine these filters with an `OR` operator.\n\n## Logs\n\nTo fetch logs for easytrade, execute `fetch logs | filter contains(container.name, \"easyatrade\")`.\nFor fetching just error-logs, add `| filter loglevel == \"ERROR\"`.\n```\n\n## Environment Variables\n\nYou can set up authentication via **Platform Tokens** (recommended) or **OAuth Client** via the following environment variables:\n\n- `DT_ENVIRONMENT` (string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n- `DT_PLATFORM_TOKEN` (string, e.g., `dt0s16.SAMPLE.abcd1234`) - **Recommended**: Dynatrace Platform Token\n- `OAUTH_CLIENT_ID` (string, e.g., `dt0s02.SAMPLE`) - Alternative: Dynatrace OAuth Client ID (for advanced use cases)\n- `OAUTH_CLIENT_SECRET` (string, e.g., `dt0s02.SAMPLE.abcd1234`) - Alternative: Dynatrace OAuth Client Secret (for advanced use cases)\n- `DT_GRAIL_QUERY_BUDGET_GB` (number, default: `1000`) - Budget limit in GB (base 1000) for Grail query bytes scanned per session. The MCP server tracks your Grail usage and warns when approaching or exceeding this limit.\n\n**Platform Tokens are recommended** for most use cases as they provide a simpler authentication flow. OAuth Clients should only be used when specific OAuth features are required.\n\nFor more information, please have a look at the documentation about\n[creating a Platform Token in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/platform-tokens), as well as\n[creating an OAuth Client in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/oauth-clients) for advanced scenarios.\n\nIn addition, depending on the features you use, the following variables can be configured:\n\n- `SLACK_CONNECTION_ID` (string) - connection ID of a [Slack Connection](https://docs.dynatrace.com/docs/analyze-explore-automate/workflows/actions/slack)\n\n### Scopes for Authentication\n\nDepending on the features you are using, the following scopes are needed:\n\n**Available for both Platform Tokens and OAuth Clients:**\n\n- `app-engine:apps:run` - needed for almost all tools\n- `app-engine:functions:run` - needed for for almost all tools\n- `environment-api:entities:read` - for retrieving ownership details from monitored entities (_currently not available for Platform Tokens_)\n- `automation:workflows:read` - read Workflows\n- `automation:workflows:write` - create and update Workflows\n- `automation:workflows:run` - run Workflows\n- `storage:buckets:read` - needed for `execute_dql` tool to read all system data stored on Grail\n- `storage:logs:read` - needed for `execute_dql` tool to read logs for reliability guardian validations\n- `storage:metrics:read` - needed for `execute_dql` tool to read metrics for reliability guardian validations\n- `storage:bizevents:read` - needed for `execute_dql` tool to read bizevents for reliability guardian validations\n- `storage:spans:read` - needed for `execute_dql` tool to read spans from Grail\n- `storage:entities:read` - needed for `execute_dql` tool to read Entities from Grail\n- `storage:events:read` - needed for `execute_dql` tool to read Events from Grail\n- `storage:security.events:read`- needed for `execute_dql` tool to read Security Events from Grail\n- `storage:system:read` - needed for `execute_dql` tool to read System Data from Grail\n- `storage:user.events:read` - needed for `execute_dql` tool to read User events from Grail\n- `storage:user.sessions:read` - needed for `execute_dql` tool to read User sessions from Grail\n- `davis-copilot:conversations:execute` - execute conversational skill (chat with Copilot)\n- `davis-copilot:nl2dql:execute` - execute Davis Copilot Natural Language (NL) to DQL skill\n- `davis-copilot:dql2nl:execute` - execute DQL to Natural Language (NL) skill\n- `email:emails:send` - needed for `send_email` tool to send emails\n- `settings:objects:read` - needed for reading ownership information and Guardians (SRG) from settings\n\n  **Note**: Please ensure that `settings:objects:read` is used, and _not_ the similarly named scope `app-settings:objects:read`.\n\n**Important**: Some features requiring `environment-api:entities:read` will only work with OAuth Clients. For most use cases, Platform Tokens provide all necessary functionality.\n\n## ✨ Example prompts ✨\n\nUse these example prompts as a starting point. Just copy them into your IDE or agent setup, adapt them to your services/stack/architecture,\nand extend them as needed. They're here to help you imagine how real-time observability and automation work together in the MCP context in your IDE.\n\n### **Basic Queries & AI Assistance**\n\n**Find a monitored entity**\n\n```\nGet all details of the entity 'my-service'\n```\n\n**Find error logs**\n\n```\nShow me error logs\n```\n\n**Write a DQL query from natural language:**\n\n```\nShow me error rates for the payment service in the last hour\n```\n\n**Explain a DQL query:**\n\n```\nWhat does this DQL do?\nfetch logs | filter dt.source_entity == 'SERVICE-123' | summarize count(), by:{severity} | sort count() desc\n```\n\n**Chat with Davis CoPilot:**\n\n```\nHow can I investigate slow database queries in Dynatrace?\n```\n\n**Send email notifications:**\n\n```\nSend an email notification about the incident to the responsible team at team@example.com with CC to manager@example.com\n```\n\n### **Advanced Incident Investigation**\n\n**Multi-phase incident response:**\n\n```\nOur checkout service is experiencing high error rates. Start a systematic 4-phase incident investigation:\n1. Detect and triage the active problems\n2. Assess user impact and affected services\n3. Perform cross-data source analysis (problems → spans → logs)\n4. Identify root cause with file/line-level precision\n```\n\n**Cross-service failure analysis:**\n\n```\nWe have cascading failures across our microservices architecture.\nAnalyze the entity relationships and trace the failure propagation from the initial problem\nthrough all downstream services. Show me the correlation timeline.\n```\n\n### **Security & Compliance Analysis**\n\n**Latest-scan vulnerability assessment:**\n\n```\nPerform a comprehensive security analysis using the latest scan data:\n- Check for new vulnerabilities in our production environment\n- Focus on critical and high-severity findings\n- Provide evidence-based remediation paths\n- Generate risk scores with team-specific guidance\n```\n\n**Multi-cloud compliance monitoring:**\n\n```\nRun a compliance assessment across our AWS, Azure, and Kubernetes environments.\nCheck for configuration drift and security posture changes in the last 24 hours.\n```\n\n### **DevOps & SRE Automation**\n\n**Deployment health gate analysis:**\n\n```\nOur latest deployment is showing performance degradation.\nRun deployment health gate analysis with:\n- Golden signals monitoring (Rate, Errors, Duration, Saturation)\n- SLO/SLI validation with error budget calculations\n- Generate automated rollback recommendation if needed\n```\n\n**Infrastructure as Code remediation:**\n\n```\nGenerate Infrastructure as Code templates to remediate the current alert patterns.\nInclude automated scaling policies and resource optimization recommendations.\n```\n\n### **Deep Transaction Analysis**\n\n**Business logic error investigation:**\n\n```\nOur payment processing is showing intermittent failures.\nPerform advanced transaction analysis:\n- Extract exception details with full stack traces\n- Correlate with deployment events and ArgoCD changes\n- Identify the exact code location causing the issue\n```\n\n**Performance correlation analysis:**\n\n```\nAnalyze the performance impact across our distributed system for the slow checkout flow.\nShow me the complete trace analysis with business context and identify bottlenecks.\n```\n\n### **Traditional Use Cases (Enhanced)**\n\n**Find open vulnerabilities on production, setup alert:**\n\n```\nI have this code snippet here in my IDE, where I get a dependency vulnerability warning for my code.\nCheck if I see any open vulnerability/cve on production.\nAnalyze a specific production problem.\nSetup a workflow that sends Slack alerts to the #devops-alerts channel when availability problems occur.\n```\n\n**Debug intermittent 503 errors:**\n\n```\nOur load balancer is intermittently returning 503 errors during peak traffic.\nPull all recent problems detected for our front-end services and\nrun a query to correlate error rates with service instance health indicators.\nI suspect we have circuit breakers triggering, but need confirmation from the telemetry data.\n```\n\n**Correlate memory issue with logs:**\n\n```\nThere's a problem with high memory usage on one of our hosts.\nGet the problem details and then fetch related logs to help understand\nwhat's causing the memory spike? Which file in this repo is this related to?\n```\n\n**Trace request flow analysis:**\n\n```\nOur users are experiencing slow checkout processes.\nCan you execute a DQL query to show me the full request trace for our checkout flow,\nso I can identify which service is causing the bottleneck?\n```\n\n**Analyze Kubernetes cluster events:**\n\n```\nOur application deployments seem to be failing intermittently.\nCan you fetch recent events from our \"production-cluster\"\nto help identify what might be causing these deployment issues?\n```\n\n## Troubleshooting\n\n### Authentication Issues\n\nIn most cases, authentication issues are related to missing scopes or invalid tokens. Please ensure that you have added all required scopes as listed above.\n\n**For Platform Tokens:**\n\n1. Verify your Platform Token has all the necessary scopes listed in the \"Scopes for Authentication\" section\n2. Ensure your token is valid and not expired\n3. Check that your user has the required permissions in your Dynatrace Environment\n\n**For OAuth Clients:**\nIn case of OAuth-related problems, you can troubleshoot SSO/OAuth issues based on our [Dynatrace Developer Documentation](https://developer.dynatrace.com/develop/access-platform-apis-from-outside/#get-bearer-token-and-call-app-function).\n\nIt is recommended to test access with the following API (which requires minimal scopes `app-engine:apps:run` and `app-engine:functions:run`):\n\n1. Use OAuth Client ID and Secret to retrieve a Bearer Token (only valid for a couple of minutes):\n\n```bash\ncurl --request POST 'https://sso.dynatrace.com/sso/oauth2/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode 'grant_type=client_credentials' \\\n  --data-urlencode 'client_id={your-client-id}' \\\n  --data-urlencode 'client_secret={your-client-secret}' \\\n  --data-urlencode 'scope=app-engine:apps:run app-engine:functions:run'\n```\n\n2. Use `access_token` from the response of the above call as the bearer-token in the next call:\n\n```bash\ncurl -X GET https://abc12345.apps.dynatrace.com/platform/management/v1/environment \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer {your-bearer-token}'\n```\n\n3. You should retrieve a result like this:\n\n```json\n{\n  \"environmentId\": \"abc12345\",\n  \"createTime\": \"2023-01-01T00:10:57.123Z\",\n  \"blockTime\": \"2025-12-07T00:00:00Z\",\n  \"state\": \"ACTIVE\"\n}\n```\n\n### Problem accessing data on Grail\n\nGrail has a dedicated section about permissions in the Dynatrace Docs. Please refer to https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model/assign-permissions-in-grail for more details.\n\n## Telemetry\n\nThe Dynatrace MCP Server includes sending Telemetry Data via Dynatrace OpenKit to help improve the product. This includes:\n\n- Server start events\n- Tool usage (which tools are called, success/failure, execution duration)\n- Error tracking for debugging and improvement\n\n**Privacy and Opt-out:**\n\n- Telemetry is **enabled by default** but can be disabled by setting `DT_MCP_DISABLE_TELEMETRY=true`\n- No sensitive data from your Dynatrace environment is tracked\n- Only anonymous usage statistics and error information are collected\n- Usage statistics and error data are transmitted to Dynatrace’s analytics endpoint\n\n**Configuration options:**\n\n- `DT_MCP_DISABLE_TELEMETRY` (boolean, default: `false`) - Disable Telemetry\n- `DT_MCP_TELEMETRY_APPLICATION_ID` (string, default: `dynatrace-mcp-server`) - Application ID for tracking\n- `DT_MCP_TELEMETRY_ENDPOINT_URL` (string, default: Dynatrace endpoint) - OpenKit endpoint URL\n- `DT_MCP_TELEMETRY_DEVICE_ID` (string, default: auto-generated) - Device identifier for tracking\n\nTo disable usage tracking, add this to your environment:\n\n```bash\nDT_MCP_DISABLE_TELEMETRY=true\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dynatrace",
        "logging",
        "monitoring",
        "logging dynatrace",
        "monitoring development",
        "oss dynatrace"
      ],
      "category": "monitoring-and-logging"
    },
    "ghrud92--simple-loki-mcp": {
      "owner": "ghrud92",
      "name": "simple-loki-mcp",
      "url": "https://github.com/ghrud92/simple-loki-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ghrud92.webp",
      "description": "Query and analyze Grafana Loki logs with full LogQL support to access log data directly from AI assistants. Simplifies log management by providing formatted results in various output formats and supporting metadata retrieval.",
      "stars": 5,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-18T22:04:21Z",
      "readme_content": "# Simple Loki MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@ghrud92/simple-loki-mcp)](https://smithery.ai/server/@ghrud92/simple-loki-mcp)\n\nLoki MCP Server is a [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/mcp) interface for querying Grafana Loki logs using `logcli`. The server enables AI assistants to access and analyze log data from Loki directly.\n\n<a href=\"https://glama.ai/mcp/servers/@ghrud92/loki-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ghrud92/loki-mcp/badge\" alt=\"Loki Server MCP server\" />\n</a>\n\n## Features\n\n- Query Loki logs with full LogQL support\n- Get label values and metadata\n- Authentication and configuration support via environment variables or config files\n- Provides formatted results in different output formats (default, raw, JSON lines)\n- Automatic fallback to HTTP API when `logcli` is not available in the environment\n\n## Prerequisites\n\n- Node.js v16 or higher\n- TypeScript\n- (Optional) [Grafana Loki logcli](https://grafana.com/docs/loki/latest/tools/logcli/) installed and accessible in your PATH. If `logcli` is not available, the server will automatically use the Loki HTTP API instead\n- Access to a Loki server instance\n\n## Installation\n\n### Installing via Smithery\n\nTo install Simple Loki MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ghrud92/simple-loki-mcp):\n\n```bash\nnpx -y @smithery/cli install @ghrud92/simple-loki-mcp --client claude\n```\n\n### for MCP\n\n```json\n{\n  \"mcpServers\": {\n    \"simple-loki\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"simple-loki-mcp\"],\n      \"env\": {\n        \"LOKI_ADDR\": \"https://loki.sup.band\"\n      }\n    }\n  }\n}\n```\n\n### npm\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/ghrud92/loki-mcp.git\ncd loki-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n## Available MCP Tools\n\n### query-loki\n\nQuery logs from Loki with filtering options.\n\nParameters:\n\n- `query` (required): Loki query string (LogQL)\n- `from`: Start timestamp (e.g. \"2023-01-01T12:00:00Z\")\n- `to`: End timestamp (e.g. \"2023-01-01T13:00:00Z\")\n- `limit`: Maximum number of logs to return\n- `batch`: Batch size for query results\n- `output`: Output format (\"default\", \"raw\", or \"jsonl\")\n- `quiet`: Suppress query metadata\n- `forward`: Display results in chronological order\n\n### get-label-values\n\nRetrieve all values for a specific label.\n\nParameters:\n\n- `label` (required): Label name to get values for\n\n### get-labels\n\nRetrieve all available labels.\n\nNo parameters required.\n\n## Configuration\n\nYou can configure Loki access using:\n\n### Environment Variables\n\n- `LOKI_ADDR`: Loki server address (URL)\n- `LOKI_USERNAME`: Username for basic auth\n- `LOKI_PASSWORD`: Password for basic auth\n- `LOKI_TENANT_ID`: Tenant ID for multi-tenant Loki\n- `LOKI_BEARER_TOKEN`: Bearer token for authentication\n- `LOKI_BEARER_TOKEN_FILE`: File containing bearer token\n- `LOKI_CA_FILE`: Custom CA file for TLS\n- `LOKI_CERT_FILE`: Client certificate file for TLS\n- `LOKI_KEY_FILE`: Client key file for TLS\n- `LOKI_ORG_ID`: Organization ID for multi-org setups\n- `LOKI_TLS_SKIP_VERIFY`: Skip TLS verification (\"true\" or \"false\")\n- `LOKI_CONFIG_PATH`: Custom path to config file\n- `DEBUG`: Enable debug logging\n\n> **Note**: When the client is using the HTTP API mode (when `logcli` is not available), the same configuration parameters are used to authenticate and connect to the Loki server.\n\n### Config Files\n\nAlternatively, create a `logcli-config.yaml` file in one of these locations:\n\n- Custom path specified by `LOKI_CONFIG_PATH`\n- Current working directory\n- Your home directory (`~/.logcli-config.yaml`)\n\nExample config file:\n\n```yaml\naddr: https://loki.example.com\nusername: user\npassword: pass\ntenant_id: mytenant\n```\n\n## Usage\n\nStart the server:\n\n```bash\nnpm start\n```\n\nFor development:\n\n```bash\nnpm run dev\n```\n\n## Implementation Details\n\n### Automatic Fallback to HTTP API\n\nThe server will automatically check if `logcli` is installed and available in the environment:\n\n1. If `logcli` is available, it will be used for all queries, providing the full functionality of the CLI tool\n2. If `logcli` is not available, the server will automatically fall back to using the Loki HTTP API:\n   - No additional configuration is needed\n   - The same authentication parameters are used for the HTTP API\n   - Response formatting is consistent with the CLI output\n   - Default limit of 1000 logs per query is applied in both modes\n\nThis automatic detection ensures that the server works seamlessly in different environments without manual configuration.\n\n## Development\n\n```bash\n# Run linter\nnpm run lint\n\n# Fix linting issues\nnpm run lint:fix\n\n# Run tests\nnpm run test\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logging",
        "logs",
        "logql",
        "log data",
        "loki logs",
        "monitoring logging"
      ],
      "category": "monitoring-and-logging"
    },
    "imprvhub--mcp-status-observer": {
      "owner": "imprvhub",
      "name": "mcp-status-observer",
      "url": "https://github.com/imprvhub/mcp-status-observer",
      "imageUrl": "/freedevtools/mcp/pfp/imprvhub.webp",
      "description": "Monitor and query the operational status of major digital platforms in real-time, providing detailed information about specific services and components. Enables users to stay informed about platform health and respond to service disruptions promptly.",
      "stars": 5,
      "forks": 5,
      "license": "Mozilla Public License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-14T04:13:39Z",
      "readme_content": "# MCP Status Observer\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/d7d5a94b-3378-479b-b5a3-35efa8904d2e) [![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/imprvhub/mcp-status-observer)](https://archestra.ai/mcp-catalog/imprvhub__mcp-status-observer)\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-status-observer)](https://smithery.ai/server/@imprvhub/mcp-status-observer)\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"padding: 15px; vertical-align: middle; border: none; text-align: center;\">\n  <a href=\"https://mseep.ai/app/imprvhub-mcp-status-observer\">\n    <img src=\"https://mseep.net/pr/imprvhub-mcp-status-observer-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" />\n  </a>\n</td>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">An integration that allows Claude Desktop to monitor and query the operational status of major digital platforms including AI providers, cloud services, and developer tools using the Model Context Protocol (MCP).</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\">\n  <a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-status-observer\">\n    <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-status-observer/badge\" alt=\"Status Observer MCP server\" />\n  </a>\n</td>\n\n</tr>\n</table>\n\n> [!IMPORTANT]\n> This project is continuously updated with new platform integrations. If you're not seeing a service that should be available, or if Claude doesn't recognize a platform, please update by running `npm run build` from a freshly cloned repository. \n> \n> **Last updated**: 2025-09-12T07:22:15Z (UTC) - Added OpenRouter status integration with RSS incident tracking\n\n## Features\n\n- Monitor world's most used digital platforms (GitHub, Slack, Discord, etc.)\n- Track AI providers including OpenRouter, OpenAI, Anthropic, and Gemini\n- Get detailed status information for specific services with incident history\n- Check status of specific components within each platform\n- Real-time updates of service status with impact analysis\n- Comprehensive incident tracking with resolution status and timelines\n- Simple query interface with commands like `status --openrouter`\n\n## Demo\n\n<p>\n  <a href=\"https://www.youtube.com/watch?v=EV1ac0PMzKg\">\n    <img src=\"public/assets/preview.png\" width=\"600\" alt=\"Status Observer MCP Demo\">\n  </a>\n</p>\n\n<details>\n<summary> Timestamps </summary>\n\nClick on any timestamp to jump to that section of the video\n\n[**00:00**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=0s) - **LinkedIn Platform Status Assessment**  \nComprehensive analysis of LinkedIn's operational health, including detailed examination of core services such as LinkedIn.com, LinkedIn Learning, Campaign Manager, Sales Navigator, Recruiter, and Talent solutions. All systems confirmed fully operational with zero service disruptions.\n\n[**00:20**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=20s) - **GitHub Infrastructure Status Overview**  \nDetailed evaluation of GitHub's service availability, covering critical components including Git operations, API requests, Actions, Webhooks, Issues, Pull Requests, Packages, Pages, Codespaces, and Copilot functionality. Complete operational status confirmed across all GitHub services.\n\n[**00:40**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=40s) - **Vercel Platform Reliability Analysis**  \nIn-depth examination of Vercel's global edge network and deployment infrastructure, featuring comprehensive status reporting on core services such as API, Dashboard, Builds, Serverless Functions, Edge Functions, and global CDN locations. All Vercel services verified operational across all regions.\n\n[**01:08**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=68s) - **Cloudflare Network Status Examination**  \nExtensive analysis of Cloudflare's global infrastructure status, detailing service availability across geographic regions and specific service components. Identified performance degradation in multiple regions (Africa, Asia, Europe, Latin America, Middle East, North America) while core services remain functional. Includes detailed assessment of regional data centers under maintenance and technical impact analysis.\n\n[**01:46**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=106s) - **Global Operational Status Report**  \nConsolidated overview of operational status across all major technology platforms and service providers, highlighting both fully operational services (GitHub, Vercel, Netlify, Asana, Atlassian, OpenRouter, etc.) and services experiencing degraded performance (Cloudflare, Twilio). Includes strategic recommendations for organizations with dependencies on affected services.\n</details>\n\n## Requirements\n\n- Node.js 16 or higher\n- Claude Desktop\n- Internet connection to access status APIs\n\n## Installation\n\n### Installing Manually\n1. Clone or download this repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-status-observer\ncd mcp-status-observer\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Running the MCP Server\n\nThere are two ways to run the MCP server:\n\n### Option 1: Running manually\n\n1. Open a terminal or command prompt\n2. Navigate to the project directory\n3. Run the server directly:\n\n```bash\nnode build/index.js\n```\n\nKeep this terminal window open while using Claude Desktop. The server will run until you close the terminal.\n\n### Option 2: Auto-starting with Claude Desktop (recommended for regular use)\n\nThe Claude Desktop can automatically start the MCP server when needed. To set this up:\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the Status Observer MCP configuration. If the file doesn't exist, create it:\n\n```json\n{\n  \"mcpServers\": {\n    \"statusObserver\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_DIRECTORY/mcp-status-observer/build/index.js\"]\n    }\n  }\n}\n```\n\n**Important**: Replace `ABSOLUTE_PATH_TO_DIRECTORY` with the **complete absolute path** where you installed the MCP\n  - macOS/Linux example: `/Users/username/mcp-status-observer`\n  - Windows example: `C:\\\\Users\\\\username\\\\mcp-status-observer`\n\nIf you already have other MCPs configured, simply add the \"statusObserver\" section inside the \"mcpServers\" object. Here's an example of a configuration with multiple MCPs:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp1\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"otherMcp2\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"statusObserver\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"ABSOLUTE_PATH_TO_DIRECTORY/mcp-status-observer/build/index.js\"\n      ]\n    }\n  }\n}\n```\n\nThe MCP server will automatically start when Claude Desktop needs it, based on the configuration in your `claude_desktop_config.json` file.\n\n## Usage\n\n1. Restart Claude Desktop after modifying the configuration\n2. In Claude, use the `status` command to interact with the Status Observer MCP Server\n3. The MCP server runs as a subprocess managed by Claude Desktop\n\n## Available Commands\n\nThe Status Observer MCP provides a single tool named `status` with several commands:\n\n| Command | Description | Parameters | Example |\n|---------|-------------|------------|---------|\n| `list` | List all available platforms | None | `status list` |\n| `--[platform]` | Get status for a specific platform | Platform name | `status --openrouter` |\n| `--all` | Get status for all platforms | None | `status --all` |\n\n## Supported Platforms\n\nThe Status Observer monitors 22 major digital platforms across various categories:\n\n### AI & Machine Learning (4)\n- **OpenRouter** - AI model routing and access platform\n- **OpenAI** - Leading AI services provider (ChatGPT, DALL-E, API)\n- **Anthropic** - AI assistant provider (Claude)\n- **Gemini** - Google's multimodal AI platform\n\n### Cloud Infrastructure (4)\n- **Google Cloud Platform** - Comprehensive cloud computing services\n- **DigitalOcean** - Developer-focused cloud infrastructure\n- **Vercel** - Frontend deployment and edge platform\n- **Netlify** - Web development and deployment platform\n\n### Developer Tools & Platforms (5)\n- **Docker** - Container platform and services\n- **GitHub** - Version control and collaboration platform\n- **npm** - JavaScript package manager and registry\n- **Atlassian** - Developer collaboration tools (Jira, Bitbucket, Confluence)\n- **Supabase** - Open source backend platform (PostgreSQL, auth, storage)\n\n### Productivity & Collaboration (5)\n- **LinkedIn** - Professional networking platform\n- **Slack** - Business communication and collaboration\n- **Asana** - Team workflow and project management\n- **Dropbox** - Cloud file storage and collaboration\n- **X (Twitter)** - Social media and real-time communication\n\n### Web Infrastructure & Security (3)\n- **Cloudflare** - Web infrastructure, CDN, and security\n- **Discord** - Developer community and communication platform\n- **Reddit** - Social news and developer community platform\n\n### Analytics & Business Tools (1)\n- **Amplitude** - Product analytics platform\n\n## Example Usage\n\nHere are various examples of how to use the Status Observer with Claude:\n\n### Direct Commands:\n\n```\n# AI Platforms\nstatus --openrouter\nstatus --openai\nstatus --anthropic\nstatus --gemini\n\n# Cloud Infrastructure\nstatus --gcp\nstatus --vercel\nstatus --digitalocean\nstatus --netlify\n\n# Developer Tools\nstatus --docker\nstatus --github\nstatus --atlassian\nstatus --supabase\nstatus --npm\n\n# Productivity & Social\nstatus --linkedin\nstatus --slack\nstatus --x\nstatus --dropbox\n\n# Web Infrastructure\nstatus --cloudflare\nstatus --discord\n\n# All platforms\nstatus --all\nstatus list\n```\n\n### Preview\n![OpenRouter Status Monitoring Preview](https://github.com/imprvhub/mcp-status-observer/raw/main/public/assets/openrouter.png)\n![GCP Status Monitoring Preview](https://github.com/imprvhub/mcp-status-observer/raw/main/public/assets/gcp.png)\n\n### Natural Language Prompts:\n\nYou can also interact with the MCP using natural language. Claude will interpret these requests and use the appropriate commands:\n\n- \"Could you check if OpenRouter is having any API issues right now?\"\n- \"What's the status of OpenAI's ChatGPT service?\"\n- \"Has there been any recent incidents with Claude or the Anthropic API?\"\n- \"Is Google Cloud Platform experiencing any outages in my region?\"\n- \"Check if Docker Hub is operational for automated builds\"\n- \"What's the current status of LinkedIn's Sales Navigator?\"\n- \"Can you tell me if Google's Gemini AI is experiencing any service disruptions?\"\n- \"Show me the status of all AI platforms including OpenRouter and OpenAI\"\n- \"Are there any active incidents affecting GitHub Actions or Git operations?\"\n- \"Check the overall health of Vercel and Netlify for my deployment pipeline\"\n- \"Has Supabase had any recent database or authentication issues?\"\n- \"What's the status of all major platforms right now?\"\n\n\n## Troubleshooting\n\n### \"Server disconnected\" error\nIf you see the error \"MCP Status Observer: Server disconnected\" in Claude Desktop:\n\n1. **Verify the server is running**:\n   - Open a terminal and manually run `node build/index.js` from the project directory\n   - If the server starts successfully, use Claude while keeping this terminal open\n\n2. **Check your configuration**:\n   - Ensure the absolute path in `claude_desktop_config.json` is correct for your system\n   - Double-check that you've used double backslashes (`\\\\`) for Windows paths\n   - Verify you're using the complete path from the root of your filesystem\n\n### Tools not appearing in Claude\nIf the Status Observer tools don't appear in Claude:\n- Make sure you've restarted Claude Desktop after configuration\n- Check the Claude Desktop logs for any MCP communication errors\n- Ensure the MCP server process is running (run it manually to confirm)\n- Verify that the MCP server is correctly registered in the Claude Desktop MCP registry\n\n### Checking if the server is running\nTo check if the server is running:\n\n- **Windows**: Open Task Manager, go to the \"Details\" tab, and look for \"node.exe\"\n- **macOS/Linux**: Open Terminal and run `ps aux | grep node`\n\nIf you don't see the server running, start it manually or use the auto-start method.\n\n## Contributing\n\n### Adding New Status APIs\n\nContributors can easily add support for additional platforms by modifying the `initializePlatforms` method in `src/index.ts`. The process is straightforward:\n\n1. Identify a platform's status API endpoint\n2. Add a new entry using the `addPlatform` method with the following parameters:\n   - `id`: A unique identifier for the platform (lowercase, no spaces)\n   - `name`: The display name of the platform\n   - `url`: The status API endpoint URL\n   - `description`: A brief description of the platform\n\nExample:\n```typescript\nthis.addPlatform('newservice', 'New Service', 'https://status.newservice.com/api/v2/summary.json', 'Description of the service');\n```\n\n### Custom API Integration\n\nFor platforms with non-standard status pages (like OpenRouter, OpenAI, Anthropic), you can create custom handlers:\n\n1. Add the platform to `initializePlatforms()`\n2. Create a TypeScript interface for the response format\n3. Add a specific handler method like `getOpenRouterStatus()`\n4. Update the main `getPlatformStatus()` method to route to your handler\n5. Add quick status support in `getQuickPlatformStatus()`\n\nExample structure for custom handlers:\n```typescript\nprivate async getCustomPlatformStatus(platform: PlatformStatus): Promise<string> {\n  // Custom parsing logic for your platform\n  // Return formatted status text\n}\n```\n\n### Platform Categories\n\nWhen adding new platforms, consider organizing them into logical categories:\n- **AI/ML**: OpenRouter, OpenAI, Anthropic, Gemini\n- **Cloud Infrastructure**: GCP, AWS, Azure, DigitalOcean\n- **Developer Tools**: GitHub, GitLab, Docker, npm\n- **Productivity**: Slack, Microsoft 365, Google Workspace\n- **Web Infrastructure**: Cloudflare, Fastly, Akamai\n\n## License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-claude-hackernews/blob/main/LICENSE) file for details.\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)\n- [MCP Series](https://github.com/mcp-series)\n\n## Changelog\n\n- **2025-09-12**: Added OpenRouter integration with RSS incident tracking and detailed impact analysis\n- **2025-04-26**: Added Docker status integration with comprehensive component monitoring\n- **2025-03-15**: Enhanced GCP regional status reporting with incident correlation\n- **2025-02-28**: Added Anthropic and Gemini AI platform monitoring\n- **2025-01-20**: Initial release with core platform support (GitHub, Vercel, Cloudflare, etc.)\n\n---\n\n*Built for the developer community by [imprvhub](https://github.com/imprvhub)*",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "imprvhub",
        "logging",
        "imprvhub mcp",
        "logging imprvhub",
        "status observer"
      ],
      "category": "monitoring-and-logging"
    },
    "jakenuts--mcp-solarwinds": {
      "owner": "jakenuts",
      "name": "mcp-solarwinds",
      "url": "https://github.com/jakenuts/mcp-solarwinds",
      "imageUrl": "/freedevtools/mcp/pfp/jakenuts.webp",
      "description": "Access and visualize SolarWinds Observability logs with advanced filtering capabilities and generate insights through real-time log analysis and reporting. Supports timestamped log entry retrieval, including advanced filtering options by group and entity.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-28T20:37:09Z",
      "readme_content": "# SolarWinds Logs MCP Server\n\nA Model Context Protocol (MCP) server for accessing and visualizing SolarWinds Observability logs.\n\n## Note - \n\nThis server is currently incomplete as it does not support \nstructured data search (a limitation of the REST API?). I'm\nuncertain if it also needs to accept a data center to use\nin the api endpoint calls. Will address both when time allows \n(needed it for a real work problem, have to fix that first)\n\n### Tools\n\n#### search_logs\nSearch SolarWinds Observability logs with optional filtering\n- Takes search parameters including filter, time range, and pagination options\n- Returns formatted log entries with timestamps, hostnames, and messages\n- Supports advanced filtering by group, entity, and more\n- Default search range is the last 24 hours\n\n#### visualize_logs\nGenerate a histogram json response for of log events\n- Formatted for Claude and canvas representations\n- Configurable time intervals (minute, hour, day)\n- Supports UTC or local time zones\n- Customizable query filters and time ranges\n- Default visualization range is the last 24 hours\n\n### Resources\n\n#### SolarWinds Log Search\n- URI Template: `solarwinds://{query}/search`\n- Returns log entries matching the specified query\n- Example: `solarwinds://error/search`\n\n## Installation\n\nOptionally install from npm:\n```bash\nnpm install -g mcp-solarwinds\n```\n\nOr clone and build from source:\n```bash\ngit clone https://github.com/@jakenuts/mcp-solarwinds.git\ncd mcp-solarwinds\nnpm install\nnpm run build\n```\nOr just use npx in your configurations\n\n### For Cline VSCode Extension\n\nAdd to `%APPDATA%/Code - Insiders/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"solarwinds\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-solarwinds\"],\n      \"env\": {\n        \"SOLARWINDS_API_TOKEN\": \"your-api-token\"\n      },\n      \"autoApprove\": [\"search_logs\", \"visualize_logs\"]\n    }\n  }\n}\n```\n\n### For Claude Desktop\n\nAdd to the appropriate config file:\n\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"solarwinds\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-solarwinds\"],\n      \"env\": {\n        \"SOLARWINDS_API_TOKEN\": \"your-api-token\"\n      }\n    }\n  }\n}\n```\n\n### Special Windows Configuration\n\nIf you encounter the ENOENT spawn npx issue on Windows, use this alternative configuration that specifies the full paths:\n\n```json\n{\n  \"mcpServers\": {\n    \"solarwinds\": {\n      \"command\": \"C:\\\\Users\\\\[username]\\\\AppData\\\\Roaming\\\\nvm\\\\[node-version]\\\\node.exe\",\n      \"args\": [\n        \"C:\\\\Users\\\\[username]\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\npm\\\\bin\\\\npx-cli.js\",\n        \"-y\",\n        \"mcp-solarwinds\"\n      ],\n      \"env\": {\n        \"SOLARWINDS_API_TOKEN\": \"your-api-token\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\nThe SolarWinds Observability MCP server requires an API token to authenticate with the SolarWinds Observability API.\n\n### Configuration Methods\n\nThere are multiple ways to provide the API token:\n\n1. **MCP Settings Configuration (Recommended)**: Configure the token in your MCP settings file\n2. **Environment Variable**: Set the `SOLARWINDS_API_TOKEN` environment variable\n3. **Local .env File (For Testing)**: Create a `.env` file in the project root with `SOLARWINDS_API_TOKEN=your-token`\n\nFor local testing, you can:\n1. Copy `.env.example` to `.env` and add your token\n2. Run the example script: `node examples/local-test.js`\n\n## Tool Usage Examples\n\n### search_logs\n\nBasic search:\n```json\n{\n  \"filter\": \"error\"\n}\n```\n\nAdvanced search with time range and pagination:\n```json\n{\n  \"filter\": \"error\",\n  \"entityId\": \"web-server\",\n  \"startTime\": \"2025-03-01T00:00:00Z\",\n  \"endTime\": \"2025-03-05T23:59:59Z\",\n  \"pageSize\": 100,\n  \"direction\": \"backward\"\n}\n```\n\n### visualize_logs\n\nBasic histogram (ASCII chart):\n```json\n{\n  \"filter\": \"error\",\n  \"interval\": \"hour\"\n}\n```\n\nAdvanced visualization (ASCII chart):\n```json\n{\n  \"filter\": \"error\",\n  \"entityId\": \"web-server\",\n  \"startTime\": \"2025-03-01T00:00:00Z\",\n  \"endTime\": \"2025-03-05T23:59:59Z\",\n  \"interval\": \"day\",\n  \"use_utc\": true\n}\n```\n\nClaude visualization (JSON format):\n```json\n{\n  \"filter\": \"error\",\n  \"interval\": \"hour\",\n  \"format\": \"json\"\n}\n```\n\nThe JSON format returns data that Claude can visualize as a chart:\n```json\n{\n  \"timeRanges\": [\"12:02\", \"12:03\", \"12:04\", \"12:05\", \"12:06\", \"12:07\", \"12:08\", \"12:09\"],\n  \"counts\": [261, 47, 48, 48, 31, 262, 270, 33],\n  \"total\": 1000,\n  \"queryParams\": {\n    \"query\": \"error\",\n    \"startTime\": \"2025-03-05T00:00:00.000Z\",\n    \"endTime\": \"2025-03-05T23:59:59.000Z\"\n  }\n}\n```\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. The MCP Inspector provides helpful debugging tools:\n\n```bash\nnpm run debug:inspector\n```\n\nThis will provide a URL to access the inspector in your browser, where you can:\n- View all MCP messages\n- Inspect request/response payloads\n- Test tools interactively\n- Monitor server state\n\nFor local testing without the MCP framework:\n```bash\n# Create a .env file with your token\ncp .env.example .env\n# Edit .env to add your token\n# Run the example script\nnode examples/local-test.js\n```\n\n## Technical Details\n\n- Built with TypeScript and the MCP SDK\n- Uses axios for API communication\n- Supports ISO 8601 date formats for time ranges\n- Generates ASCII histograms for log visualization\n- Default search range: last 24 hours\n- Default page size: 50 logs\n- Supports multiple authentication methods\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logging",
        "logs",
        "solarwinds",
        "monitoring logging",
        "observability logs",
        "solarwinds observability"
      ],
      "category": "monitoring-and-logging"
    },
    "jkosik--mcp-server-splunk": {
      "owner": "jkosik",
      "name": "mcp-server-splunk",
      "url": "https://github.com/jkosik/mcp-server-splunk",
      "imageUrl": "/freedevtools/mcp/pfp/jkosik.webp",
      "description": "Implements MCP Tools for managing Splunk resources including saved searches, alerts, and fired alerts. Supports STDIO and SSE transports for flexible deployment and interaction with Splunk data.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Go",
      "updated_at": "2025-10-01T13:32:44Z",
      "readme_content": "# MCP Server for Splunk\n\nA Go implementation of the MCP server for Splunk.\nSupports STDIO and SSE (Server-Sent Events HTTP API). Uses github.com/mark3labs/mcp-go SDK.\n\n## MCP Tools implemented\n- `list_splunk_saved_searches`\n    - Parameters:\n        - `count` (number, optional): Number of results to return (max 100, default 100)\n        - `offset` (number, optional): Offset for pagination (default 0)\n- `list_splunk_alerts`\n    - Parameters:\n        - `count` (number, optional): Number of results to return (max 100, default 10)\n        - `offset` (number, optional): Offset for pagination (default 0)\n        - `title` (string, optional): Case-insensitive substring to filter alert titles\n- `list_splunk_fired_alerts`\n    - Parameters:\n        - `count` (number, optional): Number of results to return (max 100, default 10)\n        - `offset` (number, optional): Offset for pagination (default 0)\n        - `ss_name` (string, optional): Search name pattern to filter alerts (default \"*\")\n        - `earliest` (string, optional): Time range to look back (default \"-24h\")\n- `list_splunk_indexes`\n    - Parameters:\n        - `count` (number, optional): Number of results to return (max 100, default 10)\n        - `offset` (number, optional): Offset for pagination (default 0)\n- `list_splunk_macros`\n    - Parameters:\n        - `count` (number, optional): Number of results to return (max 100, default 10)\n        - `offset` (number, optional): Offset for pagination (default 0)\n\n## MCP Prompts and Resources\n- `internal/splunk/prompt.go` implements an MCP Prompt to find Splunk alerts for a specific keyword (e.g. GitHub or OKTA) and instructs Cursor to utilise multiple MCP tools to review all Splunk alerts, indexes and macros first to provide the best answer.\n- `cmd/mcp/server/main.go` implements MCP Resource in the form of local CSV file with Splunk related content, providing further context to the chat.\n\n## Usage\n### STDIO mode (default)\n```bash\nexport SPLUNK_URL=https://your-splunk-instance:8089\nexport SPLUNK_TOKEN=your-splunk-token\n\n# List available tools\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}' | go run cmd/mcp-server-splunk/main.go | jq\n\n# Call list_splunk_saved_searches tool\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/call\",\"params\":{\"name\":\"list_splunk_saved_searches\",\"arguments\":{}}}' | go run cmd/mcp-server-splunk/main.go | jq\n```\n\n## SSE mode (Server-Sent Events HTTP API)\n```bash\nexport SPLUNK_URL=https://your-splunk-instance:8089\nexport SPLUNK_TOKEN=your-splunk-token\n\n# Start the server\ngo run cmd/mcp-server-splunk/main.go -transport sse -port 3001\n\n# Call the server and get Session ID from the output. Do not terminate the session.\ncurl http://localhost:3001/sse\n\n# Keep session running and and use different terminal window for the final MCP call\ncurl -X POST \"http://localhost:3001/message?sessionId=YOUR_SESSION_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}' | jq\n```\n\n## Installing via Smithery\n[![smithery badge](https://smithery.ai/badge/@jkosik/mcp-server-splunk)](https://smithery.ai/server/@jkosik/mcp-server-splunk)\n\n`Dockerfile` and `smithery.yaml` are used to support hosting this MCP server at [Smithery](https://smithery.ai/server/@jkosik/.\n\n\n### Local Docker build and run\n```\ndocker build -t mcp-server-splunk .\n\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}' | \\\ndocker run --rm -i \\\n  -e SPLUNK_URL=https://your-splunk-instance:8089 \\\n  -e SPLUNK_TOKEN=your-splunk-token \\\n  mcp-server-splunk | jq\n```\n\n## Cursor integration\nBy configuring MCP Settings in Cursor, you can include remote data directly into the LLM context.\n\n![Demo](docs/mcp-short.gif)\n\nIntegrate STDIO or SSE MCP Servers (see below) and use Cursor Chat.\nCursor will automatically try to use MCP Tools, Prompts or Re\nSample prompts:\n- `How many MCP tools for Splunk are available?`\n- `How many Splunk indexes do we have?`\n- `Can you list first 5 Splunk macros including underlying queries?`\n- `How many alers with \"Alert_CRITICAL\" in the name were fired in the last day?`\n- `Read the MCP Resource \"Data Dictionary\" and find the contact person for the Splunk index XYZ.`\n\n### STDIO mode\nBuild the server:\n```\ngo build -o cmd/mcp-server-splunk/mcp-server-splunk cmd/mcp-server-splunk/main.go\n```\n\nUpdate `~/.cursor/mcp.json`\n```json\n{\n  \"mcpServers\": {\n    \"splunk_stdio\": {\n      \"name\": \"Splunk MCP Server (STDIO)\",\n      \"description\": \"MCP server for Splunk integration\",\n      \"type\": \"stdio\",\n      \"command\": \"/Users/juraj/data/github.com/jkosik/mcp-server-splunk/cmd/mcp-server-splunk/mcp-server-splunk\",\n      \"env\": {\n        \"SPLUNK_URL\": \"https://your-splunk-instance:8089\",\n        \"SPLUNK_TOKEN\": \"your-splunk-token\"\n      }\n    }\n  }\n}\n```\n\n### SSE mode\nStart the server:\n```bash\nexport SPLUNK_URL=https://your-splunk-instance:8089\nexport SPLUNK_TOKEN=your-splunk-token\n\n# Start the server\ngo run cmd/mcp-server-splunk/main.go -transport sse -port 3001\n```\n\nUpdate `~/.cursor/mcp.json`\n```json\n{\n  \"mcpServers\": {\n    \"splunk_sse\": {\n      \"name\": \"Splunk MCP Server (SSE)\",\n      \"description\": \"MCP server for Splunk integration (SSE mode)\",\n      \"type\": \"sse\",\n      \"url\": \"http://localhost:3001/sse\"\n    }\n  }\n}\n```\n\n_Certified by MCP Review: https://mcpreview.com/mcp-servers/jkosik/mcp-server-splunk_\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "splunk",
        "logging",
        "monitoring",
        "server splunk",
        "logging jkosik",
        "splunk implements"
      ],
      "category": "monitoring-and-logging"
    },
    "klara-research--MCP-Analyzer": {
      "owner": "klara-research",
      "name": "MCP-Analyzer",
      "url": "https://github.com/klara-research/MCP-Analyzer",
      "imageUrl": "/freedevtools/mcp/pfp/klara-research.webp",
      "description": "Analyze and debug Model Context Protocol logs from various platforms. Filter, paginate, and retrieve log entries for troubleshooting and understanding tool interactions.",
      "stars": 13,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-05T18:47:21Z",
      "readme_content": "# MCP Server: Analyze & Debug MCP Logs\n\n[![smithery badge](https://smithery.ai/badge/@klara-research/MCP-Analyzer)](https://smithery.ai/server/@klara-research/MCP-Analyzer)\n\n<div align=\"center\">\n  <img src=\"assets/mcp_logs.png\" width=\"400\">\n  \n  <br>\n  <br>\n  <br>\n  🔍 <b>Read logs from standard locations across all platforms</b>\n  <br>\n  <br>\n  🔎 <b>Filter, paginate, and analyze large log collections</b>\n  <br>\n  <br>\n</div>\n\n## 🎯 Overview\n\nMCP Log Reader is a specialized MCP server that helps you analyze and debug Model Context Protocol logs. It provides Claude with direct access to log files, making it easy to troubleshoot MCP integrations and understand how Claude interacts with your tools.\n\n- **Multi-platform Support**: Works on macOS, Windows, and Linux with platform-specific log paths\n- **Smart Filtering**: Find specific log entries with case-insensitive text search\n- **Paginated Browsing**: Navigate large log collections efficiently\n- **Size Management**: Handles large log files with intelligent truncation\n- **Seamless Claude Integration**: Works directly with Claude Desktop\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install MCP Log Reader for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@klara-research/MCP-Analyzer):\n\n```bash\nnpx -y @smithery/cli install @klara-research/MCP-Analyzer --client claude\n```\n\n### Installing Manually\n\nInstall directly from GitHub:\n```bash\n# Clone the repository\ngit clone https://github.com/klara-research/MCP-Analyzer.git\ncd MCP-Analyzer\n\n# Install dependencies\nnpm i\n```\n\nBuild and run:\n```bash\n# Compile TypeScript\nnpx tsc\n```\n\n## 🔌 Connecting to Claude\n\nAdd the server to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"log-reader\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/MCP-Analyzer/build\"\n      ]\n    }\n  }\n}\n```\n\nThen restart Claude Desktop.\n\n## 📋 Available Parameters\n\nThe log reader supports these parameters:\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `lines` | Number of lines to read from each log file | 100 |\n| `filter` | Text to filter log entries by (case-insensitive) | \"\" |\n| `customPath` | Custom path to log directory | OS-specific |\n| `fileLimit` | Maximum number of files to read per page | 5 |\n| `page` | Page number for pagination | 1 |\n\n## 💡 Example Usage\n\nAsk Claude to use the log reader tool:\n\n```\nCan you check my MCP logs for any connection errors in the last day?\n```\n\nOr with specific parameters:\n\n```\nCan you look through MCP logs with filter=\"error\" and lines=50 to find initialization issues?\n```\n\n## ⚙️ How It Works\n\n1. The server automatically detects your OS and finds the appropriate log directory\n2. It locates all MCP log files and sorts them by modification time (newest first)\n3. The requested page of log files is retrieved based on pagination settings\n4. Files are processed with size limits to prevent overwhelming responses\n5. Filtered content is returned in a structured format with pagination details\n\n## 📄 License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logging",
        "debug",
        "logs",
        "protocol logs",
        "mcp analyzer",
        "analyze debug"
      ],
      "category": "monitoring-and-logging"
    },
    "last9--last9-mcp-server": {
      "owner": "last9",
      "name": "last9-mcp-server",
      "url": "https://github.com/last9/last9-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/last9.webp",
      "description": "Integrates real-time production context including logs, metrics, and traces into local development environments for code auto-fixing. Supports various IDEs and allows for retrieving exceptions and service graphs related to those exceptions.",
      "stars": 46,
      "forks": 8,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-02T21:44:46Z",
      "readme_content": "# Last9 MCP Server\n\n![last9 mcp demo](mcp-demo.gif)\n\nA [Model Context Protocol](https://modelcontextprotocol.io/) server\nimplementation for [Last9](https://last9.io/mcp/) that enables AI agents to\nseamlessly bring real-time production context — logs, metrics, and traces — into\nyour local environment to auto-fix code faster.\n\n- [View demo](https://www.youtube.com/watch?v=AQH5xq6qzjI)\n- Read our\n  [announcement blog post](https://last9.io/blog/launching-last9-mcp-server/)\n\n## Status\n\nWorks with Claude desktop app, or Cursor, Windsurf, and VSCode (Github Copilot)\nIDEs. Implements the following MCP\n[tools](https://modelcontextprotocol.io/docs/concepts/tools):\n\n**Observability & APM Tools:**\n\n- `get_exceptions`: Get the list of exceptions.\n- `get_service_summary`: Get service summary with throughput, error rate, and response time.\n- `get_service_environments`: Get available environments for services.\n- `get_service_performance_details`: Get detailed performance metrics for a service.\n- `get_service_operations_summary`: Get operations summary for a service.\n- `get_service_dependency_graph`: Get service dependency graph showing incoming/outgoing dependencies.\n\n**Prometheus/PromQL Tools:**\n\n- `prometheus_range_query`: Execute PromQL range queries for metrics data.\n- `prometheus_instant_query`: Execute PromQL instant queries for metrics data.\n- `prometheus_label_values`: Get label values for PromQL queries.\n- `prometheus_labels`: Get available labels for PromQL queries.\n\n**Logs Management:**\n\n- `get_logs`: Get logs filtered by service name and/or severity level.\n- `get_drop_rules`: Get drop rules for logs that determine what logs get\n  filtered out at [Last9 Control Plane](https://last9.io/control-plane)\n- `add_drop_rule`: Create a drop rule for logs at\n  [Last9 Control Plane](https://last9.io/control-plane)\n- `get_service_logs`: Get raw log entries for a specific service over a time range. Can apply filters on severity and body.\n- `get_log_attributes`: Get available log attributes (labels) for a specified time window.\n\n**Traces Management:**\n\n- `get_service_traces`: Query traces for a specific service with filtering options for span kinds, status codes, and other trace attributes.\n- `get_trace_attributes`: Get available trace attributes (series) for a specified time window.\n\n**Change Events:**\n\n- `get_change_events`: Get change events from the last9_change_events prometheus metric over a given time range.\n\n**Alert Management:**\n\n- `get_alert_config`: Get alert configurations (alert rules) from Last9.\n- `get_alerts`: Get currently active alerts from Last9 monitoring system.\n\n## Tools Documentation\n\n### get_exceptions\n\nRetrieves server-side exceptions over a specified time range.\n\nParameters:\n\n- `limit` (integer, optional): Maximum number of exceptions to return.\n  Default: 20.\n- `lookback_minutes` (integer, recommended): Number of minutes to look back from\n  now. Default: 60. Examples: 60, 30, 15.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD\n  HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD\n  HH:MM:SS). Leave empty to default to current time.\n- `span_name` (string, optional): Name of the span to filter by.\n\n### get_service_summary\n\nGet service summary over a given time range. Includes service name, environment, throughput, error rate, and response time. All values are p95 quantiles over the time range.\n\nParameters:\n\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to end_time_iso - 1 hour.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### get_service_environments\n\nGet available environments for services. Returns an array of environments that can be used with other APM tools.\n\nParameters:\n\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to end_time_iso - 1 hour.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\nNote: All other APM tools that retrieve service information (like `get_service_performance_details`, `get_service_dependency_graph`, `get_service_operations_summary`, `get_service_summary`) require an `env` parameter. This parameter must be one of the environments returned by this tool. If this tool returns an empty array, use an empty string `\"\"` for the env parameter.\n\n### get_service_performance_details\n\nGet detailed performance metrics for a specific service over a given time range.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get performance details for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### get_service_operations_summary\n\nGet a summary of operations inside a service over a given time range. Returns operations like HTTP endpoints, database queries, messaging producer and HTTP client calls.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get operations summary for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### get_service_dependency_graph\n\nGet details of the throughput, response times and error rates of incoming, outgoing and infrastructure components of a service. Useful for analyzing cascading effects of errors and performance issues.\n\nParameters:\n\n- `service_name` (string, optional): Name of the service to get the dependency graph for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### prometheus_range_query\n\nPerform a Prometheus range query to get metrics data over a specified time range. Recommended to check available labels first using `prometheus_labels` tool.\n\nParameters:\n\n- `query` (string, required): The range query to execute.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### prometheus_instant_query\n\nPerform a Prometheus instant query to get metrics data at a specific point in time. Typically should use rollup functions like sum_over_time, avg_over_time, quantile_over_time over a time window.\n\nParameters:\n\n- `query` (string, required): The instant query to execute.\n- `time_iso` (string, optional): Time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### prometheus_label_values\n\nReturn the label values for a particular label and PromQL filter query. Similar to Prometheus /label_values call.\n\nParameters:\n\n- `match_query` (string, required): A valid PromQL filter query.\n- `label` (string, required): The label to get values for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### prometheus_labels\n\nReturn the labels for a given PromQL match query. Similar to Prometheus /labels call.\n\nParameters:\n\n- `match_query` (string, required): A valid PromQL filter query.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### get_logs\n\nGets logs filtered by service name and/or severity level within a specified time range. This tool now uses the advanced v2 logs API with physical index optimization for better performance.\n\n**Note**: This tool now requires a `service_name` parameter and internally uses the same advanced infrastructure as `get_service_logs`.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get logs for.\n- `severity` (string, optional): Severity of the logs to get (automatically converted to severity_filters format).\n- `lookback_minutes` (integer, recommended): Number of minutes to look back from now. Default: 60. Examples: 60, 30, 15.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `limit` (integer, optional): Maximum number of logs to return. Default: 20.\n- `env` (string, optional): Environment to filter by. Use \"get_service_environments\" tool to get available environments.\n\n### get_drop_rules\n\nGets drop rules for logs, which determine what logs get filtered out from\nreaching Last9.\n\n### add_drop_rule\n\nAdds a new drop rule to filter out specific logs at\n[Last9 Control Plane](https://last9.io/control-plane)\n\nParameters:\n\n- `name` (string, required): Name of the drop rule.\n- `filters` (array, required): List of filter conditions to apply. Each filter\n  has:\n  - `key` (string, required): The key to filter on. Only attributes and\n    resource.attributes keys are supported. For resource attributes, use format:\n    resource.attributes[key_name] and for log attributes, use format:\n    attributes[key_name] Double quotes in key names must be escaped.\n  - `value` (string, required): The value to filter against.\n  - `operator` (string, required): The operator used for filtering. Valid\n    values:\n    - \"equals\"\n    - \"not_equals\"\n  - `conjunction` (string, required): The logical conjunction between filters.\n    Valid values:\n    - \"and\"\n\n### get_alert_config\n\nGet alert configurations (alert rules) from Last9. Returns all configured alert rules including their conditions, labels, and annotations.\n\nParameters:\n\nNone - This tool retrieves all available alert configurations.\n\nReturns information about:\n\n- Alert rule ID and name\n- Primary indicator being monitored\n- Current state and severity\n- Algorithm used for alerting\n- Entity ID and organization details\n- Properties and configuration\n- Creation and update timestamps\n- Group timeseries notification settings\n\n### get_alerts\n\nGet currently active alerts from Last9 monitoring system. Returns all alerts that are currently firing or have fired recently within the specified time window.\n\nParameters:\n\n- `timestamp` (integer, optional): Unix timestamp for the query time. Leave empty to default to current time.\n- `window` (integer, optional): Time window in seconds to look back for alerts. Defaults to 900 seconds (15 minutes). Range: 60-86400 seconds.\n\nReturns information about:\n\n- Alert rule details (ID, name, group, type)\n- Current state and severity\n- Last fired timestamp and duration\n- Rule properties and configuration\n- Alert instances with current values\n- Metric degradation information\n- Group labels and annotations for each instance\n\n### get_service_logs\n\nGet raw log entries for a specific service over a time range. This tool retrieves actual log entries including log messages, timestamps, severity levels, and other metadata. Useful for debugging issues, monitoring service behavior, and analyzing specific log patterns.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get logs for.\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now. Default: 60 minutes. Examples: 60, 30, 15.\n- `limit` (integer, optional): Maximum number of log entries to return. Default: 20.\n- `env` (string, optional): Environment to filter by. Use \"get_service_environments\" tool to get available environments.\n- `severity_filters` (array, optional): Array of severity patterns to filter logs (e.g., [\"error\", \"warn\"]). Uses OR logic.\n- `body_filters` (array, optional): Array of message content patterns to filter logs (e.g., [\"timeout\", \"failed\"]). Uses OR logic.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\nFiltering behavior:\n- Multiple filter types are combined with AND logic (service AND severity AND body)\n- Each filter array uses OR logic (matches any pattern in the array)\n\nExamples:\n- service_name=\"api\" + severity_filters=[\"error\"] + body_filters=[\"timeout\"] → finds error logs containing \"timeout\"\n- service_name=\"web\" + body_filters=[\"timeout\", \"failed\", \"error 500\"] → finds logs containing any of these patterns\n\n### get_log_attributes\n\nGet available log attributes (labels) for a specified time window. This tool retrieves all attribute names that exist in logs during the specified time range, which can be used for filtering and querying logs.\n\nParameters:\n\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now for the time window. Default: 15. Examples: 15, 30, 60.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `region` (string, optional): AWS region to query. Leave empty to use default from configuration. Examples: ap-south-1, us-east-1, eu-west-1.\n\nReturns:\n- List of log attributes grouped into two categories:\n  - Log Attributes: Standard log fields like service, severity, body, level, etc.\n  - Resource Attributes: Resource-related fields prefixed with \"resource_\" like resource_k8s.pod.name, resource_service.name, etc.\n\n### get_service_traces\n\nQuery traces for a specific service with filtering options for span kinds, status codes, and other trace attributes. This tool retrieves distributed tracing data for debugging performance issues, understanding request flows, and analyzing service interactions.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get traces for.\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now. Default: 60 minutes. Examples: 60, 30, 15.\n- `limit` (integer, optional): Maximum number of traces to return. Default: 10.\n- `env` (string, optional): Environment to filter by. Use \"get_service_environments\" tool to get available environments.\n- `span_kind` (array, optional): Filter by span types (server, client, internal, consumer, producer).\n- `span_name` (string, optional): Filter by specific span name.\n- `status_code` (array, optional): Filter by trace status (ok, error, unset, success).\n- `order` (string, optional): Field to order traces by. Default: \"Duration\". Options: Duration, Timestamp.\n- `direction` (string, optional): Sort direction. Default: \"backward\". Options: forward, backward.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\nFiltering options:\n- Combine multiple filters to narrow down specific traces of interest\n- Use time range filters with lookback_minutes or explicit start/end times\n\nExamples:\n- service_name=\"api\" + span_kind=[\"server\"] + status_code=[\"error\"] → finds failed server-side traces\n- service_name=\"payment\" + span_name=\"process_payment\" + lookback_minutes=30 → finds payment processing traces from last 30 minutes\n\n### get_trace_attributes\n\nGet available trace attributes (series) for a specified time window. This tool retrieves all attribute names that exist in traces during the specified time range, which can be used for filtering and querying traces.\n\nParameters:\n\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now for the time window. Default: 15. Examples: 15, 30, 60.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `region` (string, optional): AWS region to query. Leave empty to use default from configuration. Examples: ap-south-1, us-east-1, eu-west-1.\n\nReturns:\n- An alphabetically sorted list of all available trace attributes (e.g., http.method, http.status_code, db.name, resource_service.name, duration, etc.)\n\n### get_change_events\n\nGet change events from the last9_change_events prometheus metric over a given time range. Returns change events that occurred in the specified time window, including deployments, configuration changes, and other system modifications.\n\nParameters:\n\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now. Default: 60 minutes. Examples: 60, 30, 15.\n- `service` (string, optional): Name of the service to filter change events for.\n- `environment` (string, optional): Environment to filter by.\n- `event_name` (string, optional): Name of the change event to filter by (use available_event_names to see valid values).\n\nReturns:\n- `available_event_names`: List of all available event types that can be used for filtering\n- `change_events`: Array of timeseries data with metric labels and timestamp-value pairs\n- `count`: Total number of change events returned\n- `time_range`: Start and end time of the query window\n\nEach change event includes:\n- `metric`: Map of metric labels (service_name, env, event_type, message, etc.)\n- `values`: Array of timestamp-value pairs representing the timeseries data\n\nCommon event types include: deployment, config_change, rollback, scale_up/scale_down, restart, upgrade/downgrade, maintenance, backup/restore, health_check, certificate, database.\n\nBest practices:\n1. First call without event_name to get available_event_names\n2. Use exact event name from available_event_names for the event_name parameter\n3. Combine with other filters (service, environment, time) for precise results\n\n## Installation\n\nYou can install and run the Last9 Observability MCP server in several ways:\n\n### Local Installation\n\nFor local development and traditional STDIO usage:\n\n#### Homebrew\n\n```bash\n# Add the Last9 tap\nbrew tap last9/tap\n\n# Install the Last9 MCP CLI\nbrew install last9-mcp\n```\n\n#### NPM\n\n```bash\n# Install globally\nnpm install -g @last9/mcp-server\n\n# Or run directly with npx\nnpx @last9/mcp-server\n```\n\n## Configuration\n\n### Environment Variables\n\nThe Last9 MCP server requires the following environment variables:\n\n- `LAST9_BASE_URL`: (required) Last9 API URL from\n  [OTel integration](https://app.last9.io/integrations?integration=OpenTelemetry)\n- `LAST9_AUTH_TOKEN`: (required) Authentication token for Last9 MCP server from\n  [OTel integration](https://app.last9.io/integrations?integration=OpenTelemetry)\n- `LAST9_REFRESH_TOKEN`: (required) Refresh Token with Write permissions, needed\n  for accessing control plane APIs from\n  [API Access](https://app.last9.io/settings/api-access)\n\n## Usage\n\n## Usage with Claude Desktop\n\nConfigure the Claude app to use the MCP server:\n\n1. Open the Claude Desktop app, go to Settings, then Developer\n2. Click Edit Config\n3. Open the `claude_desktop_config.json` file\n4. Copy and paste the server config to your existing file, then save\n5. Restart Claude\n\n### If installed via Homebrew:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"/opt/homebrew/bin/last9-mcp\",\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@last9/mcp-server\"],\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n## Usage with Cursor\n\nConfigure Cursor to use the MCP server:\n\n1. Open Cursor, go to Settings, then Cursor Settings\n2. Select MCP on the left\n3. Click Add \"New Global MCP Server\" at the top right\n4. Copy and paste the server config to your existing file, then save\n5. Restart Cursor\n\n### If installed via Homebrew:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"/opt/homebrew/bin/last9-mcp\",\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@last9/mcp-server\"],\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n## Usage with Windsurf\n\nConfigure Windsurf to use the MCP server:\n\n1. Open Windsurf, go to Settings, then Developer\n2. Click Edit Config\n3. Open the `windsurf_config.json` file\n4. Copy and paste the server config to your existing file, then save\n5. Restart Windsurf\n\n### If installed via Homebrew:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"/opt/homebrew/bin/last9-mcp\",\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@last9/mcp-server\"],\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n## Usage with VS Code\n\n> Note: MCP support in VS Code is available starting v1.99 and is currently in\n> preview. For advanced configuration options and alternative setup methods,\n> [view the VS Code MCP documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n1. Open VS Code, go to Settings, select the User tab, then Features, then Chat\n2. Click \"Edit settings.json\"\n3. Copy and paste the server config to your existing file, then save\n4. Restart VS Code\n\n### If installed via Homebrew:\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"last9\": {\n        \"type\": \"stdio\",\n        \"command\": \"/opt/homebrew/bin/last9-mcp\",\n        \"env\": {\n          \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n          \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n          \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n        }\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"last9\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@last9/mcp-server\"],\n        \"env\": {\n          \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n          \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n          \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Development\n\nFor local development and testing, you can run the MCP server in HTTP mode which makes it easier to debug requests and responses.\n\n### Running in HTTP Mode\n\nSet the `HTTP_MODE` environment variable to enable HTTP server mode:\n\n```bash\n# Export required environment variables\nexport LAST9_API_TOKEN=\"your_api_token\"\nexport LAST9_BASE_URL=\"https://your-last9-endpoint\"  # Your Last9 endpoint\nexport HTTP_MODE=true\nexport HTTP_PORT=8080  # Optional, defaults to 8080\n\n# Run the server\n./last9-mcp-server\n```\n\nThe server will start on `http://localhost:8080/mcp` and you can test it with curl:\n\n### Testing with curl\n\n```bash\n# Test get_service_logs\ncurl -X POST http://localhost:8080/mcp \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Mcp-Session-Id: session_$(date +%s)000000000\" \\\n    -d '{\n      \"jsonrpc\": \"2.0\",\n      \"id\": 1,\n      \"method\": \"tools/call\",\n      \"params\": {\n        \"name\": \"get_service_logs\",\n        \"arguments\": {\n          \"service_name\": \"your-service-name\",\n          \"lookback_minutes\": 30,\n          \"limit\": 10\n        }\n      }\n    }'\n\n# Test get_service_traces\ncurl -X POST http://localhost:8080/mcp \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Mcp-Session-Id: session_$(date +%s)000000000\" \\\n    -d '{\n      \"jsonrpc\": \"2.0\",\n      \"id\": 2,\n      \"method\": \"tools/call\",\n      \"params\": {\n        \"name\": \"get_service_traces\",\n        \"arguments\": {\n          \"service_name\": \"your-service-name\",\n          \"lookback_minutes\": 60,\n          \"limit\": 5\n        }\n      }\n    }'\n\n# List available tools\ncurl -X POST http://localhost:8080/mcp \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Mcp-Session-Id: session_$(date +%s)000000000\" \\\n    -d '{\n      \"jsonrpc\": \"2.0\",\n      \"id\": 3,\n      \"method\": \"tools/list\",\n      \"params\": {}\n    }'\n```\n\n### Building from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/last9/last9-mcp-server.git\ncd last9-mcp-server\n\n# Build the binary\ngo build -o last9-mcp-server\n\n# Run in development mode\nHTTP_MODE=true ./last9-mcp-server\n```\n\n**Note**: HTTP mode is for development and testing only. When integrating with Claude Desktop or other MCP clients, use the default STDIO mode (without `HTTP_MODE=true`).\n\n## Badges\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/last9-last9-mcp-server-badge.png)](https://mseep.ai/app/last9-last9-mcp-server)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logging",
        "logs",
        "monitoring",
        "logging last9",
        "monitoring logging",
        "including logs"
      ],
      "category": "monitoring-and-logging"
    },
    "loglmhq--mcp-server-prometheus": {
      "owner": "loglmhq",
      "name": "mcp-server-prometheus",
      "url": "https://github.com/loglmhq/mcp-server-prometheus",
      "imageUrl": "/freedevtools/mcp/pfp/loglmhq.webp",
      "description": "Connects to Prometheus metrics and data through the Model Context Protocol (MCP), providing access to metric schemas and detailed metadata, along with statistical information.",
      "stars": 16,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-18T09:02:30Z",
      "readme_content": "# mcp-server-prometheus\n\nMCP server for interacting with Prometheus metrics and data.\n\nThis is a TypeScript-based MCP server that implements a Prometheus API interface. It provides a bridge between Claude and your Prometheus server through the Model Context Protocol (MCP).\n\n<a href=\"https://glama.ai/mcp/servers/y7b3qba8jy\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/y7b3qba8jy/badge\" alt=\"mcp-server-prometheus MCP server\" /></a>\n\n## Demo\n\n![demo](/demo.png)\n\n## Features\n\n### Resources\n\n- List and access Prometheus metric schema\n- Each metric resource provides:\n  - Metric name and description\n  - Detailed metadata from Prometheus\n  - Statistical information (count, min, max)\n- JSON mime type for structured data access\n\n### Current Capabilities\n\n- List all available Prometheus metrics with descriptions\n- Read detailed metric information including:\n  - Metadata and help text\n  - Current statistical data (count, min, max values)\n- Basic authentication support for secured Prometheus instances\n\n## Configuration\n\nThe server requires the following environment variable:\n\n- `PROMETHEUS_URL`: The base URL of your Prometheus instance\n\nOptional authentication configuration:\n\n- `PROMETHEUS_USERNAME`: Username for basic auth (if required)\n- `PROMETHEUS_PASSWORD`: Password for basic auth (if required)\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-prometheus\": {\n      \"command\": \"/path/to/mcp-server-prometheus/build/index.js\",\n      \"env\": {\n        \"PROMETHEUS_URL\": \"http://your-prometheus-instance:9090\"\n      }\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## API Structure\n\nThe server exposes Prometheus metrics through the following URI structure:\n\n- Base URI: `http://your-prometheus-instance:9090`\n- Metric URIs: `http://your-prometheus-instance:9090/metrics/{metric_name}`\n\nEach metric resource returns JSON data containing:\n\n- Metric name\n- Metadata (help text, type)\n- Current statistics (count, min, max)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "prometheus",
        "logging",
        "monitoring",
        "server prometheus",
        "prometheus metrics",
        "connects prometheus"
      ],
      "category": "monitoring-and-logging"
    },
    "marcoeg--mcp-server-ntopng": {
      "owner": "marcoeg",
      "name": "mcp-server-ntopng",
      "url": "https://github.com/marcoeg/mcp-server-ntopng",
      "imageUrl": "/freedevtools/mcp/pfp/marcoeg.webp",
      "description": "Enables AI agents to access and query network monitoring data stored in the NTOPNG database, facilitating traffic analysis and reporting through seamless integration. Utilizes ClickHouse for historical flows and alerts within the NTOPNG framework.",
      "stars": 0,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-22T01:50:57Z",
      "readme_content": "# mcp-server-ntopng\n[![PyPI - Version](https://img.shields.io/pypi/v/mcp-ntopng)](https://pypi.org/project/mcp-ntopng)\n\nNTOPNG Model Context Protocol Server\n\n<a href=\"https://glama.ai/mcp/servers/@marcoeg/mcp-server-ntopng\">\n<img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@marcoeg/mcp-server-ntopng/badge\" />\n\nA [Model Context Protocol](https://modelcontextprotocol.io/) server implementation for [NTOPNG](https://www.ntop.org/products/traffic-analysis/ntop/) that enables AI agents to query networks monitoring data using the NTOPNG database.\n\nThis MCP Server assumes that `ntopng` is using ClickHouse to store historical flows and alert. Check [ntopng Clickhouse](https://www.ntop.org/guides/ntopng/flow_dump/clickhouse/index.html)\n\n\n## Tools\n\n* `fetch_ntopng_all_ifids`\n- Retrieve all available interface IDs from ntopng.\n* `get_ntopng_hosts_location`\n- Fetch geographical location and additional info for hosts.\n* `fetch_ntopng_top_local_talkers`\n- Retrieve the top 10 local talkers for a specified interface.\n* `fetch_ntopng_top_remote_talkers`\n- Retrieve the top 10 remote talkers for a specified interface.\n* `get_ntopng_all_alert_stats`\n- Retrieve statistics for all alerts.\n* `get_ntopng_flow_alert_stats`\n- Retrieve statistics for flow alerts.\n* `get_ntopng_host_alert_stats`\n- Retrieve statistics for host alerts.\n* `get_ntopng_interface_alert_stats`\n- Retrieve statistics for interface alerts.\n* `get_ntopng_mac_alert_stats`\n- Retrieve statistics for MAC alerts.\n* `get_ntopng_network_alert_stats`\n- Retrieve statistics for network alerts.\n* `get_ntopng_snmp_device_alert_list`\n- Retrieve a list of SNMP device alerts.\n* `get_ntopng_snmp_device_alert_stats`\n- Retrieve statistics for SNMP device alerts.\n* `get_ntopng_system_alert_stats`\n- Retrieve statistics for system alerts.\n* `query_ntopng_flows_data`\n- Retrieve detailed flows data from the ntopng flows database.\n* `get_ntopng_top-k_flows`\n- Retrieve top-k flows data from the ntopng flows database.\n* `get_ntopng_user_alert_stats`\n- Retrieve statistics for user alerts.\n* `get_ntopng_flow_devices_stats`\n- Retrieve statistics for all flow dev`ices.\n* `get_ntopng_sflow_devices_stats`\n- Retrieve statistics for all sFlow devices.\n* `list_tables_ntopng_database`\n- List tables structure of the ntopng database.\n* `query_ntopng_database`\n- Query the ntopng Clickhouse database.\n\n## Status\n\nWorks with Claude Desktop app and other MCP compliant hosts and clients. \n\nNo support for MCP [resources](https://modelcontextprotocol.io/docs/concepts/resources) or [prompts](https://modelcontextprotocol.io/docs/concepts/prompts) yet.\n\n## Configuration\n\n1. Create or edit the Claude Desktop configuration file located at:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. Add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-ntopng\": {\n      \"command\": \"/path/to/your/uv-binary\",\n      \"args\": [\"run\", \"--with\", \"mcp-ntopng\", \"--python\", \"3.13\", \"mcp-ntopng\"]\n      \"env\": {\n        \"NTOPNG_HOST\": \"<ntopng-host>\",\n        \"NTOPNG_DBPORT\": \"<ntopng-dbport>\",\n        \"NTOPNG_DBUSER\": \"<ntopng-dbuser>\",\n        \"NTOPNG_DBPASSWORD\": \"<ntopng-dbpassword>\",\n        \"NTOPNG_SECURE\": \"true\",\n        \"NTOPNG_VERIFY\": \"true\",\n        \"NTOPNG_CONNECT_TIMEOUT\": \"30\",\n        \"NTOPNG_SEND_RECEIVE_TIMEOUT\": \"300\",\n        \"NTOPNG_API_KEY\": \"NTOPNG_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n\n3. Replace `/path/to/your/uv-binary` with the absolute path to the `uv` executable. Find the path with `which uv`. This ensures that the correct version of `uv` is used when starting the server.\n\n4. Restart Claude Desktop to apply the changes.\n\n\n## Development \n\n1. Set the environmental variables either in the `claude_desktop_config.json` file or in a `.env` file in the root of the repository.\n\n```\nNTOPNG_HOST=localhost\nNTOPNG_PORT=9000\nNTOPNG_USER=default\nNTOPNG_PASSWORD=\n```\n\n3. Run `uv sync` to install the dependencies. To install `uv` follow the instructions [here](https://docs.astral.sh/uv/). Then do `source .venv/bin/activate`.\n\n4. Install the `mcp-ntopng` package with `uv pip install -e .` from the project main directory. \n\n4. For easy testing, you can run `mcp dev mcp_ntopng/mcp_server.py` to start the MCP server. **CHANGE WITH A PROPER CHAT CLIENT**\n\n### Environment Variables\n\nThe following environment variables are used to configure the database connection:\n\n* `NTOPNG_HOST`: The hostname of the `ntopng` server\n* `NTOPNG_DBUSER`: The username for Clickhouse DB authentication\n* `NTOPNG_DBPASSWORD`: The password for Clickhouse DB authentication\n* `NTOPNG_API_KEY`: The `ntopng` authentication token.\n\n#### Optional\n* `NTOPNG_DBPORT`: The port number of the Clickhouse DB in the  `ntopng` server\n  - Default: `9000` if HTTPS is enabled, `8123` if disabled\n  - Usually doesn't need to be set unless using a non-standard port\n* `NTOPNG_SECURE`: Enable/disable a TLS connection\n  - Default: `false`\n  - Set to `true` for a secure TLS connections\n* `NTOPNG_VERIFY`: Enable/disable SSL certificate verification\n  - Default: `true`\n  - Set to `false to disable certificate verification (not recommended for production)\n* `NTOPNG_CONNECT_TIMEOUT`: Connection timeout in seconds\n  - Default: `30\n  - Increase this value if you experience connection timeouts\n* `NTOPNG_SEND_RECEIVE_TIMEOUT`: Send/receive timeout in seconds\n  - Default: `300`\n  - Increase this value for long-running queries\n\n\n> Check [TLS Setup](https://www.ntop.org/guides/ntopng/flow_dump/clickhouse/clickhouse.html#tls-connection) in the `ntopng` documentation for details about setting up a TLS connection to Clickhouse.\n\n### Development\nInstall the package on the local machine:\n```\n$ uv sync\n$ uv pip install -e .\n```\nRun the MCP Inspector\n```\n$ cd mcp_ntopng\n$ source .env\n$ CLIENT_PORT=8077 SERVER_PORT=8078  mcp dev run_mcp_ntopng.py --with clickhouse-driver --with python-dotenv --with uvicorn --with pip-system-certs\n```\nUse the local library in Claude Desktop.\n\nFind:  /Users/marco/Library/Application\\ Support/Claude/claude_desktop_config.json \n\nEdit the claude_desktop_config.json changing the local paths:\n```\n{\n    \"mcpServers\": {\n      \"mcp-ntopng\": {\n        \"command\": \"/Users/marco/Development/claude/mcp-server-ntopng/.venv/bin/python\",\n        \"args\": [\n           \"/Users/marco/Development/claude/mcp-server-ntopng/run_mcp_ntopng.py\"\n        ],\n        \"env\": {\n          \"NTOPNG_HOST\": \"marcoeg-nod004.ntoplink.com\",\n          \"NTOPNG_DBPORT\": \"9000\",\n          \"NTOPNG_DBUSER\": \"default\",\n          \"NTOPNG_DBPASSWORD\": \"\",\n          \"NTOPNG_SECURE\": \"false\",\n          \"NTOPNG_VERIFY\": \"false\",\n          \"NTOPNG_CONNECT_TIMEOUT\": \"30\",\n          \"NTOPNG_SEND_RECEIVE_TIMEOUT\": \"300\",\n          \"SELECT_QUERY_TIMEOUT_SECS\": \"30\",\n          \"NTOPNG_API_KEY\": \"NTOPNG_TOKEN\"\n        }\n      }\n    }\n  }\n  ```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ntopng",
        "monitoring",
        "logging",
        "server ntopng",
        "alerts ntopng",
        "ntopng framework"
      ],
      "category": "monitoring-and-logging"
    },
    "mdwooky--logstash-input-pluginname": {
      "owner": "mdwooky",
      "name": "logstash-input-pluginname",
      "url": "https://github.com/mdwooky/logstash-input-pluginname",
      "imageUrl": "/freedevtools/mcp/pfp/mdwooky.webp",
      "description": "Streamlines data ingestion into Logstash pipelines by integrating various input sources, facilitating enhanced data processing capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Jupyter Notebook",
      "updated_at": "2019-07-08T07:44:51Z",
      "readme_content": "# logstash-input-pluginname",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logstash",
        "logging",
        "pipelines",
        "logstash pipelines",
        "logstash input",
        "mdwooky logstash"
      ],
      "category": "monitoring-and-logging"
    },
    "monsterxx03--gospy": {
      "owner": "monsterxx03",
      "name": "gospy",
      "url": "https://github.com/monsterxx03/gospy",
      "imageUrl": "/freedevtools/mcp/pfp/monsterxx03.webp",
      "description": "Inspect and analyze running Go processes, providing detailed information about goroutines, memory usage, and binary data through an interactive terminal and an HTTP API. Features include programmatic access via a Server-Sent Events endpoint and tools for inspecting goroutine states and memory statistics.",
      "stars": 92,
      "forks": 4,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-08-23T12:55:59Z",
      "readme_content": "# Go Process Inspector\n\n[![Go Report Card](https://goreportcard.com/badge/github.com/monsterxx03/gospy)](https://goreportcard.com/report/github.com/monsterxx03/gospy)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=gospy&config=eyJ1cmwiOiJodHRwOi8vbG9jYWxob3N0Ojg5NzQvbWNwIn0%3D)\n\nA tool for inspecting and analyzing running Go processes, including goroutine states, memory statistics, and binary information.\n\n## Features\n\n- View detailed goroutine information (status, scheduling info)\n- Analyze process memory statistics\n- Cross-platform support (Linux and macOS)\n- Terminal UI for interactive inspection\n- HTTP API for programmatic access\n- mcp server\n\n## Installation\n\n```bash\ngo install github.com/monsterxx03/gospy@latest\n```\n\n## Usage\n\n### CLI Interface\n\n```bash\n# Interactive terminal UI\nsudo gospy top --pid <pid>\n\n# HTTP API server\nsudo gospy serve --port 8974\n\n# Get process summary\nsudo gospy summary --pid <pid>\n\n# Get process summary in JSON format\nsudo gospy summary --pid <pid> --json\n```\n\n#### Summary Command Options\n- `--pid/-p` - Target process ID (required)\n- `--bin/-b` - Path to binary file (optional)\n- `--json/-j` - Output results in JSON format\n\n### API Endpoints\n\n- `GET /goroutines?pid=<pid>` - List all goroutines\n- `GET /memstats?pid=<pid>` - Get memory statistics\n- `GET /runtime?pid=<pid>` - Get runtime version info\n\n### MCP Server\n\nThe MCP server provides an http (streamableHTTP) endpoint. To enable:\n\n```bash\n>>> sudo gospy serve --enable-mcp --port 8974\n\nStarting API server on port 8974\nEndpoints:\n  GET /runtime?pid=<PID>     - Get runtime info\n  GET /goroutines?pid=<PID> - Get goroutines list\n  GET /memstats?pid=<PID>   - Get memory stats\n  GET /mcp   - MCP http endpoint\n\n```\n\nAvailable MCP tools:\n- `goroutines` - Dump goroutines for a go process\n- `gomemstats` - Dump memory stats for a go process\n- `goruntime`  - Dump runtime info for a go process\n- `pgrep`      - Find pid from process name\n\nConfig in cursor\n\n![](screenshots/mcp-config.png)\n\n\n### Terminal UI Controls\n\n- `q` - Quit\n- `r` - Refresh data\n- `s` - Suspend/Resume top view\n- `/` - Search/filter goroutines\n\n### Terminal UI Screenshot\n\n![Terminal UI Screenshot](screenshots/top.gif)\n\n## Building from Source\n\n```bash\ngit clone https://github.com/monsterxx03/gospy.git\ncd gospy\nmake\n```\n\n## Requirements\n\n- Go 1.20+\n- Linux or macOS (Apple Silicon only)\n- Root privileges (required for memory access)\n\n## Root Privileges\n\ngospy requires root privileges to:\n- Read process memory (/proc/<pid>/mem on Linux)\n- Access Mach APIs on macOS\n\nRun with sudo:\n```bash\nsudo gospy top --pid <pid>\n```\n\nFor development/debugging, you may want to:\n1. Build the binary first: `make`\n2. Run with sudo: `sudo ./gospy [command]`\n\n## Credits\n\nVersion 0.7.0 was completely rewritten from scratch with [aider](https://aider.chat), which wrote >90% of the code. Additional assistance from:\n- [DeepSeek](https://deepseek.com) (R1 + V3 models) - AI coding assistant\n\nTotal AI compute cost: ~$2 USD\n\n## License\n\nMIT - See [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gospy",
        "goroutine",
        "goroutines",
        "inspecting goroutine",
        "gospy inspect",
        "goroutines memory"
      ],
      "category": "monitoring-and-logging"
    },
    "mottibec--otelcol-mcp": {
      "owner": "mottibec",
      "name": "otelcol-mcp",
      "url": "https://github.com/mottibec/otelcol-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mottibec.webp",
      "description": "Configures OpenTelemetry Collectors dynamically by managing components such as receivers, processors, and exporters. Facilitates the updating and retrieval of telemetry component information from specified resources.",
      "stars": 43,
      "forks": 1,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-08-14T14:18:15Z",
      "readme_content": "# OpenTelemetry Collector MCP Server\n\nAn MCP server implementation for configuring OpenTelemetry Collectors.\n\n## Features\n\n- **Dynamic OpenTelemetry Configuration**: Configure OpenTelemetry Collectors through MCP tools\n- **Component Management**: Add, remove, and configure receivers, processors, and exporters\n\n## Tools\n\n- **Update Resources**\n  - Updates local resource files with the latest component information from GitHub\n  - No input parameters required\n  - Returns statistics about updated components (receivers, processors, exporters)\n\n## Resources\n\n- **Receivers** (`receivers://receivers`)\n  - Lists all available OpenTelemetry receivers\n  - Returns receiver metadata including name, description, and stability\n\n- **Processors** (`processors://processors`)\n  - Lists all available OpenTelemetry processors\n  - Returns processor metadata including name, description, and stability\n\n- **Exporters** (`exporters://exporters`)\n  - Lists all available OpenTelemetry exporters\n  - Returns exporter metadata including name, description, and stability\n\n- **Component Schemas** (`component://{type}/{name}`)\n  - Retrieves configuration schema for specific components\n  - Supports listing all available schemas or getting a specific component's schema\n  - Parameters:\n    - `type`: Component type (\"receiver\", \"processor\", or \"exporter\")\n    - `name`: Name of the specific component (optional)\n\n## Configuration\n\n### Usage with mcp clients\n\nAdd this to your `mcp.json`:\n\n```json\n{\n    \"mcpServers\": {\n      \"otelcol\": {\n        \"url\": \"http://localhost:3001/sse\"\n      }\n    }\n}\n```\n\n## Development\n\nThis is a local implementation of an MCP server for OpenTelemetry configuration. To use it:\n\n1. Clone the repository\n2. Build the project using the provided build scripts\n3. Configure your MCP client to use the local server implementation\n\n## License\n\nThis MCP server is licensed under the GPL-3.0 License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the GPL-3.0 License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opentelemetry",
        "telemetry",
        "monitoring",
        "opentelemetry collectors",
        "configures opentelemetry",
        "logging mottibec"
      ],
      "category": "monitoring-and-logging"
    },
    "muaimingjun--LinuxCTS": {
      "owner": "muaimingjun",
      "name": "LinuxCTS",
      "url": "https://github.com/muaimingjun/LinuxCTS",
      "imageUrl": "/freedevtools/mcp/pfp/muaimingjun.webp",
      "description": "A comprehensive testing script for Linux systems that evaluates system performance, configuration, and service status through various tests. It provides insights into system information, conducts performance and network tests, and checks the operational status of common services.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Shell",
      "updated_at": "2025-08-17T06:54:45Z",
      "readme_content": "# LinuxCTS - Linux 综合测试脚本\n\n## 项目简介\n\nLinuxCTS 是一个用于 Linux 系统的综合测试脚本，旨在帮助用户快速、方便地对 Linux 系统进行多方面的测试，以评估系统的性能、配置和功能完整性。该脚本整合了一系列实用的测试功能，能够满足不同场景下对 Linux 系统的检测需求。\n\n## 功能特性\n\n- **系统信息检测**：全面展示 Linux 系统的基本信息，包括内核版本、操作系统发行版、CPU 信息、内存信息等，让用户对系统硬件和软件配置一目了然。\n- **性能测试**：具备多种性能测试功能，如 CPU 性能测试、内存读写速度测试、磁盘 I/O 性能测试等，帮助用户评估系统在不同负载下的性能表现。\n- **网络测试**：可以检测网络连接状态、网络速度测试、端口扫描等，方便用户排查网络相关问题，确保网络配置正确且稳定。\n- **服务状态检查**：检查常见系统服务（如 SSH、HTTP、FTP 等）的运行状态，确保服务正常运行，保障系统的可用性。\n\n## 安装与使用\n\n## 依赖\n\n```bash\n# ubuntu/debian\nsudo apt update && sudo apt install curl -y && sudo su\n# readhat/centos\nsudo yum update && sudo yum install curl -y && sudo su\n```\n\n### 一键脚本 （临时使用）\n\n```bash\nsource <(curl -s https://gitee.com/muaimingjun/LinuxCTS/raw/main/linux.sh)\n```\n\n## 安装到系统里\n\n```bash\n# 如何安装？\nsudo curl -L https://gitee.com/muaimingjun/LinuxCTS/raw/main/linux.sh > /usr/bin/linux && sudo chmod +x /usr/bin/linux\n# 如何使用\nlinux\n# 如何更新？\nsudo curl -L https://gitee.com/muaimingjun/LinuxCTS/raw/main/linux.sh > /usr/bin/linux && sudo chmod +x /usr/bin/linux\n# 如何卸载\nsudo rm -rf /usr/bin/linux\n```\n\n## 项目结构\n\n- **`app/`**：可能存放与应用程序相关的测试脚本或配置文件（具体功能可能因项目而异）。\n- **`os/`**：用于存放与操作系统相关的测试模块，例如系统信息获取、系统服务检测等功能的实现代码。\n- **`tools/`**：包含一些辅助工具或脚本，用于支持主脚本的功能实现，如性能测试工具、网络测试工具等。\n- **`.gitignore`**：指定了哪些文件或目录不需要被 Git 版本控制系统跟踪，例如临时文件、编译生成的文件等。\n- **`README.md`**：项目的说明文档，即你正在阅读的此文件，用于向用户介绍项目的功能、安装使用方法等信息。\n- **`linux.sh`**：主脚本文件，整合了各种测试功能，是整个项目的核心执行文件。\n\n## 贡献指南\n\n1. 欢迎大家对本项目进行贡献！如果你有任何改进建议或新功能想法，请先 Fork 本项目到你的 GitHub 账号。\n2. 创建一个新的分支，分支命名建议遵循`feature/你的功能名称`或`bugfix/你的bug修复名称`的格式，以便清晰区分不同类型的贡献。\n3. 在新分支上进行代码修改和开发，确保你的代码符合项目的代码风格和规范。\n4. 提交你的修改时，请提供清晰明了的提交信息，描述修改的内容和目的。\n5. 将你的分支推送到你的 GitHub 仓库，然后发起一个 Pull Request 到本项目的主仓库，详细说明你的修改内容和期望的合并原因，等待项目维护者进行审核和合并。\n\n## 许可证\n\n本项目遵循开源协议，具体许可证信息可查看项目中的 LICENSE 文件\n\n## 联系我们\n\n如果你在使用过程中遇到问题或有任何建议，欢迎通过以下方式联系我们：\n\n- **项目原作者**：[muaimingjun](https://gitee.com/muaimingjun)\n- **致谢作者**：[xccado](https://github.com/xccado/LinuxCTS)\n\n感谢你使用 LinuxCTS！希望这个脚本能够帮助你更好地管理和优化你的 Linux 系统。如果你发现任何问题或有改进的想法，请随时贡献你的力量。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "logging",
        "linux",
        "muaimingjun linuxcts",
        "logging muaimingjun",
        "monitoring logging"
      ],
      "category": "monitoring-and-logging"
    },
    "ndevvy--mcp-server-datadog": {
      "owner": "ndevvy",
      "name": "mcp-server-datadog",
      "url": "https://github.com/ndevvy/mcp-server-datadog",
      "imageUrl": "/freedevtools/mcp/pfp/ndevvy.webp",
      "description": "Retrieve and manage incidents, monitors, logs, dashboards, metrics, traces, hosts, and downtimes from Datadog for enhanced incident management and observability workflows.",
      "stars": 2,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-05-07T04:28:12Z",
      "readme_content": "# Datadog MCP Server\n\nMCP server for the Datadog API, enabling incident management and more. Forked from https://github.com/winor30/mcp-server-datadog\n\n## Features\n\n- **Observability Tools**: Provides a mechanism to leverage key Datadog monitoring features, such as incidents, monitors, logs, dashboards, and metrics, through the MCP server.\n- **Extensible Design**: Designed to easily integrate with additional Datadog APIs, allowing for seamless future feature expansion.\n\n## Tools\n\n1. `list_incidents`\n\n   - Retrieve a list of incidents from Datadog.\n   - **Inputs**:\n     - `pageSize` (optional number): Maximum number of incidents to return per page.\n     - `pageOffset` (optional number): Offset for pagination.\n   - **Returns**: Array of Datadog incidents and associated metadata.\n\n2. `get_incident`\n\n   - Retrieve detailed information about a specific Datadog incident.\n   - **Inputs**:\n     - `incidentId` (string): Incident ID to fetch details for.\n   - **Returns**: Detailed incident information (title, status, timestamps, etc.).\n\n3. `get_monitors`\n\n   - Fetch the status of Datadog monitors.\n   - **Inputs**:\n     - `groupStates` (optional array): States to filter (e.g., alert, warn, no data, ok).\n     - `name` (optional string): Filter by name.\n     - `tags` (optional array): Filter by tags.\n   - **Returns**: Monitors data and a summary of their statuses.\n\n4. `get_logs`\n\n   - Search and retrieve logs from Datadog.\n   - **Inputs**:\n     - `query` (optional string): Datadog logs query string.\n     - `from` (number): Start time in epoch seconds.\n     - `to` (number): End time in epoch seconds.\n     - `limit` (optional number): Maximum number of logs to return (defaults to 100).\n   - **Returns**: Array of matching logs.\n\n5. `list_dashboards`\n\n   - Get a list of dashboards from Datadog.\n   - **Inputs**:\n     - `name` (optional string): Filter dashboards by name.\n     - `tags` (optional array): Filter dashboards by tags.\n   - **Returns**: Array of dashboards with URL references.\n\n6. `get_dashboard`\n\n   - Retrieve a specific dashboard from Datadog.\n   - **Inputs**:\n     - `dashboardId` (string): ID of the dashboard to fetch.\n   - **Returns**: Dashboard details including title, widgets, etc.\n\n7. `create_dashboard`\n\n   - Create a new dashboard in Datadog.\n   - **Inputs**:\n     - `title` (string): The title of the dashboard.\n     - `description` (optional string): The description of the dashboard.\n     - `layoutType` (optional string): The layout type ('ordered' or 'free', defaults to 'ordered').\n     - `widgets` (optional array): The widgets to add to the dashboard.\n     - `tags` (optional array): A list of tags to associate with the dashboard.\n   - **Returns**: Details of the created dashboard including ID and URL.\n\n8. `query_metrics`\n\n   - Retrieve metrics data from Datadog.\n   - **Inputs**:\n     - `query` (string): Metrics query string (e.g., \"avg:system.cpu.user{\\*}\").\n     - `from` (number): Start time in epoch seconds.\n     - `to` (number): End time in epoch seconds.\n   - **Returns**: Metrics data for the queried timeframe.\n\n9. `get_metric_metadata`\n\n   - Get metadata for a specific metric from Datadog.\n   - **Inputs**:\n     - `metricName` (string): Name of the metric to get metadata for.\n   - **Returns**: Metadata information for the specified metric.\n\n10. `get_active_metrics`\n\n    - Get a list of active metrics with optional filtering by host, tags, and search query.\n    - **Inputs**:\n      - `query` (string): Search query string to find metrics.\n      - `from` (optional number): Unix timestamp from which to start the query (default: 24 hours ago).\n      - `host` (optional string): Filter metrics by host.\n      - `tagFilter` (optional string): Filter metrics by tags (e.g. \"env:prod,region:us-east\").\n    - **Returns**: List of metrics matching the search query and/or active metrics based on filters.\n\n11. `analyze_tag_relationships`\n\n    - Show hierarchical relationships between tags across metrics.\n    - **Inputs**:\n      - `from` (optional number): Unix timestamp from which to start analyzing tags (default: now - 1 day).\n      - `limit` (optional number): Maximum number of tag relationships to analyze (default: 50).\n      - `metricPrefix` (optional string): Optional prefix to filter metrics by (e.g., \"system.\" or \"aws.\").\n    - **Returns**: Analysis of tag relationships showing how tags are related hierarchically.\n\n12. `analyze_tag_cardinality`\n\n    - Identify high-cardinality tags that might cause performance issues.\n    - **Inputs**:\n      - `from` (optional number): Unix timestamp from which to start analyzing tags (default: now - 1 day).\n      - `limit` (optional number): Maximum number of tags to analyze (default: 50).\n      - `metricPrefix` (optional string): Optional prefix to filter metrics by (e.g., \"system.\" or \"aws.\").\n      - `minCardinality` (optional number): Minimum cardinality threshold to report (default: 10).\n    - **Returns**: Analysis of high-cardinality tags that could impact performance.\n\n13. `visualize_tag_co_occurrence`\n\n    - Visualize which tags frequently appear together for a specific metric.\n    - **Inputs**:\n      - `metricName` (string): Name of the metric to analyze tags for.\n      - `from` (optional number): Unix timestamp from which to start analyzing tags (default: now - 1 day).\n      - `limit` (optional number): Maximum number of tag pairs to analyze (default: 20).\n    - **Returns**: Visualization of tag co-occurrence patterns for the specified metric.\n\n14. `search_events`\n\n    - Search for events in Datadog.\n    - **Inputs**:\n      - `query` (string): Datadog events query string.\n      - `from` (optional string): Start time as string - either epoch seconds or relative time (e.g., \"now-40m\") (default: \"now-24h\").\n      - `to` (optional string): End time as string - either epoch seconds or relative time (e.g., \"now\") (default: \"now\").\n      - `limit` (optional number): Maximum number of events to return (default: 100).\n      - `sort` (optional string): Sort order for events (default: \"-timestamp\").\n    - **Returns**: Array of matching events from Datadog.\n\n15. `list_traces`\n\n    - Retrieve a list of APM traces from Datadog.\n    - **Inputs**:\n      - `query` (string): Datadog APM trace query string.\n      - `from` (number): Start time in epoch seconds.\n      - `to` (number): End time in epoch seconds.\n      - `limit` (optional number): Maximum number of traces to return (defaults to 100).\n      - `sort` (optional string): Sort order for traces (defaults to '-timestamp').\n      - `service` (optional string): Filter by service name.\n      - `operation` (optional string): Filter by operation name.\n    - **Returns**: Array of matching traces from Datadog APM.\n\n16. `list_apm_services`\n\n    - Get list of APM services from Datadog.\n    - **Inputs**:\n      - `limit` (optional number): Maximum number of services to return (defaults to 100).\n    - **Returns**: List of available APM services.\n\n17. `list_apm_resources`\n\n    - Get list of APM resources for a specific service from Datadog.\n    - **Inputs**:\n      - `service` (string): Service name to filter resources by.\n      - `entry_spans_only` (optional boolean): Filter to only show service entry spans.\n      - `limit` (optional number): Maximum number of resources to return (defaults to 100).\n      - `search_query` (optional string): Search query to filter resource names by.\n    - **Returns**: List of resources for the specified service.\n\n18. `list_apm_operations`\n\n    - Get list of top operation names for a specific service from Datadog.\n    - **Inputs**:\n      - `service` (string): Service name to filter operations by.\n      - `entry_spans_only` (optional boolean): Filter to only show service entry spans.\n      - `limit` (optional number): Maximum number of operations to return (defaults to 100).\n    - **Returns**: List of operation names for the specified service.\n\n19. `get_resource_hash`\n\n    - Get the resource hash for a specific resource name within a service.\n    - **Inputs**:\n      - `service` (string): Service name the resource belongs to.\n      - `resource_name` (string): Resource name to get the hash for.\n    - **Returns**: Resource hash information.\n\n20. `get_all_services`\n\n    - Extract all unique service names from logs.\n    - **Inputs**:\n      - `from` (optional number): Start time in epoch seconds (defaults to 24 hours ago).\n      - `to` (optional number): End time in epoch seconds (defaults to current time).\n      - `limit` (optional number): Maximum number of logs to search through (defaults to 1000).\n      - `query` (optional string): Optional query filter for log search.\n    - **Returns**: List of unique service names found in logs.\n\n21. `list_hosts`\n\n    - Get list of hosts from Datadog.\n    - **Inputs**:\n      - `filter` (optional string): Filter string for search results.\n      - `sort_field` (optional string): Field to sort hosts by.\n      - `sort_dir` (optional string): Sort direction (asc/desc).\n      - `start` (optional number): Starting offset for pagination.\n      - `count` (optional number): Max number of hosts to return (max: 1000).\n      - `from` (optional number): Search hosts from this UNIX timestamp.\n      - `include_muted_hosts_data` (optional boolean): Include muted hosts status and expiry.\n      - `include_hosts_metadata` (optional boolean): Include host metadata (version, platform, etc).\n    - **Returns**: Array of hosts with details.\n\n22. `get_active_hosts_count`\n\n    - Get the total number of active hosts in Datadog.\n    - **Inputs**:\n      - `from` (optional number): Number of seconds from which you want to get total number of active hosts (defaults to 2h).\n    - **Returns**: Count of total active and up hosts.\n\n23. `mute_host`\n\n    - Mute a host in Datadog.\n    - **Inputs**:\n      - `hostname` (string): The name of the host to mute.\n      - `message` (optional string): Message to associate with the muting of this host.\n      - `end` (optional number): POSIX timestamp for when the mute should end.\n      - `override` (optional boolean): If true and the host is already muted, replaces existing end time.\n    - **Returns**: Success status and confirmation message.\n\n24. `unmute_host`\n\n    - Unmute a host in Datadog.\n    - **Inputs**:\n      - `hostname` (string): The name of the host to unmute.\n    - **Returns**: Success status and confirmation message.\n\n25. `list_notebooks`\n\n    - Get list of notebooks from Datadog.\n    - **Inputs**:\n      - `query` (optional string): Return only notebooks with this query string in notebook name or author handle.\n      - `authorHandle` (optional string): Return notebooks created by the given author handle.\n      - `excludeAuthorHandle` (optional string): Return notebooks not created by the given author handle.\n      - `start` (optional number): The index of the first notebook to return.\n      - `count` (optional number): The number of notebooks to be returned.\n      - `sortField` (optional string): Sort by field (modified, name, created).\n      - `sortDir` (optional string): Sort direction (asc, desc).\n      - `type` (optional string): Return only notebooks with that metadata type.\n      - `isTemplate` (optional boolean): True value returns only template notebooks.\n      - `includeCells` (optional boolean): Value of false excludes the cells and global time for each notebook.\n    - **Returns**: List of notebooks matching the specified criteria.\n\n26. `get_notebook`\n\n    - Get a notebook from Datadog.\n    - **Inputs**:\n      - `notebookId` (number): Unique ID of the notebook to retrieve.\n    - **Returns**: Details of the requested notebook including cells and metadata.\n\n27. `create_notebook`\n\n    - Create a new notebook in Datadog.\n    - **Inputs**:\n      - `name` (string): The name of the notebook.\n      - `cells` (optional array): Cells to include in the notebook.\n      - `time` (optional string): Time settings for the notebook (defaults to '1h').\n      - `metadata` (optional object): Additional metadata for the notebook.\n    - **Returns**: Details of the created notebook.\n\n28. `add_cell_to_notebook`\n\n    - Add a cell to an existing Datadog notebook.\n    - **Inputs**:\n      - `notebookId` (number): The ID of the notebook to add the cell to.\n      - `cell` (object): The cell definition to add.\n    - **Returns**: Updated notebook information.\n\n29. `list_downtimes`\n\n    - List scheduled downtimes from Datadog.\n    - **Inputs**:\n      - `currentOnly` (optional boolean): Return only currently active downtimes when true.\n    - **Returns**: Array of scheduled downtimes with details.\n\n30. `schedule_downtime`\n\n    - Schedule a downtime in Datadog.\n    - **Inputs**:\n      - `scope` (string): Scope to apply downtime to (e.g. 'host:my-host').\n      - `start` (optional number): UNIX timestamp for the start of the downtime.\n      - `end` (optional number): UNIX timestamp for the end of the downtime.\n      - `message` (optional string): A message to include with the downtime.\n      - `timezone` (optional string): The timezone for the downtime.\n      - `monitorId` (optional number): The ID of the monitor to mute.\n      - `monitorTags` (optional array): A list of monitor tags for filtering.\n      - `recurrence` (optional object): Recurrence settings for the downtime.\n    - **Returns**: Scheduled downtime details including ID and active status.\n\n31. `cancel_downtime`\n    - Cancel a scheduled downtime in Datadog.\n    - **Inputs**:\n      - `downtimeId` (number): The ID of the downtime to cancel.\n    - **Returns**: Confirmation of downtime cancellation.\n\n## Setup\n\n### Datadog Credentials\n\nYou need valid Datadog API credentials to use this MCP server:\n\n- `DATADOG_API_KEY`: Your Datadog API key\n- `DATADOG_APP_KEY`: Your Datadog Application key\n- `DATADOG_SITE` (optional): The Datadog site (e.g. `datadoghq.eu`)\n\nExport them in your environment before running the server:\n\n```bash\nexport DATADOG_API_KEY=\"your_api_key\"\nexport DATADOG_APP_KEY=\"your_app_key\"\nexport DATADOG_SITE=\"your_datadog_site\"\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install Datadog MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ndevvy/mcp-server-datadog):\n\n```bash\nnpx -y @smithery/cli install @ndevvy/mcp-server-datadog --client claude\n```\n\n### Manual Installation\n\n```bash\npnpm install\npnpm build\npnpm watch   # for development with auto-rebuild\n```\n\n## Usage\n\nAdd to your `claude_desktop_config.json` or `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"datadog\": {\n      \"command\": \"/path/to/mcp-server-datadog/build/index.js\",\n      \"env\": {\n        \"DATADOG_API_KEY\": \"<YOUR_API_KEY>\",\n        \"DATADOG_APP_KEY\": \"<YOUR_APP_KEY>\",\n        \"DATADOG_SITE\": \"<YOUR_SITE>\" // Optional\n      }\n    }\n  }\n}\n```\n\n## Debugging\n\nBecause MCP servers communicate over standard input/output, debugging can sometimes be tricky. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector). You can run the inspector with:\n\n```bash\nnpm run inspector\n```\n\nThe inspector will provide a URL you can open in your browser to see logs and send requests manually.\n\n## License\n\nThis project is licensed under the [Apache License, Version 2.0](./LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datadog",
        "logging",
        "monitoring",
        "server datadog",
        "logging ndevvy",
        "datadog enhanced"
      ],
      "category": "monitoring-and-logging"
    },
    "samwang0723--mcp-sumologic": {
      "owner": "samwang0723",
      "name": "mcp-sumologic",
      "url": "https://github.com/samwang0723/mcp-sumologic",
      "imageUrl": "/freedevtools/mcp/pfp/samwang0723.webp",
      "description": "Search and retrieve logs from Sumo Logic using customized queries within specified time ranges, while supporting error handling and detailed logging.",
      "stars": 5,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-02T02:09:21Z",
      "readme_content": "# MCP Sumo Logic\n\nA Model Context Protocol (MCP) server that integrates with Sumo Logic's API to perform log searches.\n\n## Features\n\n- Search Sumo Logic logs using custom queries\n- Configurable time ranges for searches\n- Error handling and detailed logging\n- Docker support for easy deployment\n\n## Environment Variables\n\n```env\nENDPOINT=https://api.au.sumologic.com/api/v1  # Sumo Logic API endpoint\nSUMO_API_ID=your_api_id                       # Sumo Logic API ID\nSUMO_API_KEY=your_api_key                     # Sumo Logic API Key\n```\n\n## Setup\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Create a `.env` file with the required environment variables\n4. Build the project:\n   ```bash\n   npm run build\n   ```\n5. Start the server:\n   ```bash\n   npm start\n   ```\n\n## Docker Setup\n\n1. Build the Docker image:\n   ```bash\n   docker build -t mcp/sumologic .\n   ```\n\n2. Run the container (choose one method):\n\n   a. Using environment variables directly:\n   ```bash\n   docker run -e ENDPOINT=your_endpoint -e SUMO_API_ID=your_api_id -e SUMO_API_KEY=your_api_key mcp/sumologic\n   ```\n\n   b. Using a .env file:\n   ```bash\n   docker run --env-file .env mcp/sumologic\n   ```\n\n   Note: Make sure your .env file contains the required environment variables:\n   ```env\n   ENDPOINT=your_endpoint\n   SUMO_API_ID=your_api_id\n   SUMO_API_KEY=your_api_key\n   ```\n\n## Usage\n\nThe server exposes a `search-sumologic` tool that accepts the following parameters:\n\n- `query` (required): The Sumo Logic search query\n- `from` (optional): Start time in ISO 8601 format\n- `to` (optional): End time in ISO 8601 format\n\nExample query:\n```typescript\nconst query = '_index=app_pro_fiat_cont | json auto | fields log_identifier';\nconst results = await search(sumoClient, query, {\n  from: '2024-02-23T00:00:00Z',\n  to: '2024-02-24T00:00:00Z',\n});\n```\n\n## Error Handling\n\nThe server includes comprehensive error handling and logging:\n- API errors are caught and logged with details\n- Search job status is monitored and logged\n- Network and authentication issues are properly handled\n\n## Development\n\nTo run in development mode:\n```bash\nnpm run dev\n```\n\nFor testing:\n```bash\nnpm test\n``` ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sumologic",
        "sumo",
        "logging",
        "logs sumo",
        "sumologic search",
        "mcp sumologic"
      ],
      "category": "monitoring-and-logging"
    },
    "sapientpants--sonarqube-mcp-server": {
      "owner": "sapientpants",
      "name": "sonarqube-mcp-server",
      "url": "https://github.com/sapientpants/sonarqube-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/sapientpants.webp",
      "description": "Integrates with SonarQube to provide access to code quality metrics, detect issues, and analyze results for AI assistants.",
      "stars": 96,
      "forks": 15,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T20:22:00Z",
      "readme_content": "# SonarQube MCP Server\n\n[![CI](https://github.com/sapientpants/sonarqube-mcp-server/actions/workflows/ci.yml/badge.svg)](https://github.com/sapientpants/sonarqube-mcp-server/actions/workflows/ci.yml)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=sonarqube-mcp-server&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=sonarqube-mcp-server)\n[![Bugs](https://sonarcloud.io/api/project_badges/measure?project=sonarqube-mcp-server&metric=bugs)](https://sonarcloud.io/summary/new_code?id=sonarqube-mcp-server)\n[![Code Smells](https://sonarcloud.io/api/project_badges/measure?project=sonarqube-mcp-server&metric=code_smells)](https://sonarcloud.io/summary/new_code?id=sonarqube-mcp-server)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=sonarqube-mcp-server&metric=coverage)](https://sonarcloud.io/summary/new_code?id=sonarqube-mcp-server)\n[![Duplicated Lines (%)](https://sonarcloud.io/api/project_badges/measure?project=sonarqube-mcp-server&metric=duplicated_lines_density)](https://sonarcloud.io/summary/new_code?id=sonarqube-mcp-server)\n[![npm version](https://img.shields.io/npm/v/sonarqube-mcp-server.svg)](https://www.npmjs.com/package/sonarqube-mcp-server)\n[![npm downloads](https://img.shields.io/npm/dm/sonarqube-mcp-server.svg)](https://www.npmjs.com/package/sonarqube-mcp-server)\n[![License](https://img.shields.io/npm/l/sonarqube-mcp-server.svg)](https://github.com/sapientpants/sonarqube-mcp-server/blob/main/LICENSE)\n\nA Model Context Protocol (MCP) server that integrates with SonarQube to provide AI assistants with access to code quality metrics, issues, and analysis results.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Documentation](#documentation)\n- [Compatibility](#compatibility)\n- [Quick Start](#quick-start)\n- [Installation](#installation)\n  - [NPX](#npx-recommended)\n  - [Docker](#docker-recommended-for-production)\n  - [Local Development](#local-development)\n- [Configuration](#configuration)\n  - [Environment Variables](#environment-variables)\n  - [Authentication Methods](#authentication-methods)\n- [Available Tools](#available-tools)\n- [Usage Examples](#usage-examples)\n- [Architecture](#architecture)\n- [Development](#development)\n- [Troubleshooting](#troubleshooting)\n- [Contributing](#contributing)\n- [License](#license)\n- [External Resources](#external-resources)\n\n## Overview\n\nThe SonarQube MCP Server enables AI assistants to interact with SonarQube's code quality analysis capabilities through the Model Context Protocol. This integration allows AI assistants to:\n\n- 📊 **Retrieve code metrics and analysis results** - Access detailed quality metrics for your projects\n- 🐛 **Access and filter issues** - Search and filter code issues by severity, type, status, and more\n- 🔒 **Review security hotspots** - Find and manage security vulnerabilities with dedicated workflows\n- 🌿 **Analyze branches and PRs** - Review code quality in feature branches and pull requests\n- 📦 **Multi-project analysis** - Query issues and metrics across multiple projects simultaneously\n- ✅ **Check quality gates** - Monitor whether projects meet quality standards\n- 📈 **Analyze project quality over time** - Track metrics history and trends\n- 🔍 **View source code with issues** - See problematic code with highlighted issues\n- 🏥 **Monitor system health** - Check SonarQube instance status and availability\n- 🔄 **Enhanced error handling** - Clear error messages with solutions and automatic retry for transient failures\n\n## Documentation\n\n### Core Guides\n\n- **[Architecture Guide](docs/architecture.md)** - System architecture, design decisions, and component overview\n- **[Troubleshooting Guide](docs/troubleshooting.md)** - Common issues, debugging, and solutions\n\n### Security & Authentication\n\n- **[Security Guide](docs/security.md)** - Authentication, authorization, and security best practices\n\n## Compatibility\n\nFor detailed information about MCP protocol version support and SDK compatibility, see [COMPATIBILITY.md](COMPATIBILITY.md).\n\n## Quick Start\n\n### Prerequisites\n\n- [Claude Desktop](https://claude.ai/download) installed\n- A SonarQube instance or [SonarCloud](https://sonarcloud.io) account\n- A SonarQube/SonarCloud authentication token\n\n### 1. Get Your SonarQube Token\n\n**For SonarCloud:**\n\n1. Log in to [SonarCloud](https://sonarcloud.io)\n2. Go to **My Account** → **Security**\n3. Generate a new token\n\n**For SonarQube:**\n\n1. Log in to your SonarQube instance\n2. Go to **My Account** → **Security**\n3. Generate a new token\n\n### 2. Configure Claude Desktop\n\n1. Open Claude Desktop\n2. Go to **Settings** → **Developer** → **Edit Config**\n3. Add the SonarQube server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sonarqube-mcp-server@latest\"],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://sonarcloud.io\",\n        \"SONARQUBE_TOKEN\": \"your-token-here\",\n        \"SONARQUBE_ORGANIZATION\": \"your-org (for SonarCloud)\"\n      }\n    }\n  }\n}\n```\n\n**Alternative authentication methods:**\n\nUsing Basic Authentication:\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sonarqube-mcp-server@latest\"],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://your-sonarqube.com\",\n        \"SONARQUBE_USERNAME\": \"your-username\",\n        \"SONARQUBE_PASSWORD\": \"your-password\"\n      }\n    }\n  }\n}\n```\n\nUsing System Passcode:\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sonarqube-mcp-server@latest\"],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://your-sonarqube.com\",\n        \"SONARQUBE_PASSCODE\": \"your-system-passcode\"\n      }\n    }\n  }\n}\n```\n\n1. Restart Claude Desktop\n\n### 3. Start Using\n\nAsk Claude to analyze your SonarQube projects:\n\n```\n\"List all my SonarQube projects\"\n\"Show me critical issues in project xyz\"\n\"What's the code coverage for project xyz?\"\n\"Check the quality gate status for project xyz\"\n\"Retrieve security hotspots in project xyz and create a plan to address them\"\n\"Retrieve the issues for pr 123 in project xyz and create a plan to address them\"\n```\n\n## Installation\n\n### NPX (Recommended)\n\nThe simplest way to use the SonarQube MCP Server is through npx:\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sonarqube-mcp-server@latest\"],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://sonarqube.example.com\",\n        \"SONARQUBE_TOKEN\": \"your-sonarqube-token\",\n        \"SONARQUBE_ORGANIZATION\": \"your-organization-key\"\n      }\n    }\n  }\n}\n```\n\n### Docker (Recommended for Production)\n\nDocker provides the most reliable deployment method by packaging all dependencies and ensuring consistent behavior across different environments.\n\n> **Enterprise Deployment**: For production deployments with Kubernetes, Helm charts, and cloud-specific configurations, see our comprehensive [Deployment Guide](docs/deployment.md).\n\n#### Quick Start with Docker\n\n**For stdio transport (Claude Desktop):**\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"SONARQUBE_URL\",\n        \"-e\",\n        \"SONARQUBE_TOKEN\",\n        \"-e\",\n        \"SONARQUBE_ORGANIZATION\",\n        \"sapientpants/sonarqube-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://sonarqube.example.com\",\n        \"SONARQUBE_TOKEN\": \"your-sonarqube-token\",\n        \"SONARQUBE_ORGANIZATION\": \"your-organization-key\"\n      }\n    }\n  }\n}\n```\n\n#### Docker Hub Images\n\nOfficial images are available on Docker Hub: [`sapientpants/sonarqube-mcp-server`](https://hub.docker.com/r/sapientpants/sonarqube-mcp-server)\n\n**Available tags:**\n\n- `latest` - Latest stable release\n- `1.6.0` - Specific version (recommended for production)\n- `1.6` - Latest patch version of 1.6.x\n- `1` - Latest minor version of 1.x.x\n\n**Pull the image:**\n\n```bash\ndocker pull sapientpants/sonarqube-mcp-server:latest\n```\n\n#### Advanced Docker Configuration\n\n**With logging enabled:**\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/tmp/sonarqube-logs:/logs\",\n        \"-e\",\n        \"SONARQUBE_URL\",\n        \"-e\",\n        \"SONARQUBE_TOKEN\",\n        \"-e\",\n        \"SONARQUBE_ORGANIZATION\",\n        \"-e\",\n        \"LOG_FILE=/logs/sonarqube-mcp.log\",\n        \"-e\",\n        \"LOG_LEVEL=INFO\",\n        \"sapientpants/sonarqube-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://sonarqube.example.com\",\n        \"SONARQUBE_TOKEN\": \"your-sonarqube-token\",\n        \"SONARQUBE_ORGANIZATION\": \"your-organization-key\"\n      }\n    }\n  }\n}\n```\n\n**Using Docker Compose:**\n\n```yaml\nversion: '3.8'\nservices:\n  sonarqube-mcp:\n    image: sapientpants/sonarqube-mcp-server:latest\n    environment:\n      - SONARQUBE_URL=https://sonarqube.example.com\n      - SONARQUBE_TOKEN=${SONARQUBE_TOKEN}\n      - SONARQUBE_ORGANIZATION=${SONARQUBE_ORGANIZATION}\n      - LOG_FILE=/logs/sonarqube-mcp.log\n      - LOG_LEVEL=INFO\n    volumes:\n      - ./logs:/logs\n    stdin_open: true\n    tty: true\n```\n\n#### Building Your Own Docker Image\n\nIf you need to customize the server, you can build your own image:\n\n```bash\n# Clone the repository\ngit clone https://github.com/sapientpants/sonarqube-mcp-server.git\ncd sonarqube-mcp-server\n\n# Build the Docker image\ndocker build -t my-sonarqube-mcp-server .\n\n# Run your custom image\ndocker run -i --rm \\\n  -e SONARQUBE_URL=\"https://sonarqube.example.com\" \\\n  -e SONARQUBE_TOKEN=\"your-token\" \\\n  my-sonarqube-mcp-server\n```\n\n#### Docker Best Practices\n\n1. **Version Pinning**: Always use specific version tags in production:\n\n   ```bash\n   sapientpants/sonarqube-mcp-server:1.6.0\n   ```\n\n2. **Resource Limits**: Set appropriate resource limits:\n\n   ```bash\n   docker run -i --rm \\\n     --memory=\"256m\" \\\n     --cpus=\"0.5\" \\\n     sapientpants/sonarqube-mcp-server:1.6.0\n   ```\n\n3. **Security**: Run as non-root user (default in our image):\n\n   ```bash\n   docker run -i --rm \\\n     --user node \\\n     sapientpants/sonarqube-mcp-server:1.6.0\n   ```\n\n4. **Health Checks**: The container includes a health check that verifies the Node.js process is running\n\n### Local Development\n\nFor development or customization:\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/sonarqube-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://sonarqube.example.com\",\n        \"SONARQUBE_TOKEN\": \"your-sonarqube-token\",\n        \"SONARQUBE_ORGANIZATION\": \"your-organization-key\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Authentication (choose one method)\n\n| Variable                 | Description                                   | Required | Default |\n| ------------------------ | --------------------------------------------- | -------- | ------- |\n| **Token Authentication** |                                               |          |         |\n| `SONARQUBE_TOKEN`        | Authentication token for SonarQube API access | ✅ Yes\\* | -       |\n| **Basic Authentication** |                                               |          |         |\n| `SONARQUBE_USERNAME`     | Username for Basic authentication             | ✅ Yes\\* | -       |\n| `SONARQUBE_PASSWORD`     | Password for Basic authentication             | ✅ Yes\\* | -       |\n| **System Passcode**      |                                               |          |         |\n| `SONARQUBE_PASSCODE`     | System passcode for SonarQube authentication  | ✅ Yes\\* | -       |\n\n\\*One authentication method is required. Token authentication takes priority if multiple methods are configured.\n\n#### Connection Settings\n\n| Variable                 | Description                                              | Required  | Default                 |\n| ------------------------ | -------------------------------------------------------- | --------- | ----------------------- |\n| `SONARQUBE_URL`          | URL of your SonarQube instance                           | ❌ No     | `https://sonarcloud.io` |\n| `SONARQUBE_ORGANIZATION` | Organization key (required for SonarCloud)               | ❌ No\\*\\* | -                       |\n| `LOG_FILE`               | Path to write log files (e.g., `/tmp/sonarqube-mcp.log`) | ❌ No     | -                       |\n| `LOG_LEVEL`              | Minimum log level (DEBUG, INFO, WARN, ERROR)             | ❌ No     | `DEBUG`                 |\n\n\\*\\*Required when using SonarCloud\n\n#### HTTP Transport Settings (Advanced)\n\nBy default, the server uses stdio transport for communication with Claude Desktop. For programmatic access or running as a web service, HTTP transport is available:\n\n| Variable                                   | Description                                  | Required | Default     |\n| ------------------------------------------ | -------------------------------------------- | -------- | ----------- |\n| `MCP_TRANSPORT_TYPE`                       | Transport type (`stdio` or `http`)           | ❌ No    | `stdio`     |\n| `MCP_HTTP_PORT`                            | Port for HTTP server                         | ❌ No    | `3000`      |\n| `MCP_HTTP_SESSION_TIMEOUT`                 | Session timeout in milliseconds              | ❌ No    | `1800000`   |\n| `MCP_HTTP_ALLOWED_HOSTS`                   | Comma-separated list of allowed hosts        | ❌ No    | `localhost` |\n| `MCP_HTTP_ALLOWED_ORIGINS`                 | Comma-separated list of allowed CORS origins | ❌ No    | `*`         |\n| `MCP_HTTP_ENABLE_DNS_REBINDING_PROTECTION` | Enable DNS rebinding protection              | ❌ No    | `false`     |\n\n### Authentication Methods\n\nThe server supports three authentication methods, with important differences between SonarQube versions:\n\n#### 1. Token Authentication (Recommended)\n\n##### SonarQube 10.0+ (Bearer Token)\n\n- Starting with SonarQube 10.0, Bearer token authentication is the recommended approach\n- Most secure and flexible option\n- Tokens can have limited permissions\n- Configuration:\n  ```json\n  {\n    \"env\": {\n      \"SONARQUBE_TOKEN\": \"your-token-here\"\n    }\n  }\n  ```\n\n##### SonarQube < 10.0 (Token as Username)\n\n- For versions before 10.0, tokens must be sent as the username in Basic authentication\n- No password is required when using a token as username\n- The server automatically handles this based on your SonarQube version\n- Configuration remains the same - just use `SONARQUBE_USERNAME` with the token value:\n  ```json\n  {\n    \"env\": {\n      \"SONARQUBE_USERNAME\": \"your-token-here\"\n    }\n  }\n  ```\n\n#### 2. Basic Authentication\n\n- Traditional username and password authentication\n- Suitable for self-hosted SonarQube instances\n- May not work with SonarCloud if 2FA is enabled\n- Configuration:\n  ```json\n  {\n    \"env\": {\n      \"SONARQUBE_USERNAME\": \"your-username\",\n      \"SONARQUBE_PASSWORD\": \"your-password\"\n    }\n  }\n  ```\n\n#### 3. System Passcode\n\n- Special authentication for SonarQube system administration\n- Typically used for automated deployment scenarios\n- Configuration:\n  ```json\n  {\n    \"env\": {\n      \"SONARQUBE_PASSCODE\": \"your-system-passcode\"\n    }\n  }\n  ```\n\n**Note:** Token authentication takes priority if multiple authentication methods are configured. The server will automatically use the appropriate authentication strategy based on your SonarQube version.\n\n### SonarCloud vs SonarQube\n\n**For SonarCloud:**\n\n- Set `SONARQUBE_URL` to `https://sonarcloud.io`\n- `SONARQUBE_ORGANIZATION` is required\n- Token authentication is recommended\n\n**For SonarQube Server:**\n\n- Set `SONARQUBE_URL` to your instance URL\n- `SONARQUBE_ORGANIZATION` is typically not needed\n- All authentication methods are supported\n\n### HTTP Transport Mode\n\nThe server supports HTTP transport for programmatic access and web service deployments. This enables integration with custom clients and web applications.\n\n#### Running as an HTTP Server\n\nStart the server with HTTP transport:\n\n```bash\n# Using environment variables\nMCP_TRANSPORT_TYPE=http MCP_HTTP_PORT=3000 npx sonarqube-mcp-server\n\n# With Docker\ndocker run -i --rm \\\n  -p 3000:3000 \\\n  -e MCP_TRANSPORT_TYPE=http \\\n  -e MCP_HTTP_PORT=3000 \\\n  -e SONARQUBE_URL=https://sonarcloud.io \\\n  -e SONARQUBE_TOKEN=your-token \\\n  sapientpants/sonarqube-mcp-server:latest\n```\n\n#### HTTP API Endpoints\n\nWhen running in HTTP mode, the server exposes the following endpoints:\n\n- `GET /health` - Health check endpoint\n- `POST /session` - Create a new session\n- `DELETE /session/:sessionId` - Close a session\n- `POST /mcp` - Execute MCP requests\n- `GET /events/:sessionId` - Server-sent events for notifications\n\n#### Example HTTP Client\n\nSee [examples/http-client.ts](examples/http-client.ts) for a complete TypeScript client example.\n\nBasic usage with curl:\n\n```bash\n# Health check\ncurl http://localhost:3000/health\n\n# Create session\nSESSION_ID=$(curl -X POST http://localhost:3000/session | jq -r .sessionId)\n\n# Execute MCP request\ncurl -X POST http://localhost:3000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"sessionId\\\": \\\"$SESSION_ID\\\",\n    \\\"method\\\": \\\"tools/list\\\",\n    \\\"params\\\": {}\n  }\"\n\n# Close session\ncurl -X DELETE http://localhost:3000/session/$SESSION_ID\n```\n\n#### Security Considerations\n\nWhen running in HTTP mode:\n\n1. **Enable DNS rebinding protection** for public deployments:\n\n   ```bash\n   MCP_HTTP_ENABLE_DNS_REBINDING_PROTECTION=true\n   ```\n\n2. **Configure CORS** for browser-based clients:\n\n   ```bash\n   MCP_HTTP_ALLOWED_ORIGINS=https://yourapp.com,https://anotherapp.com\n   ```\n\n3. **Set session timeouts** appropriately:\n\n   ```bash\n   MCP_HTTP_SESSION_TIMEOUT=900000  # 15 minutes\n   ```\n\n4. **Use HTTPS** in production (configure through a reverse proxy like nginx)\n\n### Elicitation Configuration (Experimental)\n\nThe server supports interactive user input through MCP's elicitation capability. This feature is opt-in and requires compatible MCP clients.\n\n**Environment Variables:**\n\n- `SONARQUBE_MCP_ELICITATION`: Set to `true` to enable elicitation\n- `SONARQUBE_MCP_BULK_THRESHOLD`: Number of items before confirmation (default: 5)\n- `SONARQUBE_MCP_REQUIRE_COMMENTS`: Set to `true` to require comments for resolutions\n- `SONARQUBE_MCP_INTERACTIVE_SEARCH`: Set to `true` for interactive disambiguation\n\n**Example Configuration:**\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sonarqube-mcp-server@latest\"],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://sonarcloud.io\",\n        \"SONARQUBE_TOKEN\": \"your-token\",\n        \"SONARQUBE_MCP_ELICITATION\": \"true\",\n        \"SONARQUBE_MCP_BULK_THRESHOLD\": \"10\",\n        \"SONARQUBE_MCP_REQUIRE_COMMENTS\": \"true\"\n      }\n    }\n  }\n}\n```\n\n**Features When Enabled:**\n\n1. **Bulk Operation Confirmation**: Prompts for confirmation before marking multiple issues\n2. **Comment Collection**: Collects explanatory comments when marking issues as false positive or won't fix\n3. **Authentication Setup**: Guides through authentication setup when credentials are missing\n4. **Search Disambiguation**: Helps select from multiple matching components or projects\n\n**Note:** This feature requires MCP clients that support elicitation. Not all clients may support this capability.\n\n### Logging Configuration\n\nThe server supports file-based logging for debugging and monitoring. Since MCP servers use stdout for protocol communication, logs are written to a file instead of stdout/stderr to avoid interference.\n\n**Enable Logging:**\n\n```json\n{\n  \"mcpServers\": {\n    \"sonarqube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sonarqube-mcp-server@latest\"],\n      \"env\": {\n        \"SONARQUBE_URL\": \"https://sonarcloud.io\",\n        \"SONARQUBE_TOKEN\": \"your-token-here\",\n        \"SONARQUBE_ORGANIZATION\": \"your-org\",\n        \"LOG_FILE\": \"/tmp/sonarqube-mcp.log\",\n        \"LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n\n**Log Levels:**\n\n- `DEBUG`: Detailed information for debugging\n- `INFO`: General information about server operation\n- `WARN`: Warning events that might lead to issues\n- `ERROR`: Error events (server continues running)\n\n**Example Log Output:**\n\n```\n2024-01-15T10:30:45.123Z INFO [index] Starting SonarQube MCP server\n2024-01-15T10:30:45.234Z INFO [index] Environment variables validated successfully\n2024-01-15T10:30:45.345Z INFO [index] SonarQube client created successfully\n2024-01-15T10:30:45.456Z INFO [index] SonarQube MCP server started successfully\n2024-01-15T10:30:50.123Z DEBUG [index] Handling SonarQube projects request\n2024-01-15T10:30:50.567Z INFO [index] Successfully retrieved projects {\"count\": 5}\n```\n\n## Available Tools\n\n### Permission Requirements\n\nDifferent SonarQube tools require different permission levels:\n\n**Tools requiring Admin permissions:**\n\n- `projects` - Lists all SonarQube projects with metadata (visibility, lastAnalysisDate, revision)\n\n**Tools accessible to all users:**\n\n- `components` - Search and navigate projects, directories, and files (requires 'Browse' permission on at least one project)\n- All other tools require appropriate permissions based on the resources being accessed\n\n#### Listing Projects\n\n**For Administrators:**\nUse the `projects` tool to get full project metadata including visibility, last analysis date, and revision info.\n\n**For All Users:**\nUse the `components` tool with project qualifier:\n\n- \"List all projects I have access to\" → `components` with `qualifiers: ['TRK']`\n- \"Search for projects containing 'mobile'\" → `components` with `query: 'mobile', qualifiers: ['TRK']`\n\nThe `components` tool provides a more accessible alternative for non-admin users to discover projects they have access to.\n\n### Project Management\n\n#### `projects`\n\nList all SonarQube projects with pagination support.\n\n**Parameters:**\n\n- `page` (optional): Page number for results pagination\n- `page_size` (optional): Number of items per page\n\n### Metrics and Measures\n\n#### `metrics`\n\nGet available metrics from SonarQube.\n\n**Parameters:**\n\n- `page` (optional): Page number for results pagination\n- `page_size` (optional): Number of items per page\n\n#### `measures_component`\n\nGet measures for a specific component.\n\n**Parameters:**\n\n- `component` (required): Component key\n- `metric_keys` (required): Array of metric keys\n- `additional_fields` (optional): Additional fields to return\n- `branch` (optional): Branch name\n- `pull_request` (optional): Pull request key\n- `period` (optional): Period index\n\n#### `measures_components`\n\nGet measures for multiple components.\n\n**Parameters:**\n\n- `component_keys` (required): Array of component keys\n- `metric_keys` (required): Array of metric keys\n- Additional parameters same as `measures_component`\n- `page` (optional): Page number\n- `page_size` (optional): Items per page\n\n#### `measures_history`\n\nGet measures history for a component.\n\n**Parameters:**\n\n- `component` (required): Component key\n- `metrics` (required): Array of metric keys\n- `from` (optional): Start date (YYYY-MM-DD)\n- `to` (optional): End date (YYYY-MM-DD)\n- `branch` (optional): Branch name\n- `pull_request` (optional): Pull request key\n- `page` (optional): Page number\n- `page_size` (optional): Items per page\n\n### Issue Management\n\n#### `issues`\n\nSearch and filter SonarQube issues by severity, status, assignee, tag, file path, and more. Critical for dashboards, targeted clean-up sprints, security audits, and regression testing. Supports faceted search for aggregations.\n\n**Component/File Path Filters:**\n\n- `project_key` (optional): Single project key (backward compatible)\n- `projects` (optional): Array of project keys for multi-project analysis\n- `component_keys` (optional): Array of component keys (file paths, directories, or modules) - use this to filter issues by specific files or folders\n- `components` (optional): Alias for component_keys\n- `on_component_only` (optional): Boolean to return only issues on specified components, not sub-components\n\n**Branch/PR Support:**\n\n- `branch` (optional): Branch name for branch analysis\n- `pull_request` (optional): Pull request ID for PR analysis\n\n**Issue Filters:**\n\n- `issues` (optional): Array of specific issue keys to retrieve\n- `severity` (optional): Single severity (deprecated, use severities)\n- `severities` (optional): Array of severities (INFO, MINOR, MAJOR, CRITICAL, BLOCKER)\n- `statuses` (optional): Array of statuses (OPEN, CONFIRMED, REOPENED, RESOLVED, CLOSED)\n- `resolutions` (optional): Array of resolutions (FALSE-POSITIVE, WONTFIX, FIXED, REMOVED)\n- `resolved` (optional): Boolean filter for resolved/unresolved\n- `types` (optional): Array of types (CODE_SMELL, BUG, VULNERABILITY, SECURITY_HOTSPOT)\n\n**Clean Code Taxonomy (SonarQube 10.x+):**\n\n- `clean_code_attribute_categories` (optional): Array (ADAPTABLE, CONSISTENT, INTENTIONAL, RESPONSIBLE)\n- `impact_severities` (optional): Array (HIGH, MEDIUM, LOW)\n- `impact_software_qualities` (optional): Array (MAINTAINABILITY, RELIABILITY, SECURITY)\n- `issue_statuses` (optional): Array of new issue status values\n\n**Rules and Tags:**\n\n- `rules` (optional): Array of rule keys\n- `tags` (optional): Array of issue tags - essential for security audits, regression testing, and categorized analysis\n\n**Date Filters:**\n\n- `created_after` (optional): Issues created after date (YYYY-MM-DD)\n- `created_before` (optional): Issues created before date (YYYY-MM-DD)\n- `created_at` (optional): Issues created on date (YYYY-MM-DD)\n- `created_in_last` (optional): Issues created in last period (e.g., \"30d\", \"1m\")\n\n**Assignment:**\n\n- `assigned` (optional): Boolean filter for assigned/unassigned\n- `assignees` (optional): Array of assignee logins - critical for targeted clean-up sprints and workload analysis\n- `author` (optional): Single author login\n- `authors` (optional): Array of author logins\n\n**Security Standards:**\n\n- `cwe` (optional): Array of CWE identifiers\n- `owasp_top10` (optional): Array of OWASP Top 10 categories\n- `owasp_top10_v2021` (optional): Array of OWASP Top 10 2021 categories\n- `sans_top25` (optional): Array of SANS Top 25 categories\n- `sonarsource_security` (optional): Array of SonarSource security categories\n- `sonarsource_security_category` (optional): Additional security categories\n\n**Other Filters:**\n\n- `languages` (optional): Array of programming languages\n- `facets` (optional): Array of facets to aggregate\n- `facet_mode` (optional): Facet aggregation mode ('effort' or 'count')\n- `since_leak_period` (optional): Boolean for leak period filter (deprecated)\n- `in_new_code_period` (optional): Boolean for new code period filter\n\n**Sorting:**\n\n- `s` (optional): Sort field (e.g., 'SEVERITY', 'CREATION_DATE', 'UPDATE_DATE')\n- `asc` (optional): Boolean for ascending sort direction (default: false)\n\n**Response Control:**\n\n- `additional_fields` (optional): Array of additional fields to include\n- `page` (optional): Page number for pagination\n- `page_size` (optional): Number of items per page\n\n**Faceted Search (Dashboard Support):**\n\n- `facets` (optional): Array of facets to compute for aggregations. Available facets: severities, statuses, resolutions, rules, tags, types, authors, assignees, languages, etc.\n- `facet_mode` (optional): Mode for facet computation: 'count' (number of issues) or 'effort' (remediation effort)\n\n**Example Use Cases:**\n\n1. **Dashboard Query** - Get issue counts by severity and assignee:\n\n```json\n{\n  \"project_key\": \"my-project\",\n  \"facets\": [\"severities\", \"assignees\", \"tags\"],\n  \"facet_mode\": \"count\"\n}\n```\n\n1. **Security Audit** - Find critical security issues in authentication modules:\n\n```json\n{\n  \"project_key\": \"my-project\",\n  \"component_keys\": [\"src/auth/\", \"src/security/\"],\n  \"tags\": [\"security\", \"vulnerability\"],\n  \"severities\": [\"CRITICAL\", \"BLOCKER\"],\n  \"statuses\": [\"OPEN\", \"REOPENED\"]\n}\n```\n\n1. **Sprint Planning** - Get open issues for specific team members:\n\n```json\n{\n  \"project_key\": \"my-project\",\n  \"assignees\": [\"john.doe@example.com\", \"jane.smith@example.com\"],\n  \"statuses\": [\"OPEN\", \"CONFIRMED\"],\n  \"facets\": [\"severities\", \"types\"],\n  \"facet_mode\": \"effort\"\n}\n```\n\n1. **File-Specific Analysis** - Issues in a specific file:\n\n```json\n{\n  \"project_key\": \"my-project\",\n  \"component_keys\": [\"src/main/java/com/example/PaymentService.java\"],\n  \"on_component_only\": true\n}\n```\n\n### Component Navigation\n\n#### `components`\n\nSearch and navigate SonarQube components (projects, directories, files). Supports text search, filtering by type/language, and tree navigation.\n\n**Search Parameters:**\n\n- `query` (optional): Text search query\n- `qualifiers` (optional): Array of component types (TRK, DIR, FIL, UTS, BRC, APP, VW, SVW, LIB)\n- `language` (optional): Programming language filter\n\n**Tree Navigation Parameters:**\n\n- `component` (optional): Component key for tree navigation\n- `strategy` (optional): Tree traversal strategy ('all', 'children', 'leaves')\n\n**Common Parameters:**\n\n- `asc` (optional): Sort ascending/descending\n- `ps` (optional): Page size (default: 100, max: 500)\n- `p` (optional): Page number\n- `branch` (optional): Branch name\n- `pullRequest` (optional): Pull request ID\n\n**Component Qualifiers:**\n\n- `TRK`: Project\n- `DIR`: Directory\n- `FIL`: File\n- `UTS`: Unit Test\n- `BRC`: Branch\n- `APP`: Application\n- `VW`: View\n- `SVW`: Sub-view\n- `LIB`: Library\n\n**Example Use Cases:**\n\n1. **Find specific files:**\n\n```json\n{\n  \"query\": \"UserService\",\n  \"qualifiers\": [\"FIL\"]\n}\n```\n\n1. **List all test files in a project:**\n\n```json\n{\n  \"component\": \"my-project\",\n  \"qualifiers\": [\"UTS\"]\n}\n```\n\n1. **Navigate directory structure:**\n\n```json\n{\n  \"component\": \"my-project:src/main\",\n  \"strategy\": \"children\",\n  \"qualifiers\": [\"DIR\", \"FIL\"]\n}\n```\n\n1. **Search for components by language:**\n\n```json\n{\n  \"language\": \"java\",\n  \"qualifiers\": [\"FIL\"],\n  \"query\": \"Controller\"\n}\n```\n\n1. **Get project list:**\n\n```json\n{\n  \"qualifiers\": [\"TRK\"]\n}\n```\n\n### Security Hotspots\n\n#### `hotspots`\n\nSearch for security hotspots with specialized filters for security review workflows.\n\n**Parameters:**\n\n- `project_key` (optional): Project key to filter hotspots\n- `branch` (optional): Branch name for branch analysis\n- `pull_request` (optional): Pull request ID for PR analysis\n- `status` (optional): Hotspot status (TO_REVIEW, REVIEWED)\n- `resolution` (optional): Hotspot resolution (FIXED, SAFE)\n- `files` (optional): Array of file paths to filter\n- `assigned_to_me` (optional): Boolean to show only assigned hotspots\n- `since_leak_period` (optional): Boolean for leak period filter\n- `in_new_code_period` (optional): Boolean for new code period filter\n- `page` (optional): Page number for pagination\n- `page_size` (optional): Number of items per page\n\n#### `hotspot`\n\nGet detailed information about a specific security hotspot including security context.\n\n**Parameters:**\n\n- `hotspot_key` (required): The unique key of the hotspot\n\n**Returns:**\n\n- Detailed hotspot information including:\n  - Security category and vulnerability probability\n  - Rule information and security context\n  - Changelog and comments\n  - Code flows and locations\n\n#### `update_hotspot_status`\n\nUpdate the status of a security hotspot (requires appropriate permissions).\n\n**Parameters:**\n\n- `hotspot_key` (required): The unique key of the hotspot\n- `status` (required): New status (TO_REVIEW, REVIEWED)\n- `resolution` (optional): Resolution when status is REVIEWED (FIXED, SAFE)\n- `comment` (optional): Comment explaining the status change\n\n### Quality Gates\n\n#### `quality_gates`\n\nList available quality gates.\n\nNo parameters required.\n\n#### `quality_gate`\n\nGet quality gate conditions.\n\n**Parameters:**\n\n- `id` (required): Quality gate ID\n\n#### `quality_gate_status`\n\nGet project quality gate status.\n\n**Parameters:**\n\n- `project_key` (required): Project key\n- `branch` (optional): Branch name\n- `pull_request` (optional): Pull request key\n\n### Source Code\n\n#### `source_code`\n\nView source code with issues highlighted.\n\n**Parameters:**\n\n- `key` (required): File key\n- `from` (optional): Start line\n- `to` (optional): End line\n- `branch` (optional): Branch name\n- `pull_request` (optional): Pull request key\n\n#### `scm_blame`\n\nGet SCM blame information for source code.\n\n**Parameters:**\n\n- Same as `source_code`\n\n### System Monitoring\n\n#### `system_health`\n\nGet the health status of the SonarQube instance.\n\nNo parameters required.\n\n#### `system_status`\n\nGet the status of the SonarQube instance.\n\nNo parameters required.\n\n#### `system_ping`\n\nPing the SonarQube instance to check if it is up.\n\nNo parameters required.\n\n### Issue Resolution and Management\n\n#### `markIssueFalsePositive`\n\nMark an issue as false positive.\n\n**Parameters:**\n\n- `issue_key` (required): The key of the issue to mark\n- `comment` (optional): Comment explaining why it's a false positive\n\n#### `markIssueWontFix`\n\nMark an issue as won't fix.\n\n**Parameters:**\n\n- `issue_key` (required): The key of the issue to mark\n- `comment` (optional): Comment explaining why it won't be fixed\n\n#### `markIssuesFalsePositive`\n\nMark multiple issues as false positive in bulk.\n\n**Parameters:**\n\n- `issue_keys` (required): Array of issue keys to mark\n- `comment` (optional): Comment applying to all issues\n\n#### `markIssuesWontFix`\n\nMark multiple issues as won't fix in bulk.\n\n**Parameters:**\n\n- `issue_keys` (required): Array of issue keys to mark\n- `comment` (optional): Comment applying to all issues\n\n#### `addCommentToIssue`\n\nAdd a comment to a SonarQube issue.\n\n**Parameters:**\n\n- `issue_key` (required): The key of the issue to comment on\n- `text` (required): The comment text (supports markdown formatting)\n\n#### `assignIssue`\n\nAssign a SonarQube issue to a user or unassign it.\n\n**Parameters:**\n\n- `issueKey` (required): The key of the issue to assign\n- `assignee` (optional): Username of the assignee. Leave empty to unassign the issue\n\n**Example usage:**\n\n```json\n{\n  \"issueKey\": \"PROJECT-123\",\n  \"assignee\": \"john.doe\"\n}\n```\n\n## Usage Examples\n\n### Basic Project Analysis\n\n```\n\"List all my SonarQube projects\"\n\"Show me the code coverage for project xyz\"\n\"What metrics are available for analysis?\"\n```\n\n### Issue Investigation\n\n```\n\"Show me all critical bugs in project abc\"\n\"Find security vulnerabilities in the main branch\"\n\"List all code smells created in the last week\"\n\"Show unresolved issues assigned to john.doe\"\n\"Analyze issues in the feature/new-login branch\"\n\"Compare issues between main and develop branches\"\n\"Find issues across multiple projects: proj1, proj2, proj3\"\n\"Show me issues sorted by severity in descending order\"\n\"Find all issues with clean code impact on reliability\"\n```\n\n### Component Navigation\n\n```\n\"Find all files containing 'UserService' in their name\"\n\"List all test files in my project\"\n\"Show me the directory structure of src/main\"\n\"Find all Java controller files\"\n\"List all projects in SonarQube\"\n\"Navigate to the authentication module\"\n\"Search for TypeScript files in the frontend directory\"\n\"Show me all directories under src/components\"\n```\n\n### Issue Management\n\n```\n\"Assign issue PROJECT-123 to john.doe\"\n\"Unassign issue PROJECT-456\"\n\"Mark issue ABC-789 as false positive with comment: 'Test code only'\"\n\"Add comment to issue XYZ-111: 'Fixed in commit abc123'\"\n\"Bulk mark issues DEF-222, DEF-223 as won't fix\"\n```\n\n### Quality Monitoring\n\n```\n\"Check the quality gate status for my main project\"\n\"Show me the code coverage history for the last month\"\n\"What are the quality gate conditions?\"\n\"Compare metrics between develop and main branches\"\n```\n\n### Security Hotspot Review\n\n```\n\"Find all security hotspots that need review in project xyz\"\n\"Show me hotspots in the authentication module\"\n\"Get details for hotspot HSP-12345\"\n\"List all hotspots assigned to me\"\n\"Mark hotspot HSP-12345 as safe with explanation\"\n\"Find hotspots in the new code period\"\n\"Show security hotspots in pull request #42\"\n```\n\n### Source Code Analysis\n\n```\n\"Show me the source code for file xyz with issues highlighted\"\n\"Get blame information for the problematic file\"\n\"View issues in the authentication module\"\n```\n\n### System Health\n\n```\n\"Check if SonarQube is running\"\n\"What's the health status of the SonarQube instance?\"\n\"Show me the system status\"\n```\n\n## Architecture\n\nThe SonarQube MCP Server follows a modular architecture:\n\n```\n┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n│  Claude Desktop │────▶│  MCP Server      │────▶│  SonarQube API  │\n│  (MCP Client)   │◀────│  (index.ts)      │◀────│                 │\n└─────────────────┘     └──────────────────┘     └─────────────────┘\n                               │\n                               ▼\n                        ┌──────────────────┐\n                        │  SonarQube       │\n                        │  Client          │\n                        │  (sonarqube.ts)  │\n                        └──────────────────┘\n                               │\n                               ▼\n                        ┌──────────────────┐\n                        │  API Module      │\n                        │  (api.ts)        │\n                        └──────────────────┘\n```\n\n### Key Components\n\n1. **MCP Server (`index.ts`)**: Main entry point that initializes the MCP server and registers all available tools\n2. **SonarQube Client (`sonarqube.ts`)**: Handles business logic and parameter transformation\n3. **API Module (`api.ts`)**: Manages HTTP requests to the SonarQube API\n4. **Type Definitions**: TypeScript interfaces for type safety\n\n### Data Flow\n\n1. MCP clients make requests through registered tools\n2. Tool handlers validate and transform parameters\n3. SonarQube client methods process the requests\n4. API module executes HTTP requests\n5. Responses are formatted and returned to the client\n\n## Development\n\n### Prerequisites\n\n- Node.js 22 or higher\n- pnpm 10.17.0 or higher\n- Docker (for container builds)\n\n### Setup\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/sapientpants/sonarqube-mcp-server.git\ncd sonarqube-mcp-server\n```\n\n1. Install dependencies:\n\n```bash\npnpm install\n```\n\n1. Build the project:\n\n```bash\npnpm build\n```\n\n### Development Commands\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Run in development mode with auto-reload\npnpm dev\n\n# Run tests\npnpm test\n\n# Run tests with coverage\npnpm test:coverage\n\n# Lint the code\npnpm lint\n\n# Fix linting issues\npnpm lint:fix\n\n# Check types\npnpm check-types\n\n# Format code\npnpm format\n\n# Run all validations\npnpm validate\n\n# Inspect MCP schema\npnpm inspect\n```\n\n### Testing\n\nThe project uses Jest for testing with:\n\n- Unit tests for all major components\n- Mocked HTTP responses using `nock`\n- Coverage reporting\n- TypeScript support\n\nRun specific test files:\n\n```bash\nNODE_ENV=test NODE_OPTIONS='--experimental-vm-modules --no-warnings' jest src/__tests__/file-name.test.ts\n```\n\n### Code Quality\n\nThe project maintains high code quality through:\n\n- TypeScript for type safety\n- ESLint for code linting\n- Prettier for code formatting\n- Jest for testing\n- SonarCloud for continuous code analysis\n\n## Common Issues and Solutions\n\n### Quick Fixes\n\n#### \"Authentication failed\"\n\n- **Cause**: Invalid or expired token\n- **Solution**: Generate a new token in SonarQube/SonarCloud\n\n#### \"Project not found\"\n\n- **Cause**: Incorrect project key or insufficient permissions\n- **Solution**: Verify the project key and check token permissions\n\n#### \"Organization required\"\n\n- **Cause**: Using SonarCloud without organization parameter\n- **Solution**: Add `SONARQUBE_ORGANIZATION` to your configuration\n\n#### \"Connection refused\"\n\n- **Cause**: Incorrect URL or network issues\n- **Solution**: Verify `SONARQUBE_URL` and network connectivity\n\n#### \"No output or errors visible\"\n\n- **Cause**: Errors might be happening but not visible in Claude Desktop\n- **Solution**: Enable logging with `LOG_FILE` and check the log file for detailed error messages\n\n### FAQ\n\n**Q: Can I use this with both SonarQube and SonarCloud?**\nA: Yes! Set the appropriate `SONARQUBE_URL` and include `SONARQUBE_ORGANIZATION` for SonarCloud.\n\n**Q: What permissions does my token need?**\nA: The token needs \"Execute Analysis\" permission and access to the projects you want to analyze.\n\n**Q: How do I filter issues by multiple criteria?**\nA: The `issues` tool supports extensive filtering. You can combine multiple parameters like severity, type, status, and date ranges.\n\n**Q: Can I analyze pull requests?**\nA: Yes! Many tools support `branch` and `pull_request` parameters for branch and PR analysis.\n\n## Troubleshooting\n\n### Common Error Messages and Solutions\n\n#### Authentication Errors\n\n##### Error: \"Authentication failed\"\n\n- **Solution**: Check that your SONARQUBE_TOKEN is valid and not expired. Generate a new token from your SonarQube user profile.\n\n##### Error: \"No SonarQube authentication configured\"\n\n- **Solution**: Set one of the following authentication methods:\n  - `SONARQUBE_TOKEN` for token-based authentication (recommended)\n  - `SONARQUBE_USERNAME` and `SONARQUBE_PASSWORD` for basic authentication\n  - `SONARQUBE_PASSCODE` for system passcode authentication\n\n#### Authorization Errors\n\n##### Error: \"Access denied\"\n\n- **Solution**: Ensure your token has the required permissions for the operation. Common required permissions:\n  - \"Execute Analysis\" for code analysis\n  - \"Browse\" for reading project data\n  - \"Administer Issues\" for issue management operations\n\n#### Resource Not Found Errors\n\n##### Error: \"Resource not found\"\n\n- **Solution**: Verify that:\n  - The project key/component exists in SonarQube\n  - You have access to the resource\n  - The URL path is correct (no typos in project keys)\n\n#### Network and Connection Errors\n\n##### Error: \"Connection refused\"\n\n- **Solution**: Check that:\n  - The SonarQube server is running\n  - The SONARQUBE_URL is correct\n  - There are no firewall rules blocking the connection\n\n##### Error: \"Network error\" or timeout errors\n\n- **Solution**:\n  - Verify your network connection\n  - Check if the SonarQube server is accessible\n  - Ensure the URL doesn't have a trailing slash\n  - For self-hosted instances, verify SSL certificates\n\n#### Rate Limiting\n\n##### Error: \"Rate limit exceeded\"\n\n- **Solution**: The server automatically retries rate-limited requests with exponential backoff. If you continue to hit rate limits:\n  - Reduce the frequency of your requests\n  - Implement request batching where possible\n  - Contact your SonarQube administrator to increase rate limits\n\n#### Configuration Errors\n\n##### Error: \"Invalid SONARQUBE_URL\"\n\n- **Solution**: Provide a valid URL including the protocol:\n  - ✅ Correct: `https://sonarcloud.io`\n  - ✅ Correct: `https://sonarqube.example.com`\n  - ❌ Wrong: `sonarcloud.io` (missing protocol)\n  - ❌ Wrong: `https://sonarqube.example.com/` (trailing slash)\n\n### Debugging Tips\n\n1. **Enable Debug Logging**:\n\n   ```bash\n   export LOG_LEVEL=DEBUG\n   ```\n\n2. **Check Environment Variables**:\n\n   ```bash\n   echo $SONARQUBE_URL\n   echo $SONARQUBE_TOKEN\n   echo $SONARQUBE_ORGANIZATION\n   ```\n\n3. **Test Connection**:\n   Use the `ping` tool to verify connectivity:\n\n   ```bash\n   # In your MCP client\n   sonarqube.ping\n   ```\n\n4. **Verify Permissions**:\n   Use the `projects` tool to list accessible projects:\n   ```bash\n   # In your MCP client\n   sonarqube.projects\n   ```\n\n### Retry Behavior\n\nThe server automatically retries failed requests for transient errors:\n\n- **Network errors**: Retried up to 3 times\n- **Rate limiting**: Retried with exponential backoff\n- **Server errors (5xx)**: Retried up to 3 times\n\nRetry delays: 1s → 2s → 4s (capped at 10s)\n\n### Getting Help\n\nIf you continue to experience issues:\n\n1. Check the [GitHub Issues](https://github.com/sapientpants/sonarqube-mcp-server/issues) for similar problems\n2. Enable debug logging and collect error details\n3. Create a new issue with:\n   - Error messages\n   - Environment details (OS, Node version)\n   - SonarQube version\n   - Steps to reproduce\n\n## Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.\n\n### How to Contribute\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n### Development Guidelines\n\n- Write tests for new features\n- Update documentation as needed\n- Follow the existing code style\n- Ensure all tests pass\n- Add appropriate error handling\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## External Resources\n\n### SonarQube Documentation\n\n- [SonarQube Documentation](https://docs.sonarqube.org/latest/)\n- [SonarCloud Documentation](https://docs.sonarcloud.io/)\n- [Web API Documentation](https://docs.sonarqube.org/latest/extend/web-api/)\n\n### Model Context Protocol\n\n- [MCP Documentation](https://modelcontextprotocol.io/)\n- [MCP Specification](https://github.com/modelcontextprotocol/specification)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n\n---\n\nMade with ❤️ by the SonarQube MCP Server community\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sonarqube",
        "logging",
        "ai",
        "sonarqube mcp",
        "integrates sonarqube",
        "sonarqube provide"
      ],
      "category": "monitoring-and-logging"
    },
    "seekrays--mcp-monitor": {
      "owner": "seekrays",
      "name": "mcp-monitor",
      "url": "https://github.com/seekrays/mcp-monitor",
      "imageUrl": "/freedevtools/mcp/pfp/seekrays.webp",
      "description": "Exposes system metrics such as CPU, memory, disk, network, and host information through an MCP-compatible interface, enabling real-time retrieval of system data for LLMs.",
      "stars": 71,
      "forks": 15,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-02T04:26:49Z",
      "readme_content": "# MCP System Monitor\n![Go](https://github.com/seekrays/mcp-monitor/actions/workflows/go.yml/badge.svg)\n![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/seekrays/mcp-monitor?sort=semver)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-blue?style=flat&logo=discord)](https://discord.gg/kbMJ9Qpf)\n\nA system monitoring tool that exposes system metrics via the Model Context Protocol (MCP). This tool allows LLMs to retrieve real-time system information through an MCP-compatible interface.\n\n![](./doc/snapshot-1.png)\n\n## Features\n\nThis tool provides the following monitoring capabilities:\n\n- **CPU Information**: Usage percentage, core count, and detailed CPU info\n- **Memory Information**: Virtual and swap memory usage\n- **Disk Information**: Disk usage, partitions, and I/O statistics\n- **Network Information**: Network interfaces, connections, and traffic statistics\n- **Host Information**: System details, uptime, boot time, and users\n- **Process Information**: Process listing, sorting, and detailed per-process statistics\n\n\n## Available Tools\n\n### 1. CPU Information\n\n```\nTool: get_cpu_info\nDescription: Get CPU information and usage\nParameters:\n  - per_cpu (boolean, default: false): Whether to return data for each core\n```\n\n### 2. Memory Information\n\n```\nTool: get_memory_info\nDescription: Get system memory usage information\nParameters: None\n```\n\n### 3. Disk Information\n\n```\nTool: get_disk_info\nDescription: Get disk usage information\nParameters:\n  - path (string, default: \"/\"): Specify the disk path to query\n  - all_partitions (boolean, default: false): Whether to return information for all partitions\n```\n\n### 4. Network Information\n\n```\nTool: get_network_info\nDescription: Get network interface and traffic information\nParameters:\n  - interface (string, optional): Specify the network interface name to query\n```\n\n### 5. Host Information\n\n```\nTool: get_host_info\nDescription: Get host system information\nParameters: None\n```\n\n### 6. Process Information\n\n```\nTool: get_process_info\nDescription: Get process information\nParameters:\n  - pid (number, optional): Process ID to get detailed information for a specific process\n  - limit (number, default: 10): Limit the number of processes returned\n  - sort_by (string, default: \"cpu\"): Sort field (cpu, memory, pid, name)\n```\n\n\n## Installation\n\n```bash\ngit clone https://github.com/seekrays/mcp-monitor.git\ncd mcp-monitor\nmake build\n```\n\n## Usage\n\nRun the compiled binary:\n\n```bash\n./mcp-monitor\n```\n\nThe server starts in stdio mode, ready to communicate with an MCP-compatible LLM client.\n\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "llms",
        "mcp",
        "mcp monitor",
        "logging seekrays",
        "seekrays mcp"
      ],
      "category": "monitoring-and-logging"
    },
    "serkanh--cloudwatch-logs-mcp": {
      "owner": "serkanh",
      "name": "cloudwatch-logs-mcp",
      "url": "https://github.com/serkanh/cloudwatch-logs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/serkanh.webp",
      "description": "Access and analyze AWS CloudWatch logs by listing log groups and retrieving log entries directly through an AI interface. Streamline log management and enhance monitoring capabilities.",
      "stars": 25,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-29T10:08:33Z",
      "readme_content": "# CloudWatch Logs MCP Server\n\nAn MCP (Model Context Protocol) server that provides tools for accessing AWS CloudWatch logs. This server allows AI assistants to list log groups and read log entries from AWS CloudWatch.\n\n## Available Tools\n\n### list_groups\n\nLists available CloudWatch log groups.\n\n**Parameters:**\n\n- `prefix` (optional): Log group name prefix\n- `region` (optional): AWS region\n- `accessKeyId` (optional): AWS access key ID\n- `secretAccessKey` (optional): AWS secret access key\n- `sessionToken` (optional): AWS session token\n\n**Returns:** JSON string with the list of log groups, including `logGroupName`, `creationTime`, and `storedBytes`.\n\n### get_logs\n\nGets CloudWatch logs from a specific log group.\n\n**Parameters:**\n\n- `logGroupName` (required): The name of the log group\n- `logStreamName` (optional): The name of the log stream\n- `startTime` (optional): Start time in ISO format or relative time (e.g., \"5m\", \"1h\", \"1d\")\n- `endTime` (optional): End time in ISO format\n- `filterPattern` (optional): Filter pattern for the logs\n- `region` (optional): AWS region\n- `accessKeyId` (optional): AWS access key ID\n- `secretAccessKey` (optional): AWS secret access key\n- `sessionToken` (optional): AWS session token\n\n**Returns:** JSON string with the log events, including `timestamp`, `message`, and `logStreamName`.\n\n## Setup\n\n### AWS Credentials\n\nEnsure you have AWS credentials configured. You can set them up using the AWS CLI or by setting environment variables:\n\n- `AWS_ACCESS_KEY_ID`\n- `AWS_SECRET_ACCESS_KEY`\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"cloudwatch-logs\": {\n      \"command\": \"python3\",\n      \"args\": [\"/path/to/cloudwatch-logs-mcp/main.py\"],\n      \"env\": {\n        \"AWS_ACCESS_KEY_ID\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"AWS_SECRET_ACCESS_KEY\": \"<YOUR_SECRET_ACCESS_KEY>\",\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Docker\n\nIf you prefer to run the server in a Docker container, you can set up a Dockerfile and use the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"cloudwatch-logs\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"AWS_ACCESS_KEY_ID\",\n        \"-e\",\n        \"AWS_SECRET_ACCESS_KEY\",\n        \"mcp/cloudwatch-logs\"\n      ],\n      \"env\": {\n        \"AWS_ACCESS_KEY_ID\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"AWS_SECRET_ACCESS_KEY\": \"<YOUR_SECRET_ACCESS_KEY>\",\n      }\n    }\n  }\n}\n```\n\n## Implementation Details\n\nThis server is built using the FastMCP class from the MCP SDK, which provides a simple way to create MCP servers. The server exposes two main tools:\n\n1. `list_groups`: Lists available CloudWatch log groups\n2. `get_logs`: Reads log entries from specific log groups\n\nEach tool is implemented as an async function decorated with `@mcp.tool()`. The server uses the boto3 library to interact with the AWS CloudWatch Logs API.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cloudwatch",
        "logging",
        "logs",
        "cloudwatch logs",
        "aws cloudwatch",
        "serkanh cloudwatch"
      ],
      "category": "monitoring-and-logging"
    },
    "signal-slot--mcp-systemd-coredump": {
      "owner": "signal-slot",
      "name": "mcp-systemd-coredump",
      "url": "https://github.com/signal-slot/mcp-systemd-coredump",
      "imageUrl": "/freedevtools/mcp/pfp/signal-slot.webp",
      "description": "Access and manage system core dumps using systemd functionality, enabling the listing, extraction, and removal of core dumps for effective debugging. Analyze core dump data to enhance system diagnostics and troubleshooting capabilities.",
      "stars": 2,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-06T04:03:09Z",
      "readme_content": "# systemd-coredump MCP Server\n\nA Model Context Protocol (MCP) server for interacting with systemd-coredump functionality. This enables MCP-capable applications to access, manage, and analyze system core dumps.\n\n[![npm version](https://img.shields.io/npm/v/@taskjp/server-systemd-coredump.svg?v=0.1.1)](https://www.npmjs.com/package/@taskjp/server-systemd-coredump)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Features\n\n- List all available coredumps in the system\n- Get detailed information about specific coredumps\n- Extract coredump files to a specified location\n- Remove coredumps from the system\n\n## Prerequisites\n\n- Node.js 18+ and npm\n- systemd-coredump must be installed and configured on the system\n- `coredumpctl` command-line utility must be available\n\n## Installation\n\n### From npm (recommended)\n\n#### Global Installation\n\n```bash\nnpm install -g @taskjp/server-systemd-coredump\n```\n\n#### Local Installation\n\n```bash\nnpm install @taskjp/server-systemd-coredump\n```\n\n### From Source\n\n1. Clone the repository or download the source code\n2. Install dependencies:\n\n```bash\ncd systemd-coredump-server\nnpm install\n```\n\n3. Build the server:\n\n```bash\nnpm run build\n```\n\n## Configuration\n\nAdd the server to your MCP settings configuration file:\n\n### If installed from npm globally:\n\n```json\n\"systemd-coredump\": {\n  \"command\": \"systemd-coredump-server\",\n  \"args\": [],\n  \"disabled\": false,\n  \"autoApprove\": []\n}\n```\n\n### If installed from npm locally:\n\n```json\n\"systemd-coredump\": {\n  \"command\": \"node\",\n  \"args\": [\"node_modules/@taskjp/server-systemd-coredump/build/index.js\"],\n  \"disabled\": false,\n  \"autoApprove\": []\n}\n```\n\n### If installed from source:\n\n```json\n\"systemd-coredump\": {\n  \"command\": \"node\",\n  \"args\": [\"/path/to/systemd-coredump-server/build/index.js\"],\n  \"disabled\": false,\n  \"autoApprove\": []\n}\n```\n\n## Usage\n\n### Available Tools\n\nThe server provides the following tools:\n\n1. **list_coredumps**: List all available coredumps in the system\n\n   ```json\n   {\n     \"name\": \"list_coredumps\"\n   }\n   ```\n\n2. **get_coredump_info**: Get detailed information about a specific coredump\n\n   ```json\n   {\n     \"name\": \"get_coredump_info\",\n     \"arguments\": {\n       \"id\": \"2023-04-20 12:34:56-12345\"\n     }\n   }\n   ```\n\n3. **extract_coredump**: Extract a coredump to a file\n\n   ```json\n   {\n     \"name\": \"extract_coredump\",\n     \"arguments\": {\n       \"id\": \"2023-04-20 12:34:56-12345\",\n       \"outputPath\": \"/path/to/output/core.dump\"\n     }\n   }\n   ```\n\n4. **remove_coredump**: Remove a coredump from the system\n\n   ```json\n   {\n     \"name\": \"remove_coredump\",\n     \"arguments\": {\n       \"id\": \"2023-04-20 12:34:56-12345\"\n     }\n   }\n   ```\n\n5. **get_coredump_config**: Get the current core dump configuration of the system\n\n   ```json\n   {\n     \"name\": \"get_coredump_config\"\n   }\n   ```\n\n   This tool returns information about the current core dump configuration, including:\n   - Whether core dumps are enabled\n   - The current core pattern\n   - The core size limit\n   - Whether systemd is handling the core dumps\n\n6. **set_coredump_enabled**: Enable or disable core dump generation\n\n   ```json\n   {\n     \"name\": \"set_coredump_enabled\",\n     \"arguments\": {\n       \"enabled\": true\n     }\n   }\n   ```\n\n   Setting `enabled` to `true` will enable core dumps, while `false` will disable them.\n   Note: This changes the ulimit settings for the current shell. For permanent system-wide\n   changes, root privileges and modification of system configuration files would be required.\n\n7. **get_stacktrace**: Get stack trace from a coredump using GDB\n\n   ```json\n   {\n     \"name\": \"get_stacktrace\",\n     \"arguments\": {\n       \"id\": \"2023-04-20 12:34:56-12345\"\n     }\n   }\n   ```\n\n   This tool uses GDB to extract a formatted stack trace from the coredump.\n   Note: Requires the GDB debugger to be installed on the system.\n\n### Available Resources\n\nThe server exposes two types of resources:\n\n1. **Coredump Information**\n   - URI format: `coredump:///<id>`\n   - Returns JSON with detailed coredump information\n\n2. **Stack Traces**\n   - URI format: `stacktrace:///<id>`\n   - Returns a formatted stack trace from the coredump\n\nWhere `<id>` is the unique identifier for a coredump in the format: `<timestamp>-<pid>`.\n\nFor example:\n\n```\ncoredump:///2023-04-20 12:34:56-12345\nstacktrace:///2023-04-20 12:34:56-12345\n```\n\n## Note on Permissions\n\nSome operations may require elevated privileges, especially when extracting or removing coredumps. Ensure the user running the MCP server has appropriate permissions to access system coredumps.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "coredump",
        "systemd",
        "core",
        "systemd coredump",
        "core dump",
        "core dumps"
      ],
      "category": "monitoring-and-logging"
    },
    "srtux--mcp": {
      "owner": "srtux",
      "name": "mcp",
      "url": "https://github.com/srtux/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/srtux.webp",
      "description": "Enables natural language queries for Google Cloud logs by converting user queries into Google Cloud Logging Query Language (LQL) and retrieving relevant log entries. Provides a REST API for integration and can be deployed on Google Cloud Run or GKE.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-21T08:30:18Z",
      "readme_content": "# MCP Logging NL Query Server\n\nThis project provides a Model Context Protocol (MCP) server that allows developers and AI Agents to query Google Cloud Logging using natural language. The server uses Vertex AI Gemini 2.5 to translate natural language queries into Google Cloud Logging Query Language (LQL), then queries Cloud Logging and returns the results.\n\n## Features\n- **Natural language to LQL translation** using Vertex AI Gemini 2.5\n- **Flexible log querying**: filter on monitored resource, log name, severity, time, and more\n- **REST API** for easy integration\n- **Ready for deployment** on Google Cloud Run or GKE\n\n## API Usage\n\n### Endpoints\n\n#### 1. Natural Language Query\n`POST /logs/nl_query`\n\nRequest:\n```json\n{\n  \"query\": \"Show me all error logs from yesterday for my Cloud Run service 'my-service'\",\n  \"max_results\": 20\n}\n```\nResponse:\n```json\n{\n  \"lql\": \"resource.type = \\\"cloud_run_revision\\\" AND resource.labels.service_name = \\\"my-service\\\" AND severity = ERROR AND timestamp >= \\\"2025-04-17T00:00:00Z\\\" AND timestamp < \\\"2025-04-18T00:00:00Z\\\"\",\n  \"entries\": [ ... log entries ... ]\n}\n```\n\n#### 2. LQL Filter Query\n`POST /logs/query`\n\nRequest:\n```json\n{\n  \"filter\": \"resource.type=\\\"cloud_run_revision\\\" AND severity=ERROR\",\n  \"max_results\": 20\n}\n```\nResponse:\n```json\n{\n  \"lql\": \"resource.type=\\\"cloud_run_revision\\\" AND severity=ERROR\",\n  \"entries\": [ ... log entries ... ]\n}\n```\n\n### OpenAPI & Tooling\n- OpenAPI/Swagger docs available at `/docs` and `/openapi.json` when running.\n- Both endpoints are also discoverable as MCP tools for agent frameworks (Smithery, Claude Desktop, etc).\n\n### Example curl commands\n```sh\ncurl -X POST $MCP_BASE_URL/logs/nl_query -H 'Content-Type: application/json' -d '{\"query\": \"Show error logs for my Cloud Run service\", \"max_results\": 2}'\n\ncurl -X POST $MCP_BASE_URL/logs/query -H 'Content-Type: application/json' -d '{\"filter\": \"resource.type=\\\"cloud_run_revision\\\" AND severity=ERROR\", \"max_results\": 2}'\n```\n\n### Tests\n- Example test script: `test_main.py` (see repo)\n\n### .gitignore\n- Standard Python ignores included (see repo)\n\n## Deployment\n\n### Running on Google Cloud Run\nYou can deploy this server to [Google Cloud Run](https://cloud.google.com/run) for a fully managed, scalable solution.\n\n**Steps:**\n1. **Build the Docker image:**\n   ```sh\n   gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/mcp-logging-server\n   ```\n2. **Deploy to Cloud Run:**\n   ```sh\n   gcloud run deploy mcp-logging-server \\\n     --image gcr.io/YOUR_PROJECT_ID/mcp-logging-server \\\n     --platform managed \\\n     --region YOUR_REGION \\\n     --allow-unauthenticated \\\n     --port 8080\n   ```\n   Replace `YOUR_PROJECT_ID` and `YOUR_REGION` with your actual GCP project ID and region (e.g., `us-central1`).\n\n3. **Set Environment Variables:**\n   - In the Cloud Run deployment UI or with the `--set-env-vars` flag, provide:\n     - `VERTEX_PROJECT=your-gcp-project-id`\n     - `VERTEX_LOCATION=us-central1` (or your region)\n   - **Credentials:**\n     - Prefer using the Cloud Run service account with the right IAM roles (Logging Viewer, Vertex AI User).\n     - You usually do NOT need to set `GOOGLE_APPLICATION_CREDENTIALS` on Cloud Run unless using a non-default service account key.\n\n4. **IAM Permissions:**\n   - Ensure the Cloud Run service account has:\n     - `roles/logging.viewer`\n     - `roles/aiplatform.user`\n\n5. **Accessing the Service:**\n   - After deployment, Cloud Run will provide a service URL (e.g., `https://mcp-logging-server-xxxxxx.a.run.app`).\n   - Use this as your `$MCP_BASE_URL` in API requests.\n\n### Google Cloud Authentication Setup\nThis project requires Google Cloud Application Default Credentials (ADC) to access Logging and Vertex AI APIs.\n\n**Steps to Set Up Credentials:**\n1. **Create a Service Account:**\n   - Go to the [Google Cloud Console → IAM & Admin → Service Accounts](https://console.cloud.google.com/iam-admin/serviceaccounts).\n   - Select your project.\n   - Create or select a service account with permissions: _Logging Viewer_ and _Vertex AI User_.\n2. **Create and Download a Key:**\n   - In the Service Account, click \"Manage keys\" → \"Add key\" → \"Create new key\" (choose JSON).\n   - Download the JSON key file to your computer.\n3. **Set the Environment Variable:**\n   - In your terminal, set the environment variable to the path of your downloaded key:\n     ```sh\n     export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/service-account-key.json\"\n     ```\n   - Replace `/path/to/your/service-account-key.json` with the actual path.\n4. **(Optional) Set Project and Location:**\n   - You may also need:\n     ```sh\n     export VERTEX_PROJECT=your-gcp-project-id\n     export VERTEX_LOCATION=us-central1\n     ```\n5. **Verify Authentication:**\n   - Run a simple `gcloud` or Python client call to ensure authentication is working.\n   - If you see `DefaultCredentialsError`, check your environment variable and file path.\n\n### Prerequisites\n- Python 3.9+\n- Google Cloud project with Logging and Vertex AI APIs enabled\n- Service account with permissions for Logging Viewer and Vertex AI User\n- Set environment variables:\n  - `VERTEX_PROJECT`: Your GCP project ID\n  - `VERTEX_LOCATION`: Vertex AI region (default: `us-central1`)\n  - `GOOGLE_APPLICATION_CREDENTIALS`: Path to your service account JSON key file\n\n### Local Development\n```sh\npip install -r requirements.txt\nexport VERTEX_PROJECT=your-project-id\nexport VERTEX_LOCATION=us-central1\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json\npython main.py\n```\n\n### Deploy to Cloud Run\n```sh\ngcloud builds submit --tag gcr.io/$VERTEX_PROJECT/mcp-logging-server\n gcloud run deploy mcp-logging-server \\\n    --image gcr.io/$VERTEX_PROJECT/mcp-logging-server \\\n    --platform managed \\\n    --region $VERTEX_LOCATION \\\n    --allow-unauthenticated\n```\n\n## Example Natural Language Queries\n- Show all logs from Kubernetes clusters\n- Show error logs from Compute Engine and AWS EC2 instances\n- Find Admin Activity audit logs for project my-project\n- Find logs containing the word unicorn\n- Find logs with both unicorn and phoenix\n- Find logs where textPayload contains both unicorn and phoenix\n- Find logs where textPayload contains the phrase 'unicorn phoenix'\n- Show logs from yesterday for Cloud Run service 'my-service'\n- Show logs from the last 30 minutes\n- Show logs for logName containing request_log in GKE\n- Show logs where pod_name matches foo or bar using regex\n- Show logs for Compute Engine where severity is WARNING or higher\n- Show logs for Cloud SQL instances in us-central1\n- Show logs for Pub/Sub topics containing 'payments'\n- Show logs for log entries between two timestamps\n- Show logs where jsonPayload.message matches regex 'foo.*bar'\n- Show logs where labels.env is not prod\n\nFor more LQL examples, see the [official documentation](https://cloud.google.com/logging/docs/view/logging-query-language).\n\n## License\nApache 2.0\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logging",
        "logs",
        "log",
        "logging srtux",
        "cloud logging",
        "cloud logs"
      ],
      "category": "monitoring-and-logging"
    },
    "truaxki--mcp-variance-log": {
      "owner": "truaxki",
      "name": "mcp-variance-log",
      "url": "https://github.com/truaxki/mcp-variance-log",
      "imageUrl": "/freedevtools/mcp/pfp/truaxki.webp",
      "description": "Logs unusual events in conversations and analyzes statistical variations, storing the results in a SQLite database. Designed for use with MCP-compatible clients like Claude Desktop, it enhances interaction by monitoring conversation structure.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-26T00:17:05Z",
      "readme_content": "# MCP Variance Log\n\nAgentic tool that looks for statistical variations in conversation structure and logs unusual events to a SQLite database. Built using the Model Context Protocol (MCP), this system is designed to be used with Claude Desktop or other MCP-compatible clients.\n\n<a href=\"https://glama.ai/mcp/servers/acqq8twrk6\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/acqq8twrk6/badge\" alt=\"Variance Log MCP server\" /></a>\n\n## Requirements\n\n- Python ≥3.10\n- MCP ≥1.2.0\n- SQLite3\n- UV (Python package installer and environment manager)\n\n## Quick Start\n\n```bash\n# Clone and install\ngit clone https://github.com/truaxki/mcp-variance-log.git\ncd mcp-variance-log\nuv pip install -e .\n```\n\nAdd to Claude Desktop config (`%APPDATA%\\Claude\\claude_desktop_config.json` on Windows):\n```json\n{\n  \"mcpServers\": {\n    \"mcp-variance-log\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\Users\\\\username\\\\source\\\\repos\\\\mcp-variance-log\",  # Update this path\n        \"run\",\n        \"mcp-variance-log\"\n      ]\n    }\n  }\n}\n```\n\nAdd to Claude Desktop preferences:\n```\nAutomatic Logging Process:\n- Silently monitor each interaction\n- When MEDIUM or LOW probability events occur, use this tool to log them\n- Each log includes context and reasoning\n- Logging happens without disrupting conversation flow\n```\n\n## Usage\n\n### Automatic Logging Examples\n```\nprompt: \"What if I told you I could lift 1000 lbs?\"\n\n{\n  `user_id`: `user_1`,\n  `reasoning`: `Claim of 1000 lb lift capability is highly unusual as it approaches world record territory, warranting logging as a MEDIUM probability event`,\n  `session_id`: `session_1`,\n  `context_summary`: `User made an extremely improbable claim about personal strength capabilities`,\n  `message_content`: `What if i told you i could lift 1000 lbs?`,\n  `interaction_type`: `claim_verification`,\n  `response_content`: `That would be extremely unlikely. The current world record for weight lifted by a human (deadlift) is 1,104.5 lbs, set by Hafthor Björnsson in 2020 under very specific conditions. Only a handful of people in history have lifted close to 1,000 lbs.`,\n  `probability_class`: `MEDIUM`\n}\n```\n\n```\nprompt: \"What if I told you I'm a Data Scientist that can lift 1000 pounds?\"\n\n{\n  `user_id`: `user_1`,\n  `reasoning`: `User's follow-up combines unlikely strength claim with unrelated professional credential, suggesting potential pattern of unusual claims`,\n  `session_id`: `session_1`,\n  `context_summary`: `User added data scientist credential to previous improbable strength claim`,\n  `message_content`: `what if i told you I'm a data scientist that can lift 1000 pounds?`,\n  `interaction_type`: `claim_verification`,\n  `response_content`: `Your professional background as a data scientist doesn't make the 1000-pound lift claim more plausible. Such a lift would still be near the all-time world record of 1,104.5 lbs.`,\n  `probability_class`: `LOW`\n}\n```\n\n### Dataframe view\n```\nprompt: \"Read-logs from this morning\"\n\nID   | Time         | Prob   | Type   | Context                                      \n------------------------------------------------------------------------------------------\n29   | 01-24 17:57  | LOW    | claim_ | User added data scientist credential to pr...\n28   | 01-24 17:56  | MEDIUM | claim_ | User made an extremely improbable claim ab...\n```\n\n### Text 2 SQL\n```\nprompt: \"Can you search the logs for entry 29?\"\n\n[{'log_id': 29, 'timestamp': '2025-01-24 17:57:07', 'session_id': 'session_1', 'user_id': 'user_1', 'interaction_type': 'claim_verification', 'probability_class': 'LOW', 'message_content': \"what if i told you I'm a data scientist that can lift 1000 pounds?\", 'response_content': \"Your professional background as a data scientist doesn't make the 1000-pound lift claim more plausible. Such a lift would still be near the all-time world record of 1,104.5 lbs.\", 'context_summary': 'User added data scientist credential to previous improbable strength claim', 'reasoning': \"User's follow-up combines unlikely strength claim with unrelated professional credential, suggesting potential pattern of unusual claims\"}]\n```\n\n\n## Detailed Installation\n\n1. Ensure Python 3.10+ and UV are installed.\n\nInstall UV using one of these methods:\n\n```bash\n# Using pip (recommended for Windows)\npip install uv\n\n# Using installation script (Linux/MacOS)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n```\n\n2. Clone and install:\n```bash\ngit clone https://github.com/truaxki/mcp-variance-log.git\ncd mcp-variance-log\nuv pip install -e .\n```\n\n3. Configure Claude Desktop:\n\nAdd to `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-variance-log\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"PATH_TO_REPO/mcp-variance-log\",\n        \"run\",\n        \"mcp-variance-log\"\n      ]\n    }\n  }\n}\n```\n\nConfig locations:\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\n## Tools\n\n### Monitoring\n- `log-query`: Tracks conversation patterns\n  - HIGH: Common interactions (not logged)\n  - MEDIUM: Unusual patterns (logged)\n  - LOW: Critical events (priority logged)\n\n### Query\n- `read-logs`: View logs with filtering\n- `read_query`: Execute SELECT queries\n- `write_query`: Execute INSERT/UPDATE/DELETE\n- `create_table`: Create tables\n- `list_tables`: Show all tables\n- `describe_table`: Show table structure\n\n\nLocated at `data/varlog.db` relative to installation.\n\n### Schema\n\n```sql\nCREATE TABLE chat_monitoring (\n    log_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n    session_id TEXT NOT NULL,\n    user_id TEXT NOT NULL,\n    interaction_type TEXT NOT NULL,\n    probability_class TEXT CHECK(probability_class IN ('HIGH', 'MEDIUM', 'LOW')),\n    message_content TEXT NOT NULL,\n    response_content TEXT NOT NULL,\n    context_summary TEXT,\n    reasoning TEXT\n);\n```\n\n## Troubleshooting\n\n1. Database Access\n- Error: \"Failed to connect to database\"\n  - Check file permissions\n  - Verify path in config\n  - Ensure `/data` directory exists\n  \n2. Installation Issues\n- Error: \"No module named 'mcp'\"\n  - Run: `uv pip install mcp>=1.2.0`\n- Error: \"UV command not found\"\n  - Install UV: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n  \n3. Configuration\n- Error: \"Failed to start MCP server\"\n  - Verify config.json syntax\n  - Check path separators (use \\\\ on Windows)\n  - Ensure UV is in your system PATH\n\n## Contributing\n\n1. Fork the repository\n2. Create feature branch\n3. Submit pull request\n\n## License\n\nMIT\n\n## Support\n\nIssues: [GitHub Issues](https://github.com/truaxki/mcp-variance-log/issues)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logging",
        "logs",
        "monitoring",
        "monitoring conversation",
        "conversations analyzes",
        "logging truaxki"
      ],
      "category": "monitoring-and-logging"
    },
    "twodoorsdev--react-native-debugger-mcp": {
      "owner": "twodoorsdev",
      "name": "react-native-debugger-mcp",
      "url": "https://github.com/twodoorsdev/react-native-debugger-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/twodoorsdev.webp",
      "description": "Connects to a React Native application debugger to retrieve console logs from Metro, facilitating real-time log access for debugging. Aids in identifying and resolving issues more efficiently during app development.",
      "stars": 26,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-22T15:35:56Z",
      "readme_content": "# React Native Debugger MCP\n\nAn MCP server that connects to your React Native application debugger.\n\n## ✨ Key Features\n\n- Can retrieve console logs from Metro\n\n## 🚀 Quick Start\n\nAdd the following to your Claude Desktop/Cursor MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"react-native-debugger-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@twodoorsdev/react-native-debugger-mcp\"]\n    }\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "debugger",
        "debugging",
        "react",
        "native debugger",
        "react native",
        "debugger mcp"
      ],
      "category": "monitoring-and-logging"
    },
    "westsideori--cursor-a11y-mcp": {
      "owner": "westsideori",
      "name": "cursor-a11y-mcp",
      "url": "https://github.com/westsideori/cursor-a11y-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/westsideori.webp",
      "description": "Run accessibility tests on web applications to identify compliance issues and enhance digital accessibility. Utilize axe-core and Puppeteer to generate detailed violation reports with information about impact levels and affected elements.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-16T21:34:07Z",
      "readme_content": "# Cursor A11y MCP\n\nA Model Context Protocol (MCP) server that provides accessibility testing capabilities AI agents. This tool helps identify accessibility issues in web applications using axe-core and Puppeteer.\n\n<a href=\"https://glama.ai/mcp/servers/mik2l7a1tw\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mik2l7a1tw/badge\" alt=\"Cursor A11y MCP server\" />\n</a>\n\n## Features\n\n- Run accessibility tests on any URL or local development server\n- Powered by axe-core for comprehensive accessibility testing\n- Provides detailed violation reports including:\n  - Impact level\n  - Description of the issue\n  - Help text and documentation links\n  - Affected HTML elements\n  - Failure summaries\n\n## Project Structure\n\n- `src/` - Source code for the MCP server and accessibility testing tool\n- `test-site/` - A React application with intentional accessibility issues for testing\n- `build/` - Compiled version of the source code\n\n## Installation\n\n```bash\nnpm install\n```\n\nThen install the test site dependencies:\n\n```bash\ncd test-site\nnpm install\ncd ..\n```\n\n## Usage\n\n### Starting the MCP Server\n\n```bash\nnpm run build\nnpm start\n```\n\n### Running the Test Site\n\n```bash\nnpm run start:test-site\n```\n\nThe test site will be available at `http://localhost:5000`.\n\n### Running Accessibility Tests\n\nThe tool accepts two types of inputs:\n\n1. A full URL to test\n2. A relative path that will be appended to `http://localhost:5000`\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk`: ^1.4.1\n- `puppeteer`: ^24.1.1\n- `zod`: ^3.24.1\n\n### Test Site Dependencies\n\n- `react`: ^18.2.0\n- `react-dom`: ^18.2.0\n- `react-scripts`: 5.0.1\n\n## Development\n\n1. Make changes to the source code in the `src/` directory\n2. Run `npm run build` to compile the changes\n3. Start the server with `npm start`\n\n## Configuring in Cursor\n\nTo add this accessibility testing tool to Cursor's MCP Server settings:\n\n1. Open Cursor's Settings (⌘ + ,)\n2. Navigate to \"Features\" > \"MCP Servers\"\n3. Add a new MCP Server with the following configuration:\n   - Name: `a11y`\n   - Select `command` from the dropdown\n   - Command: `node path/to/cursor-a11y-mcp/index/file/in/build/folder`\n     (Replace `path/to/cursor-a11y-mcp/index/file/in/build/folder` with the absolute path to your index.js file in the build folder.)\n4. Click `Add`\n5. The accessibility testing tool will now be available in Cursor's Composer\n\n## Usage in Composer\n\nTo use the accessibility testing tool in Cursor's Composer:\n\n1. Run in your terminal:\n\n```bash\nnpm run start:test-site\n```\n\nThis will start the test site at `http://localhost:5000`\n\n2. In Cursor's Composer, type `use a11y tool`\n3. Composer will prompt you to run the tool\n4. After running the tool, you will see the accessibility violations in the response, and code actions to fix the violations\n5. The Composer may prompt you to use the tool again to confirm that the violations are fixed\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Version\n\nCurrent version: 2.0.1",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "accessibility",
        "compliance",
        "cursor",
        "accessibility tests",
        "accessibility utilize",
        "run accessibility"
      ],
      "category": "monitoring-and-logging"
    },
    "winor30--mcp-server-datadog": {
      "owner": "winor30",
      "name": "mcp-server-datadog",
      "url": "https://github.com/winor30/mcp-server-datadog",
      "imageUrl": "/freedevtools/mcp/pfp/winor30.webp",
      "description": "Integrates with the Datadog API to facilitate access to monitoring features, including incident management, logs, and metrics. Supports streamlined observability processes for enhanced incident response and monitoring capabilities.",
      "stars": 101,
      "forks": 44,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-30T12:07:04Z",
      "readme_content": "# Datadog MCP Server\n\n> **DISCLAIMER**: This is a community-maintained project and is not officially affiliated with, endorsed by, or supported by Datadog, Inc. This MCP server utilizes the Datadog API but is developed independently as part of the [Model Context Protocol](https://github.com/modelcontextprotocol/servers) ecosystem.\n\n![NPM Version](https://img.shields.io/npm/v/%40winor30%2Fmcp-server-datadog)![Build and Test](https://github.com/winor30/mcp-server-datadog/actions/workflows/ci.yml/badge.svg)[![codecov](https://codecov.io/gh/winor30/mcp-server-datadog/graph/badge.svg?token=BG4ZB74X92)](https://codecov.io/gh/winor30/mcp-server-datadog)[![smithery badge](https://smithery.ai/badge/@winor30/mcp-server-datadog)](https://smithery.ai/server/@winor30/mcp-server-datadog)\n\nMCP server for the Datadog API, enabling incident management and more.\n\n<a href=\"https://glama.ai/mcp/servers/bu8gtzkwfr\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/bu8gtzkwfr/badge\" alt=\"mcp-server-datadog MCP server\" />\n</a>\n\n## Features\n\n- **Observability Tools**: Provides a mechanism to leverage key Datadog monitoring features, such as incidents, monitors, logs, dashboards, and metrics, through the MCP server.\n- **Extensible Design**: Designed to easily integrate with additional Datadog APIs, allowing for seamless future feature expansion.\n\n## Tools\n\n1. `list_incidents`\n\n   - Retrieve a list of incidents from Datadog.\n   - **Inputs**:\n     - `filter` (optional string): Filter parameters for incidents (e.g., status, priority).\n     - `pagination` (optional object): Pagination details like page size/offset.\n   - **Returns**: Array of Datadog incidents and associated metadata.\n\n2. `get_incident`\n\n   - Retrieve detailed information about a specific Datadog incident.\n   - **Inputs**:\n     - `incident_id` (string): Incident ID to fetch details for.\n   - **Returns**: Detailed incident information (title, status, timestamps, etc.).\n\n3. `get_monitors`\n\n   - Fetch the status of Datadog monitors.\n   - **Inputs**:\n     - `groupStates` (optional array): States to filter (e.g., alert, warn, no data, ok).\n     - `name` (optional string): Filter by name.\n     - `tags` (optional array): Filter by tags.\n   - **Returns**: Monitors data and a summary of their statuses.\n\n4. `get_logs`\n\n   - Search and retrieve logs from Datadog.\n   - **Inputs**:\n     - `query` (string): Datadog logs query string.\n     - `from` (number): Start time in epoch seconds.\n     - `to` (number): End time in epoch seconds.\n     - `limit` (optional number): Maximum number of logs to return (defaults to 100).\n   - **Returns**: Array of matching logs.\n\n5. `list_dashboards`\n\n   - Get a list of dashboards from Datadog.\n   - **Inputs**:\n     - `name` (optional string): Filter dashboards by name.\n     - `tags` (optional array): Filter dashboards by tags.\n   - **Returns**: Array of dashboards with URL references.\n\n6. `get_dashboard`\n\n   - Retrieve a specific dashboard from Datadog.\n   - **Inputs**:\n     - `dashboard_id` (string): ID of the dashboard to fetch.\n   - **Returns**: Dashboard details including title, widgets, etc.\n\n7. `query_metrics`\n\n   - Retrieve metrics data from Datadog.\n   - **Inputs**:\n     - `query` (string): Metrics query string.\n     - `from` (number): Start time in epoch seconds.\n     - `to` (number): End time in epoch seconds.\n   - **Returns**: Metrics data for the queried timeframe.\n\n8. `list_traces`\n\n   - Retrieve a list of APM traces from Datadog.\n   - **Inputs**:\n     - `query` (string): Datadog APM trace query string.\n     - `from` (number): Start time in epoch seconds.\n     - `to` (number): End time in epoch seconds.\n     - `limit` (optional number): Maximum number of traces to return (defaults to 100).\n     - `sort` (optional string): Sort order for traces (defaults to '-timestamp').\n     - `service` (optional string): Filter by service name.\n     - `operation` (optional string): Filter by operation name.\n   - **Returns**: Array of matching traces from Datadog APM.\n\n9. `list_hosts`\n\n   - Get list of hosts from Datadog.\n   - **Inputs**:\n     - `filter` (optional string): Filter string for search results.\n     - `sort_field` (optional string): Field to sort hosts by.\n     - `sort_dir` (optional string): Sort direction (asc/desc).\n     - `start` (optional number): Starting offset for pagination.\n     - `count` (optional number): Max number of hosts to return (max: 1000).\n     - `from` (optional number): Search hosts from this UNIX timestamp.\n     - `include_muted_hosts_data` (optional boolean): Include muted hosts status and expiry.\n     - `include_hosts_metadata` (optional boolean): Include host metadata (version, platform, etc).\n   - **Returns**: Array of hosts with details including name, ID, aliases, apps, mute status, and more.\n\n10. `get_active_hosts_count`\n\n    - Get the total number of active hosts in Datadog.\n    - **Inputs**:\n      - `from` (optional number): Number of seconds from which you want to get total number of active hosts (defaults to 2h).\n    - **Returns**: Count of total active and up hosts.\n\n11. `mute_host`\n\n    - Mute a host in Datadog.\n    - **Inputs**:\n      - `hostname` (string): The name of the host to mute.\n      - `message` (optional string): Message to associate with the muting of this host.\n      - `end` (optional number): POSIX timestamp for when the mute should end.\n      - `override` (optional boolean): If true and the host is already muted, replaces existing end time.\n    - **Returns**: Success status and confirmation message.\n\n12. `unmute_host`\n\n    - Unmute a host in Datadog.\n    - **Inputs**:\n      - `hostname` (string): The name of the host to unmute.\n    - **Returns**: Success status and confirmation message.\n\n13. `list_downtimes`\n\n    - List scheduled downtimes from Datadog.\n    - **Inputs**:\n      - `currentOnly` (optional boolean): Return only currently active downtimes when true.\n      - `monitorId` (optional number): Filter by monitor ID.\n    - **Returns**: Array of scheduled downtimes with details including scope, monitor information, and schedule.\n\n14. `schedule_downtime`\n\n    - Schedule a downtime in Datadog.\n    - **Inputs**:\n      - `scope` (string): Scope to apply downtime to (e.g. 'host:my-host').\n      - `start` (optional number): UNIX timestamp for the start of the downtime.\n      - `end` (optional number): UNIX timestamp for the end of the downtime.\n      - `message` (optional string): A message to include with the downtime.\n      - `timezone` (optional string): The timezone for the downtime (e.g. 'UTC', 'America/New_York').\n      - `monitorId` (optional number): The ID of the monitor to mute.\n      - `monitorTags` (optional array): A list of monitor tags for filtering.\n      - `recurrence` (optional object): Recurrence settings for the downtime.\n        - `type` (string): Recurrence type ('days', 'weeks', 'months', 'years').\n        - `period` (number): How often to repeat (must be >= 1).\n        - `weekDays` (optional array): Days of the week for weekly recurrence.\n        - `until` (optional number): UNIX timestamp for when the recurrence ends.\n    - **Returns**: Scheduled downtime details including ID and active status.\n\n15. `cancel_downtime`\n\n    - Cancel a scheduled downtime in Datadog.\n    - **Inputs**:\n      - `downtimeId` (number): The ID of the downtime to cancel.\n    - **Returns**: Confirmation of downtime cancellation.\n\n16. `get_rum_applications`\n\n    - Get all RUM applications in the organization.\n    - **Inputs**: None.\n    - **Returns**: List of RUM applications.\n\n17. `get_rum_events`\n\n    - Search and retrieve RUM events from Datadog.\n    - **Inputs**:\n      - `query` (string): Datadog RUM query string.\n      - `from` (number): Start time in epoch seconds.\n      - `to` (number): End time in epoch seconds.\n      - `limit` (optional number): Maximum number of events to return (default: 100).\n    - **Returns**: Array of RUM events.\n\n18. `get_rum_grouped_event_count`\n\n    - Search, group and count RUM events by a specified dimension.\n    - **Inputs**:\n      - `query` (optional string): Additional query filter for RUM search (default: \"\\*\").\n      - `from` (number): Start time in epoch seconds.\n      - `to` (number): End time in epoch seconds.\n      - `groupBy` (optional string): Dimension to group results by (default: \"application.name\").\n    - **Returns**: Grouped event counts.\n\n19. `get_rum_page_performance`\n\n    - Get page (view) performance metrics from RUM data.\n    - **Inputs**:\n      - `query` (optional string): Additional query filter for RUM search (default: \"\\*\").\n      - `from` (number): Start time in epoch seconds.\n      - `to` (number): End time in epoch seconds.\n      - `metricNames` (array of strings): Array of metric names to retrieve (e.g., 'view.load_time', 'view.first_contentful_paint').\n    - **Returns**: Performance metrics including average, min, max, and count for each metric.\n\n20. `get_rum_page_waterfall`\n\n    - Retrieve RUM page (view) waterfall data filtered by application name and session ID.\n    - **Inputs**:\n      - `applicationName` (string): Application name to filter events.\n      - `sessionId` (string): Session ID to filter events.\n    - **Returns**: Waterfall data for the specified application and session.\n\n## Setup\n\n### Datadog Credentials\n\nYou need valid Datadog API credentials to use this MCP server:\n\n- `DATADOG_API_KEY`: Your Datadog API key\n- `DATADOG_APP_KEY`: Your Datadog Application key\n- `DATADOG_SITE` (optional): The Datadog site (e.g. `datadoghq.eu`)\n\nExport them in your environment before running the server:\n\n```bash\nexport DATADOG_API_KEY=\"your_api_key\"\nexport DATADOG_APP_KEY=\"your_app_key\"\nexport DATADOG_SITE=\"your_datadog_site\"\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install Datadog MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@winor30/mcp-server-datadog):\n\n```bash\nnpx -y @smithery/cli install @winor30/mcp-server-datadog --client claude\n```\n\n### Manual Installation\n\n```bash\npnpm install\npnpm build\npnpm watch   # for development with auto-rebuild\n```\n\n## Usage with Claude Desktop\n\nTo use this with Claude Desktop, add the following to your `claude_desktop_config.json`:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`  \nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"datadog\": {\n      \"command\": \"/path/to/mcp-server-datadog/build/index.js\",\n      \"env\": {\n        \"DATADOG_API_KEY\": \"<YOUR_API_KEY>\",\n        \"DATADOG_APP_KEY\": \"<YOUR_APP_KEY>\",\n        \"DATADOG_SITE\": \"<YOUR_SITE>\" // Optional\n      }\n    }\n  }\n}\n```\n\nOr specify via `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-datadog\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@winor30/mcp-server-datadog\"],\n      \"env\": {\n        \"DATADOG_API_KEY\": \"<YOUR_API_KEY>\",\n        \"DATADOG_APP_KEY\": \"<YOUR_APP_KEY>\",\n        \"DATADOG_SITE\": \"<YOUR_SITE>\" // Optional\n      }\n    }\n  }\n}\n```\n\n## Debugging\n\nBecause MCP servers communicate over standard input/output, debugging can sometimes be tricky. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector). You can run the inspector with:\n\n```bash\nnpm run inspector\n```\n\nThe inspector will provide a URL you can open in your browser to see logs and send requests manually.\n\n## Contributing\n\nContributions are welcome! Feel free to open an issue or a pull request if you have any suggestions, bug reports, or improvements to propose.\n\n## License\n\nThis project is licensed under the [Apache License, Version 2.0](./LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datadog",
        "logging",
        "monitoring",
        "server datadog",
        "datadog api",
        "datadog integrates"
      ],
      "category": "monitoring-and-logging"
    },
    "xzq-xu--jvm-mcp-server": {
      "owner": "xzq-xu",
      "name": "jvm-mcp-server",
      "url": "https://github.com/xzq-xu/jvm-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/xzq-xu.webp",
      "description": "Monitor and analyze Java processes with real-time insights into JVM performance, memory usage, and thread information through a Python interface. Provides capabilities such as thread stack trace analysis, dynamic log adjustments, and AI-driven performance analysis.",
      "stars": 69,
      "forks": 15,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T02:43:52Z",
      "readme_content": "# JVM MCP Server\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/Python-3.6+-blue.svg\" alt=\"Python Version\">\n  <img src=\"https://img.shields.io/badge/JDK-8+-green.svg\" alt=\"JDK Version\">\n  <img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" alt=\"License\">\n</p>\n\n[English](README.md) | [中文](README_zh.md)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/xzq-xu-jvm-mcp-server-badge.png)](https://mseep.ai/app/xzq-xu-jvm-mcp-server)\n\n\nA lightweight JVM monitoring and diagnostic MCP (Multi-Agent Communication Protocol) server implementation based on native JDK tools. Provides AI agents with powerful capabilities to monitor and analyze Java applications without requiring third-party tools like Arthas.\n\n## Features\n\n- **Zero Dependencies**: Uses only native JDK tools (jps, jstack, jmap, etc.)\n- **Lightweight**: Minimal resource consumption compared to agent-based solutions\n- **High Compatibility**: Works with all Java versions and platforms\n- **Non-Intrusive**: No modifications to target applications required\n- **Secure**: Uses only JDK certified tools and commands\n- **Remote Monitoring**: Support for both local and remote JVM monitoring via SSH\n\n## Core Capabilities\n\n### Basic Monitoring\n- Java process listing and identification\n- JVM basic information retrieval\n- Memory usage monitoring\n- Thread information and stack trace analysis\n- Class loading statistics\n- Detailed class structure information\n\n### Advanced Features\n- Method call path analysis\n- Class decompilation\n- Method search and inspection\n- Method invocation monitoring\n- Logger level management\n- System resource dashboard\n\n## System Requirements\n\n- Python 3.6+\n- JDK 8+\n- Linux/Unix/Windows OS\n- SSH access (for remote monitoring)\n\n## Installation\n\n### Using uv (Recommended)\n\n```bash\n# Install uv if not already installed\ncurl -LsSf https://astral.sh/uv/install.sh | sh  # Linux/macOS\n# or\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"  # Windows\n\n# Install the package\nuv pip install jvm-mcp-server\n```\n\n### Using pip\n\n```bash\npip install jvm-mcp-server\n```\n\n### From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/your-repo/jvm-mcp-server.git\ncd jvm-mcp-server\n\n# Using uv (recommended)\nuv venv  # Create virtual environment\nuv sync  # Install dependencies\n\n# Or install in development mode\nuv pip install -e .\n```\n\n## Quick Start\n\n### Starting the Server\n\n#### Using uv (Recommended)\n\n```bash\n# Local mode\nuv run jvm-mcp-server\n\n# Using environment variables file for remote mode\nuv run --env-file .env jvm-mcp-server\n\n# In specific directory\nuv --directory /path/to/project run --env-file .env jvm-mcp-server\n```\n\n#### Using uvx\n\n```bash\n# Local mode\nuvx run jvm-mcp-server\n\n# With environment variables\nuvx run --env-file .env jvm-mcp-server\n```\n\n#### Using Python directly\n\n```python\nfrom jvm_mcp_server import JvmMcpServer\n\n# Local mode\nserver = JvmMcpServer()\nserver.run()\n\n# Remote mode (via environment variables)\n# Set SSH_HOST, SSH_PORT, SSH_USER, SSH_PASSWORD or SSH_KEY\nimport os\nos.environ['SSH_HOST'] = 'user@remote-host'\nos.environ['SSH_PORT'] = '22'\nserver = JvmMcpServer()\nserver.run()\n```\n\n### Using with MCP Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"jvm-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/jvm-mcp-server\",\n        \"run\",\n        \"--env-file\",\n        \"/path/to/jvm-mcp-server/.env\",\n        \"jvm-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\nJVM-MCP-Server provides a comprehensive set of tools for JVM monitoring and diagnostics:\n\n- `list_java_processes`: List all Java processes\n- `get_thread_info`: Get thread information for a specific process\n- `get_jvm_info`: Get JVM basic information\n- `get_memory_info`: Get memory usage information\n- `get_stack_trace`: Get thread stack trace information\n- `get_class_info`: Get detailed class information including structure\n- `get_stack_trace_by_method`: Get method call path\n- `decompile_class`: Decompile class source code\n- `search_method`: Search for methods in classes\n- `watch_method`: Monitor method invocations\n- `get_logger_info`: Get logger information\n- `set_logger_level`: Set logger levels\n- `get_dashboard`: Get system resource dashboard\n- `get_jcmd_output`: Execute JDK jcmd commands\n- `get_jstat_output`: Execute JDK jstat commands\n\nFor detailed documentation on each tool, see [Available Tools](./doc/available_tools.md).\n\n## Architecture\n\nJVM-MCP-Server is built on a modular architecture:\n\n1. **Command Layer**: Wraps JDK native commands\n2. **Executor Layer**: Handles local and remote command execution\n3. **Formatter Layer**: Processes and formats command output\n4. **MCP Interface**: Exposes functionality through FastMCP protocol\n\n### Key Components\n\n- `BaseCommand`: Abstract base class for all commands\n- `CommandExecutor`: Interface for command execution (local and remote)\n- `OutputFormatter`: Interface for formatting command output\n- `JvmMcpServer`: Main server class that registers all tools\n\n## Development Status\n\nThe project is in active development. See [Native_TODO.md](Native_TODO.md) for current progress.\n\n### Completed\n- Core architecture and command framework\n- Basic commands implementation (jps, jstack, jmap, jinfo, jcmd, jstat)\n- Class information retrieval system\n- MCP tool parameter type compatibility fixes\n\n### In Progress\n- Caching mechanism\n- Method tracing\n- Performance monitoring\n- Error handling improvements\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgements\n\n- JDK tools documentation\n- FastMCP protocol specification\n- Contributors and testers ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jvm",
        "monitoring",
        "java",
        "java processes",
        "jvm performance",
        "analyze java"
      ],
      "category": "monitoring-and-logging"
    },
    "yamato-snow--2025_McpLab_FastMCP": {
      "owner": "yamato-snow",
      "name": "2025_McpLab_FastMCP",
      "url": "https://github.com/yamato-snow/2025_McpLab_FastMCP",
      "imageUrl": "/freedevtools/mcp/pfp/yamato-snow.webp",
      "description": "Built using TypeScript, this server facilitates client session management for MCP by defining tools, resources, and prompts, while supporting authentication, logging, and real-time updates through SSE. It provides features like error handling, CORS, and CLI tools for testing and debugging.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-12T01:24:10Z",
      "readme_content": "# FastMCP\n\nFastMCPは、クライアントセッション管理が可能な[MCP](https://glama.ai/mcp)サーバーを構築するためのTypeScriptフレームワークです。\n\n> [!NOTE]\n>\n> Python実装版は[FastMCP Python](https://github.com/jlowin/fastmcp)をご覧ください。\n\n## 主な機能\n\nFastMCPは以下の機能を提供します：\n\n- シンプルなツール、リソース、プロンプト定義\n- [認証機能](#認証)\n- [セッション管理](#セッション)\n- [画像コンテンツ対応](#画像の返却)\n- [ロギング](#ロギング)\n- [エラーハンドリング](#エラー)\n- [SSE(Server-Sent Events)](#sse)\n- CORS（デフォルトで有効）\n- [進捗通知](#進捗通知)\n- [型付きサーバーイベント](#型付きサーバーイベント)\n- [プロンプト引数の自動補完](#プロンプト引数の自動補完)\n- [サンプリングリクエスト](#サンプリングリクエスト)\n- 自動SSEピング\n- ルート管理\n- [テスト](#mcp-cliでテスト)や[デバッグ](#mcp-inspectorで検査)のためのCLI\n\n## インストール方法\n\n```bash\nnpm install fastmcp\n```\n\n## クイックスタート\n\n> [!NOTE]\n>\n> FastMCPの実際の使用例は多数あります。[事例紹介](#事例紹介)をご覧ください。\n\n```ts\nimport { FastMCP } from \"fastmcp\";\nimport { z } from \"zod\"; // または他の検証ライブラリ（Standard Schemaをサポートしているもの）\n\nconst server = new FastMCP({\n  name: \"マイサーバー\",\n  version: \"1.0.0\",\n});\n\nserver.addTool({\n  name: \"add\",\n  description: \"2つの数値を足し算します\",\n  parameters: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n  execute: async (args) => {\n    return String(args.a + args.b);\n  },\n});\n\nserver.start({\n  transportType: \"stdio\",\n});\n```\n\nこれだけで動作するMCPサーバーができました！\n\nターミナルで以下のようにテストできます：\n\n```bash\ngit clone https://github.com/punkpeye/fastmcp.git\ncd fastmcp\n\npnpm install\npnpm build\n\n# CLIを使った足し算サーバーの例をテスト：\nnpx fastmcp dev src/examples/addition.ts\n# MCP Inspectorを使った足し算サーバーの例を検査：\nnpx fastmcp inspect src/examples/addition.ts\n```\n\n### SSE\n\n[Server-Sent Events](https://developer.mozilla.org/ja/docs/Web/API/Server-sent_events)（SSE）は、サーバーがHTTPS接続を介してクライアントにリアルタイム更新を送信するメカニズムです。MCPにおいて、SSEは主にリモートMCP通信を可能にするために使用され、リモートマシンでホストされたMCPにアクセスしてネットワーク経由で更新を中継できるようにします。\n\nSSEサポート付きでサーバーを実行することもできます：\n\n```ts\nserver.start({\n  transportType: \"sse\",\n  sse: {\n    endpoint: \"/sse\",\n    port: 8080,\n  },\n});\n```\n\nこれにより、サーバーが起動し、`http://localhost:8080/sse`でSSE接続をリッスンします。\n\nその後、`SSEClientTransport`を使用してサーバーに接続できます：\n\n```ts\nimport { SSEClientTransport } from \"@modelcontextprotocol/sdk/client/sse.js\";\n\nconst client = new Client(\n  {\n    name: \"example-client\",\n    version: \"1.0.0\",\n  },\n  {\n    capabilities: {},\n  },\n);\n\nconst transport = new SSEClientTransport(new URL(`http://localhost:8080/sse`));\n\nawait client.connect(transport);\n```\n\n## 基本概念\n\n### ツール\n\nMCPの[ツール](https://modelcontextprotocol.io/docs/concepts/tools)では、サーバーが実行可能な関数を公開し、クライアントやLLMがアクションを実行するために呼び出すことができます。\n\nFastMCPはツールパラメーターの定義に[Standard Schema](https://standardschema.dev)仕様を使用しています。これにより、Zod、ArkType、Valibotなど、仕様を実装している好みのスキーマ検証ライブラリを使用できます。\n\n**Zodの例：**\n\n```typescript\nimport { z } from \"zod\";\n\nserver.addTool({\n  name: \"fetch-zod\",\n  description: \"URLのコンテンツを取得します（Zodを使用）\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args) => {\n    return await fetchWebpageContent(args.url);\n  },\n});\n```\n\n**ArkTypeの例：**\n\n```typescript\nimport { type } from \"arktype\";\n\nserver.addTool({\n  name: \"fetch-arktype\",\n  description: \"URLのコンテンツを取得します（ArkTypeを使用）\",\n  parameters: type({\n    url: \"string\",\n  }),\n  execute: async (args) => {\n    return await fetchWebpageContent(args.url);\n  },\n});\n```\n\n**Valibotの例：**\n\nValibotにはピア依存関係@valibot/to-json-schemaが必要です。\n\n```typescript\nimport * as v from \"valibot\";\n\nserver.addTool({\n  name: \"fetch-valibot\",\n  description: \"URLのコンテンツを取得します（Valibotを使用）\",\n  parameters: v.object({\n    url: v.string(),\n  }),\n  execute: async (args) => {\n    return await fetchWebpageContent(args.url);\n  },\n});\n```\n\n#### 文字列を返す\n\n`execute`は文字列を返すことができます：\n\n```js\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args) => {\n    return \"こんにちは、世界！\";\n  },\n});\n```\n\nこれは以下と同等です：\n\n```js\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args) => {\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: \"こんにちは、世界！\",\n        },\n      ],\n    };\n  },\n});\n```\n\n#### リストを返す\n\nメッセージのリストを返したい場合は、`content`プロパティを持つオブジェクトを返せます：\n\n```js\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args) => {\n    return {\n      content: [\n        { type: \"text\", text: \"1つ目のメッセージ\" },\n        { type: \"text\", text: \"2つ目のメッセージ\" },\n      ],\n    };\n  },\n});\n```\n\n#### 画像の返却\n\n画像のコンテンツオブジェクトを作成するには、`imageContent`を使用します：\n\n```js\nimport { imageContent } from \"fastmcp\";\n\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args) => {\n    return imageContent({\n      url: \"https://example.com/image.png\",\n    });\n\n    // または...\n    // return imageContent({\n    //   path: \"/path/to/image.png\",\n    // });\n\n    // または...\n    // return imageContent({\n    //   buffer: Buffer.from(\"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\", \"base64\"),\n    // });\n\n    // または...\n    // return {\n    //   content: [\n    //     await imageContent(...)\n    //   ],\n    // };\n  },\n});\n```\n\n`imageContent`関数は以下のオプションを受け取ります：\n\n- `url`: 画像のURL\n- `path`: 画像ファイルへのパス\n- `buffer`: バッファとしての画像データ\n\n`url`、`path`、`buffer`のいずれか1つのみを指定する必要があります。\n\n上の例は以下と同等です：\n\n```js\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args) => {\n    return {\n      content: [\n        {\n          type: \"image\",\n          data: \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\",\n          mimeType: \"image/png\",\n        },\n      ],\n    };\n  },\n});\n```\n\n#### ロギング\n\nツールはコンテキストオブジェクトの`log`を使用してクライアントにメッセージをログ出力できます：\n\n```js\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args, { log }) => {\n    log.info(\"ファイルをダウンロード中...\", {\n      url: args.url,\n    });\n\n    // ...\n\n    log.info(\"ファイルをダウンロードしました\");\n\n    return \"完了\";\n  },\n});\n```\n\n`log`オブジェクトには以下のメソッドがあります：\n\n- `debug(message: string, data?: SerializableValue)`\n- `error(message: string, data?: SerializableValue)`\n- `info(message: string, data?: SerializableValue)`\n- `warn(message: string, data?: SerializableValue)`\n\n#### エラー\n\nユーザーに表示されるべきエラーは、`UserError`インスタンスとしてスローする必要があります：\n\n```js\nimport { UserError } from \"fastmcp\";\n\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args) => {\n    if (args.url.startsWith(\"https://example.com\")) {\n      throw new UserError(\"このURLは許可されていません\");\n    }\n\n    return \"完了\";\n  },\n});\n```\n\n#### 進捗通知\n\nツールはコンテキストオブジェクトの`reportProgress`を呼び出すことで進捗を報告できます：\n\n```js\nserver.addTool({\n  name: \"download\",\n  description: \"ファイルをダウンロードします\",\n  parameters: z.object({\n    url: z.string(),\n  }),\n  execute: async (args, { reportProgress }) => {\n    reportProgress({\n      progress: 0,\n      total: 100,\n    });\n\n    // ...\n\n    reportProgress({\n      progress: 100,\n      total: 100,\n    });\n\n    return \"完了\";\n  },\n});\n```\n\n### リソース\n\n[リソース](https://modelcontextprotocol.io/docs/concepts/resources)は、MCPサーバーがクライアントに提供したいあらゆる種類のデータを表します。これには以下が含まれます：\n\n- ファイルの内容\n- スクリーンショットや画像\n- ログファイル\n- その他多数\n\n各リソースは一意のURIで識別され、テキストまたはバイナリデータを含むことができます。\n\n```ts\nserver.addResource({\n  uri: \"file:///logs/app.log\",\n  name: \"アプリケーションログ\",\n  mimeType: \"text/plain\",\n  async load() {\n    return {\n      text: await readLogFile(),\n    };\n  },\n});\n```\n\n> [!NOTE]\n>\n> `load`は複数のリソースを返すことができます。これは例えば、ディレクトリが読み込まれたときにディレクトリ内のファイルのリストを返すために使用できます。\n>\n> ```ts\n> async load() {\n>   return [\n>     {\n>       text: \"1つ目のファイルの内容\",\n>     },\n>     {\n>       text: \"2つ目のファイルの内容\",\n>     },\n>   ];\n> }\n> ```\n\n`load`でバイナリコンテンツを返すこともできます：\n\n```ts\nasync load() {\n  return {\n    blob: 'base64でエンコードされたデータ'\n  };\n}\n```\n\n### リソーステンプレート\n\nリソーステンプレートを定義することもできます：\n\n```ts\nserver.addResourceTemplate({\n  uriTemplate: \"file:///logs/{name}.log\",\n  name: \"アプリケーションログ\",\n  mimeType: \"text/plain\",\n  arguments: [\n    {\n      name: \"name\",\n      description: \"ログの名前\",\n      required: true,\n    },\n  ],\n  async load({ name }) {\n    return {\n      text: `${name}のサンプルログ内容`,\n    };\n  },\n});\n```\n\n#### リソーステンプレート引数の自動補完\n\nリソーステンプレート引数の自動補完を有効にするために、`complete`関数を提供します：\n\n```ts\nserver.addResourceTemplate({\n  uriTemplate: \"file:///logs/{name}.log\",\n  name: \"アプリケーションログ\",\n  mimeType: \"text/plain\",\n  arguments: [\n    {\n      name: \"name\",\n      description: \"ログの名前\",\n      required: true,\n      complete: async (value) => {\n        if (value === \"サンプル\") {\n          return {\n            values: [\"サンプルログ\"],\n          };\n        }\n\n        return {\n          values: [],\n        };\n      },\n    },\n  ],\n  async load({ name }) {\n    return {\n      text: `${name}のサンプルログ内容`,\n    };\n  },\n});\n```\n\n### プロンプト\n\n[プロンプト](https://modelcontextprotocol.io/docs/concepts/prompts)は、サーバーが再利用可能なプロンプトテンプレートとワークフローを定義し、クライアントがユーザーやLLMに簡単に提示できるようにします。これにより、一般的なLLMインタラクションを標準化して共有するための強力な方法を提供します。\n\n```ts\nserver.addPrompt({\n  name: \"git-commit\",\n  description: \"Gitコミットメッセージを生成します\",\n  arguments: [\n    {\n      name: \"changes\",\n      description: \"Gitの差分または変更の説明\",\n      required: true,\n    },\n  ],\n  load: async (args) => {\n    return `これらの変更に対する簡潔かつ説明的なコミットメッセージを生成してください：\\n\\n${args.changes}`;\n  },\n});\n```\n\n#### プロンプト引数の自動補完\n\nプロンプトは引数の自動補完を提供できます：\n\n```js\nserver.addPrompt({\n  name: \"countryPoem\",\n  description: \"国についての詩を書きます\",\n  load: async ({ name }) => {\n    return `こんにちは、${name}さん！`;\n  },\n  arguments: [\n    {\n      name: \"name\",\n      description: \"国の名前\",\n      required: true,\n      complete: async (value) => {\n        if (value === \"日\") {\n          return {\n            values: [\"日本\"],\n          };\n        }\n\n        return {\n          values: [],\n        };\n      },\n    },\n  ],\n});\n```\n\n#### `enum`を使用したプロンプト引数の自動補完\n\n引数に`enum`配列を提供すると、サーバーは自動的に引数の補完を提供します。\n\n```js\nserver.addPrompt({\n  name: \"countryPoem\",\n  description: \"国についての詩を書きます\",\n  load: async ({ name }) => {\n    return `こんにちは、${name}さん！`;\n  },\n  arguments: [\n    {\n      name: \"name\",\n      description: \"国の名前\",\n      required: true,\n      enum: [\"日本\", \"フランス\", \"イタリア\"],\n    },\n  ],\n});\n```\n\n### 認証\n\nFastMCPではカスタム関数を使用してクライアントを`authenticate`できます：\n\n```ts\nimport { AuthError } from \"fastmcp\";\n\nconst server = new FastMCP({\n  name: \"マイサーバー\",\n  version: \"1.0.0\",\n  authenticate: ({request}) => {\n    const apiKey = request.headers[\"x-api-key\"];\n\n    if (apiKey !== '123') {\n      throw new Response(null, {\n        status: 401,\n        statusText: \"Unauthorized\",\n      });\n    }\n\n    // ここで返すものは`context.session`オブジェクトでアクセスできます\n    return {\n      id: 1,\n    }\n  },\n});\n```\n\nこれで、ツール内で認証されたセッションデータにアクセスできます：\n\n```ts\nserver.addTool({\n  name: \"sayHello\",\n  execute: async (args, { session }) => {\n    return `こんにちは、${session.id}さん！`;\n  },\n});\n```\n\n### セッション\n\n`session`オブジェクトは`FastMCPSession`のインスタンスであり、アクティブなクライアントセッションを記述します。\n\n```ts\nserver.sessions;\n```\n\nクライアントとサーバー間の1対1通信を可能にするために、各クライアント接続に対して新しいサーバーインスタンスを割り当てます。\n\n### 型付きサーバーイベント\n\n`on`メソッドを使用してサーバーから発行されるイベントをリッスンできます：\n\n```ts\nserver.on(\"connect\", (event) => {\n  console.log(\"クライアント接続:\", event.session);\n});\n\nserver.on(\"disconnect\", (event) => {\n  console.log(\"クライアント切断:\", event.session);\n});\n```\n\n## `FastMCPSession`\n\n`FastMCPSession`はクライアントセッションを表し、クライアントとやり取りするためのメソッドを提供します。\n\n`FastMCPSession`インスタンスの取得方法については、[セッション](#セッション)の例を参照してください。\n\n### `requestSampling`\n\n`requestSampling`は[サンプリング](https://modelcontextprotocol.io/docs/concepts/sampling)リクエストを作成し、レスポンスを返します。\n\n```ts\nawait session.requestSampling({\n  messages: [\n    {\n      role: \"user\",\n      content: {\n        type: \"text\",\n        text: \"現在のディレクトリにはどのファイルがありますか？\",\n      },\n    },\n  ],\n  systemPrompt: \"あなたは役立つファイルシステムアシスタントです。\",\n  includeContext: \"thisServer\",\n  maxTokens: 100,\n});\n```\n\n### `clientCapabilities`\n\n`clientCapabilities`プロパティにはクライアント機能が含まれています。\n\n```ts\nsession.clientCapabilities;\n```\n\n### `loggingLevel`\n\n`loggingLevel`プロパティは、クライアントによって設定されたロギングレベルを記述します。\n\n```ts\nsession.loggingLevel;\n```\n\n### `roots`\n\n`roots`プロパティには、クライアントによって設定されたルートが含まれています。\n\n```ts\nsession.roots;\n```\n\n### `server`\n\n`server`プロパティには、セッションに関連付けられたMCPサーバーのインスタンスが含まれています。\n\n```ts\nsession.server;\n```\n\n### 型付きセッションイベント\n\n`on`メソッドを使用してセッションから発行されるイベントをリッスンできます：\n\n```ts\nsession.on(\"rootsChanged\", (event) => {\n  console.log(\"ルート変更:\", event.roots);\n});\n\nsession.on(\"error\", (event) => {\n  console.error(\"エラー:\", event.error);\n});\n```\n\n## サーバーの実行\n\n### MCP-CLIでテスト\n\nサーバーをテストしてデバッグする最速の方法は、`fastmcp dev`を使用することです：\n\n```bash\nnpx fastmcp dev server.js\nnpx fastmcp dev server.ts\n```\n\nこれにより、[`mcp-cli`](https://github.com/wong2/mcp-cli)を使用してターミナルでMCPサーバーをテストおよびデバッグするためのサーバーが実行されます。\n\n### MCP Inspectorで検査\n\nもう一つの方法は、公式の[`MCP Inspector`](https://modelcontextprotocol.io/docs/tools/inspector)を使用してWebUIでサーバーを検査することです：\n\n```bash\nnpx fastmcp inspect server.ts\n```\n\n## よくある質問\n\n### Claude Desktopで使用するには？\n\nガイド https://modelcontextprotocol.io/quickstart/user に従って、次の設定を追加してください：\n\n```json\n{\n  \"mcpServers\": {\n    \"my-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"tsx\",\n        \"/プロジェクトへのパス/src/index.ts\"\n      ],\n      \"env\": {\n        \"環境変数名\": \"値\"\n      }\n    }\n  }\n}\n```\n\n## 事例紹介\n\n> [!NOTE]\n>\n> FastMCPを使用したサーバーを開発した場合は、ぜひ[PR提出](https://github.com/punkpeye/fastmcp)して事例として紹介してください！\n\n- [apinetwork/piapi-mcp-server](https://github.com/apinetwork/piapi-mcp-server) - Midjourney/Flux/Kling/LumaLabs/Udio/Chrip/Trellisを使用してメディアを生成\n- [domdomegg/computer-use-mcp](https://github.com/domdomegg/computer-use-mcp) - コンピュータを制御\n- [LiterallyBlah/Dradis-MCP](https://github.com/LiterallyBlah/Dradis-MCP) – Dradisでプロジェクトと脆弱性を管理\n- [Meeting-Baas/meeting-mcp](https://github.com/Meeting-Baas/meeting-mcp) - 会議ボットの作成、議事録の検索、録画データの管理\n- [drumnation/unsplash-smart-mcp-server](https://github.com/drumnation/unsplash-smart-mcp-server) – AIエージェントがUnsplashからプロの写真をシームレスに検索、推奨、配信できるようにする\n- [ssmanji89/halopsa-workflows-mcp](https://github.com/ssmanji89/halopsa-workflows-mcp) - HaloPSAワークフローとAIアシスタントの統合\n- [aiamblichus/mcp-chat-adapter](https://github.com/aiamblichus/mcp-chat-adapter) – LLMがチャット完了を使用するためのクリーンなインターフェースを提供\n\n## 謝辞\n\n- FastMCPは[Jonathan Lowin](https://github.com/jlowin)による[Python実装](https://github.com/jlowin/fastmcp)に着想を得ています。\n- コードベースの一部は[LiteMCP](https://github.com/wong2/litemcp)から採用されました。\n- コードベースの一部は[Model Context protocolでSSEをやってみる](https://dev.classmethod.jp/articles/mcp-sse/)から採用されました。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "2025_mcplab_fastmcp",
        "logging",
        "yamato",
        "logging yamato",
        "2025_mcplab_fastmcp built",
        "typescript server"
      ],
      "category": "monitoring-and-logging"
    },
    "ynu--mcp-ynu": {
      "owner": "ynu",
      "name": "mcp-ynu",
      "url": "https://github.com/ynu/mcp-ynu",
      "imageUrl": "/freedevtools/mcp/pfp/ynu.webp",
      "description": "Dynamically load tools, resources, and prompts from specified directories to enhance application capabilities. Offers automatic module discovery and logging features for efficient server management.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-24T03:02:28Z",
      "readme_content": "# MCP-YNU - FastMCP Server\n\nA dynamic MCP server implementation using FastMCP that automatically loads tools, resources, and prompts from respective directories.\n\n## Features\n\n- Dynamic loading of modules from `tools/`, `resources/`, and `prompts/` directories\n- Automatic discovery and registration of modules\n- Simple configuration and extensibility\n- Type hints for better code clarity and static analysis\n- Comprehensive logging for monitoring server activity\n\n## Recent Updates\n\n- Added type hints throughout the codebase\n- Improved MCP instance handling\n- Added logging functionality\n- Added MIT license\n- Updated documentation with reference links\n\n## Directory Structure\n\n```\nmcp-ynu/\n├── tools/          # Directory for tool modules\n│   ├── __init__.py\n│   ├── example.py\n├── resources/      # Directory for resource modules\n│   ├── __init__.py\n│   ├── example.py\n├── prompts/        # Directory for prompt modules\n│   ├── __init__.py\n│   ├── example.py\n├── logger.py       # Logger implementation\n├── main.py         # Main implementation\n├── mcp_server.py   # MCP server implementation\n├── README.md       # Project documentation\n├── LICENSE         # MIT License\n└── pyproject.toml  # Project configuration\n```\n\n## Usage\n\n1. Create modules in the appropriate directories\n2. Import mcp via `from mcp_server import mcp` \n3. Run the server:\n\n```bash\npython main.py\n```\n\n## Example Modules\n\n### Tools Module Example (tools/example.py)\n```python\nfrom mcp_server import mcp\nimport httpx\n\n@mcp.tool()\ndef calculate_bmi(weight_kg: float, height_m: float) -> float:\n    \"\"\"Calculate BMI given weight in kg and height in meters\"\"\"\n    return weight_kg / (height_m**2)\n\n\n@mcp.tool()\nasync def fetch_weather(city: str) -> str:\n    \"\"\"Fetch current weather for a city\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{city}\")\n        return response.text\n```\n\n### Resources Module Example (resources/example.py)\n```python\nfrom mcp_server import mcp\n\n@mcp.resource(\"config://app\")\ndef get_config() -> str:\n    \"\"\"Static configuration data\"\"\"\n    return \"App configuration here\"\n\n\n@mcp.resource(\"users://{user_id}/profile\")\ndef get_user_profile(user_id: str) -> str:\n    \"\"\"Dynamic user data\"\"\"\n    return f\"Profile data for user {user_id}\"\n```\n\n### Prompts Module Example (prompts/example.py)\n```python\nfrom mcp_server import mcp\nfrom mcp.server.fastmcp.prompts import base\n\n@mcp.prompt()\ndef review_code(code: str) -> str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n\n@mcp.prompt()\ndef debug_error(error: str) -> list[base.Message]:\n    return [\n        base.UserMessage(\"I'm seeing this error:\"),\n        base.UserMessage(error),\n        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n    ]\n```\n\n## Debugging\n\n1. Update `MCP_TRANSPORT_TYPE` in `.env`, Execute `python main.py` to start the mcp server\n2. Execute `npx @modelcontextprotocol/inspector` to open the [inspect](http://localhost:5173/).\n3. Choose `SSE` Transport Type with URL `http://localhost:<mcp_server_port>/sse` or Choose `STDIO` Transport Type with Command `python` and Arguments `/path/to/main.py`\n\n![@modelcontextprotocol/inspector](inspect.png)\n\n## Requirements\n\n- Python >= 3.10\n- FastMCP\n\n## Reference Links\n\n- [MCP Python SDK Documentation](https://github.com/modelcontextprotocol/python-sdk)\n- [MCP Core Concepts](https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#core-concepts)\n- [FastMCP Implementation](https://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/server/fastmcp.py)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "logging",
        "ynu",
        "monitoring",
        "logging ynu",
        "ynu mcp",
        "mcp ynu"
      ],
      "category": "monitoring-and-logging"
    },
    "zhongmingyuan--mcp-my-mac": {
      "owner": "zhongmingyuan",
      "name": "mcp-my-mac",
      "url": "https://github.com/zhongmingyuan/mcp-my-mac",
      "imageUrl": "/freedevtools/mcp/pfp/zhongmingyuan.webp",
      "description": "Exposes Mac system information through a simple API, providing real-time data about hardware, software, and environmental setups. Designed for experimentation with AI and Deep Learning on Mac systems.",
      "stars": 1,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-15T16:28:04Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/zhongmingyuan-mcp-my-mac-badge.png)](https://mseep.ai/app/zhongmingyuan-mcp-my-mac)\n\n<a href=\"https://glama.ai/mcp/servers/@zhongmingyuan/mcp-my-mac\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@zhongmingyuan/mcp-my-mac/badge\" alt=\"mcp-my-mac MCP server\" />\n</a>\n\n# MCP My Mac\n\nA lightweight server that exposes Mac system information via a simple API, allowing AI assistants like Claude to access real-time system information about your Mac. This tool is primarily designed for Mac users who want to experiment with AI and Deep Learning on their machines.\n\n> **Status: BETA** - This project is currently in beta. We're actively looking for feedback to improve functionality and user experience. Please share your thoughts and suggestions!\n\n## Why Use It?\n\n- Provides Claude Desktop or other MCP clients with access to your Mac's hardware specifications, system configuration, and resource usage\n- Enables more targeted and accurate assistance for software optimization and troubleshooting\n- Runs as a secure local API with minimal overhead\n- Only executes safe, verified commands:\n  - `system_profiler` - to gather system information\n  - `conda` - to analyze Python environment configurations\n\n## Installation\n\n### Method 1: Using UV + Git Clone\n\n#### Prerequisites\n- Python 3.8 or higher\n- UV package manager installed\n\n#### Steps\n\n1. Clone the repository:   ```bash\n   git clone git@github.com:zhongmingyuan/mcp-my-mac.git   ```\n\n2. Configure for your AI client:\n\n   **[Claude Desktop]** Add the following to your MCP server config file:\n   ```json\n   \"mcpServers\": {\n       \"mcp-my-mac\": {\n           \"command\": \"uv\",\n           \"args\": [\n               \"--directory\",\n               \"/YOUR_PATH_TO/mcp-my-mac\",\n               \"run\",\n               \"-m\",\n               \"mcp_server_my_mac\"\n           ]\n       }\n   }\n   ```\n   > Note: Replace `/YOUR_PATH_TO` with the actual path where you cloned the repository.\n\n   **[Cursor]** Add tool by selecting \"command\" in UI:\n   ```bash\n   uv run --directory /YOUR_PATH_TO/mcp-my-mac mcp_server_my_mac\n   ```\n\n## Usage\n\nAfter installation, Claude Desktop will automatically connect to this API when running on your Mac, allowing it to access system information when needed for answering your questions or providing assistance.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mac",
        "monitoring",
        "logging",
        "mac information",
        "mac systems",
        "mcp mac"
      ],
      "category": "monitoring-and-logging"
    }
  }
}