{
  "category": "monitoring",
  "categoryDisplay": "Monitoring",
  "description": "Access and analyze application monitoring data. Enables AI models to review error reports and performance metrics.",
  "totalRepositories": 14,
  "repositories": {
    "MindscapeHQ--server-raygun": {
      "owner": "MindscapeHQ",
      "name": "server-raygun",
      "url": "https://github.com/MindscapeHQ/mcp-server-raygun",
      "imageUrl": "",
      "description": "Raygun API V3 integration for crash reporting and real user monitoring",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "raygun",
        "mindscapehq",
        "raygun api",
        "server raygun",
        "application monitoring"
      ],
      "category": "monitoring"
    },
    "VictoriaMetrics-Community--mcp-victoriametrics": {
      "owner": "VictoriaMetrics-Community",
      "name": "mcp-victoriametrics",
      "url": "https://github.com/VictoriaMetrics-Community/mcp-victoriametrics",
      "imageUrl": "",
      "description": "Provides comprehensive integration with your [VictoriaMetrics instance APIs](https://docs.victoriametrics.com/victoriametrics/url-examples/) and [documentation](https://docs.victoriametrics.com/) for monitoring, observability, and debugging tasks related to your VictoriaMetrics instances",
      "stars": 73,
      "forks": 9,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-01T00:22:15Z",
      "readme_content": "# VictoriaMetrics MCP Server\n\n[![Latest Release](https://img.shields.io/github/v/release/VictoriaMetrics-Community/mcp-victoriametrics?sort=semver&label=&filter=!*-victorialogs&logo=github&labelColor=gray&color=gray&link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics-Community%2Fmcp-victoriametrics%2Freleases%2Flatest)](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/VictoriaMetrics-Community/mcp-victoriametrics)](https://archestra.ai/mcp-catalog/victoriametrics-community__mcp-victoriametrics)\n[![smithery badge](https://smithery.ai/badge/@VictoriaMetrics-Community/mcp-victoriametrics)](https://smithery.ai/server/@VictoriaMetrics-Community/mcp-victoriametrics)\n![License](https://img.shields.io/github/license/VictoriaMetrics-Community/mcp-victoriametrics?labelColor=green&label=&link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics-Community%2Fmcp-victoriametrics%2Fblob%2Fmain%2FLICENSE)\n![Slack](https://img.shields.io/badge/Join-4A154B?logo=slack&link=https%3A%2F%2Fslack.victoriametrics.com)\n![X](https://img.shields.io/twitter/follow/VictoriaMetrics?style=flat&label=Follow&color=black&logo=x&labelColor=black&link=https%3A%2F%2Fx.com%2FVictoriaMetrics)\n![Reddit](https://img.shields.io/reddit/subreddit-subscribers/VictoriaMetrics?style=flat&label=Join&labelColor=red&logoColor=white&logo=reddit&link=https%3A%2F%2Fwww.reddit.com%2Fr%2FVictoriaMetrics)\n\nThe implementation of [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server for [VictoriaMetrics](https://docs.victoriametrics.com/victoriametrics/).\n\nThis provides access to your VictoriaMetrics instance and seamless integration with [VictoriaMetrics APIs](https://docs.victoriametrics.com/victoriametrics/url-examples/) and [documentation](https://docs.victoriametrics.com/).\nIt can give you a comprehensive interface for monitoring, observability, and debugging tasks related to your VictoriaMetrics instances, enable advanced automation and interaction capabilities for engineers and tools.\n\n## Features\n\nThis MCP server allows you to use almost all read-only APIs of VictoriaMetrics, i.e. all functions available in [VMUI](https://docs.victoriametrics.com/#vmui):\n\n- Querying metrics and exploring data (even drawing graphs if your client supports it)\n- Listing and exporting available metrics, labels, labels values and entire series\n- Analyzing and testing your alerting and recording rules and alerts\n- Showing parameters of your VictoriaMetrics instance\n- Exploring cardinality of your data and metrics usage statistics\n- Analyzing, tracing, prettifying and explaining your queries\n- Debugging your relabeling rules, downsampling and retention policy configurations \n- Integration with [VictoriaMetrics Cloud](https://docs.victoriametrics.com/victoriametrics-cloud/)\n \nIn addition, the MCP server contains embedded up-to-date documentation and is able to search it without online access.\n\nMore details about the exact available tools and prompts can be found in the [Usage](#usage) section.\n\nYou can combine functionality of tools, docs search in your prompts and invent great usage scenarios for your VictoriaMetrics instance.\nJust check the [Dialog example](#dialog-example) section to see how it can work.\nAnd please note the fact that the quality of the MCP Server and its responses depends very much on the capabilities of your client and the quality of the model you are using.\n\nYou can also combine the MCP server with other observability or doc search related MCP Servers and get even more powerful results.\n\n## Try without installation\n\nThere is a publicly available instance of the VictoriaMetrics MCP Server that you can use to test the features without installing it: \n\n```\nhttps://play-mcp.victoriametrics.com/mcp\n```\n\n**Attention!** This URL is not supposed to be opened in a browser, it is intended to be used in MCP clients.\n\nIt's available in [Streamable HTTP](#modes) mode and configured to work with [Public VictoriaMetrics Playground](https://play.victoriametrics.com).\n\nHere is example of configuration for [Claude Desktop](https://claude.ai/download):\n\n![image](https://github.com/user-attachments/assets/b9cca3ff-f4c3-47bc-aac7-9359f12e858e)\n\n## Requirements\n\n- [VictoriaMetrics](https://docs.victoriametrics.com/victoriametrics/) or [VictoriaMetrics Cloud](https://docs.victoriametrics.com/victoriametrics-cloud/) instance ([single-node](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/) or [cluster](https://docs.victoriametrics.com/victoriametrics/cluster-victoriametrics/))\n- Go 1.24 or higher (if you want to build from source)\n\n## Installation\n\n### Go\n\n```bash\ngo install github.com/VictoriaMetrics-Community/mcp-victoriametrics/cmd/mcp-victoriametrics@latest\n```\n\n### Binaries\n\nJust download the latest release from [Releases](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases) page and put it to your PATH.\n\nExample for Linux x86_64 (note that other architectures and platforms are also available):\n\n```bash\nlatest=$(curl -s https://api.github.com/repos/VictoriaMetrics-Community/mcp-victoriametrics/releases/latest | grep 'tag_name' | cut -d\\\" -f4)\nwget https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/download/$latest/mcp-victoriametrics_Linux_x86_64.tar.gz\ntar axvf mcp-victoriametrics_Linux_x86_64.tar.gz\n```\n\n### Docker\n\nYou can run VictoriaMetrics MCP Server using Docker. \n\nThis is the easiest way to get started without needing to install Go or build from source.\n\n```bash\ndocker run -d --name mcp-victoriametrics \\\n  -e MCP_SERVER_MODE=sse \\\n  -e VM_INSTANCE_ENTRYPOINT=https://play.victoriametrics.com \\\n  -e VM_INSTANCE_TYPE=cluster \\\n  ghcr.io/victoriametrics-community/mcp-victoriametrics\n```\n\nYou should replace environment variables with your own parameters.\n\nNote that the `MCP_SERVER_MODE=sse` flag is used to enable Server-Sent Events mode, which used by MCP clients to connect.\nAlternatively, you can use `MCP_SERVER_MODE=http` to enable Streamable HTTP mode. More details about server modes can be found in the [Configuration](#configuration) section.\n\nSee available docker images in [github registry](https://github.com/orgs/VictoriaMetrics-Community/packages/container/package/mcp-victoriametrics).\n\nAlso see [Using Docker instead of binary](#using-docker-instead-of-binary) section for more details about using Docker with MCP server with clients in stdio mode.\n\n### Source Code\n\nFor building binary from source code you can use the following approach:\n\n- Clone repo:\n  \n  ```bash\n  git clone https://github.com/VictoriaMetrics-Community/mcp-victoriametrics.git\n  cd mcp-victoriametrics\n  ```\n- Build binary from cloned source code: \n  \n  ```bash\n  make build\n  # after that you can find binary mcp-victoriametrics and copy this file to your PATH or run inplace\n  ```\n- Build image from cloned source code:\n  \n  ```bash\n  docker build -t mcp-victoriametrics .\n  # after that you can use docker image mcp-victoriametrics for running or pushing\n  ```\n\n### Smithery\n\nTo install VictoriaMetrics MCP Server for your client automatically via Smithery, yo can use the following commands:\n\n```bash\n# Get the list of supported MCP clients\nnpx -y @smithery/cli list clients\n#Available clients:\n#  claude\n#  cline\n#  windsurf\n#  roocode\n#  witsy\n#  enconvo\n#  cursor\n#  vscode\n#  vscode-insiders\n#  boltai\n#  amazon-bedrock\n\n# Install VictoriaMetrics MCP server for your client\nnpx -y @smithery/cli install @VictoriaMetrics-Community/mcp-victoriametrics --client <YOUR-CLIENT-NAME>\n# and follow the instructions\n```\n\n## Configuration\n\nMCP Server for VictoriaMetrics is configured via environment variables:\n\n| Variable                                 | Description                                                                                                                                                                                                                                                            | Required                               | Default          | Allowed values         |\n|------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------|------------------|------------------------|\n| `VM_INSTANCE_ENTRYPOINT` / `VMC_API_KEY` | URL to VictoriaMetrics instance (it should be root `/` URL of vmsingle or vmselect)                                                                                                                                                                                    | Yes (if you don't use `VMC_API_KEY`)   | -                | -                      |\n| `VM_INSTANCE_TYPE`                       | Type of VictoriaMetrics instance                                                                                                                                                                                                                                       | Yes (if you don't use ``VMC_API_KEY``) | -                | `single`, `cluster`    |\n| `VM_INSTANCE_BEARER_TOKEN`               | Authentication token for VictoriaMetrics API                                                                                                                                                                                                                           | No                                     | -                | -                      |\n| `VM_INSTANCE_HEADERS`      | Custom HTTP headers to send with requests (comma-separated key=value pairs) | No       | -                | -                      |\n| `VMC_API_KEY`                            | [API key from VictoriaMetrics Cloud Console](https://docs.victoriametrics.com/victoriametrics-cloud/api/)                                                                                                                                                              | No                                     | -                | -                      |\n| `MCP_SERVER_MODE`                        | Server operation mode. See [Modes](#modes) for details.                                                                                                                                                                                                                | No                                     | `stdio`          | `stdio`, `sse`, `http` |\n| `MCP_LISTEN_ADDR`                        | Address for SSE or HTTP server to listen on                                                                                                                                                                                                                            | No                                     | `localhost:8080` | -                      |\n| `MCP_DISABLED_TOOLS`                     | Comma-separated list of tools to disable                                                                                                                                                                                                                               | No                                     | -                | -                      |\n| `MCP_DISABLE_RESOURCES`                  | Disable all resources (documentation tool will continue to work)                                                                                                                                                                                                       | No                                     | `false`          | `false`, `true`        |                   \n| `MCP_HEARTBEAT_INTERVAL`                 | Defines the heartbeat interval for the streamable-http protocol. <br /> It means the MCP server will send a heartbeat to the client through the GET connection, <br /> to keep the connection alive from being closed by the network infrastructure (e.g. gateways)    | No                                     | `30s`            | -                      |\n\nYou can use two options to connect to your VictoriaMetrics instance:\n\n- Using `VM_INSTANCE_ENTRYPOINT` + `VM_INSTANCE_TYPE` + `VM_INSTANCE_BEARER_TOKEN` (optional) environment variables to connect to any single-node or cluster instance of VictoriaMetrics.\n- Using `VMC_API_KEY` environment variable to work with your [VictoriaMetrics Cloud](https://victoriametrics.com/products/cloud/) instances.\n\n### Modes\n\nMCP Server supports the following modes of operation (transports):\n\n- `stdio` - Standard input/output mode, where the server reads commands from standard input and writes responses to standard output. This is the default mode and is suitable for local servers.\n- `sse` - Server-Sent Events. Server will expose the `/sse` and `/message` endpoints for SSE connections.\n- `http` - Streamable HTTP. Server will expose the `/mcp` endpoint for HTTP connections.\n\nMore info about traqnsports you can find in MCP docs:\n\n- [Core concepts -> Transports](https://modelcontextprotocol.io/docs/concepts/transports)\n- [Specifications -> Transports](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports)\n\n### –°onfiguration examples\n\n```bash\n# For a single-node instance\nexport VM_INSTANCE_ENTRYPOINT=\"http://localhost:8428\"\nexport VM_INSTANCE_TYPE=\"single\"\nexport VM_INSTANCE_BEARER_TOKEN=\"your-token\"\n\n# For a cluster\nexport VM_INSTANCE_ENTRYPOINT=\"https://play.victoriametrics.com\"\nexport VM_INSTANCE_TYPE=\"cluster\"\nexport MCP_DISABLED_TOOLS=\"export,metric_statistics,test_rules\" # disable export, statistics and rules unit test tools\n\n# For VictoriaMetrics Cloud\nexport VMC_API_KEY=\"<you-api-key>\"\n\n# Server mode\nexport MCP_SERVER_MODE=\"sse\"\nexport MCP_LISTEN_ADDR=\"0.0.0.0:8080\"\n\n# Custom headers for authentication (e.g., behind a reverse proxy)\n# Expected syntax is key=value separated by commas\nexport VM_INSTANCE_HEADERS=\"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n```\n\n## Endpoints\n\nIn SSE and HTTP modes the MCP server provides the following endpoints:\n\n| Endpoint             | Description                                                                                       |\n|----------------------|---------------------------------------------------------------------------------------------------|\n| `/sse` + `/message`  | Endpoints for messages in SSE mode (for MCP clients that support SSE)                             |\n| `/mcp`               | HTTP endpoint for streaming messages in HTTP mode (for MCP clients that support Streamable HTTP)  |\n| `/metrics`           | Metrics in Prometheus format for monitoring the MCP server                                        |\n| `/health/liveness`   | Liveness check endpoint to ensure the server is running                                           |\n| `/health/readiness`  | Readiness check endpoint to ensure the server is ready to accept requests                         |\n\n## Setup in clients\n\n### Cursor\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server` and paste the following configuration into your Cursor `~/.cursor/mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"victoriametrics\": {\n      \"command\": \"/path/to/mcp-victoriametrics\",\n      \"env\": {\n        \"VM_INSTANCE_ENTRYPOINT\": \"<YOUR_VM_INSTANCE>\",\n        \"VM_INSTANCE_TYPE\": \"<YOUR_VM_INSTANCE_TYPE>\",\n        \"VM_INSTANCE_BEARER_TOKEN\": \"<YOUR_VM_BEARER_TOKEN>\",\n        \"VM_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n### Claude Desktop\n\nAdd this to your Claude Desktop `claude_desktop_config.json` file (you can find it if open `Settings` -> `Developer` -> `Edit config`):\n\n```json\n{\n  \"mcpServers\": {\n    \"victoriametrics\": {\n      \"command\": \"/path/to/mcp-victoriametrics\",\n      \"env\": {\n        \"VM_INSTANCE_ENTRYPOINT\": \"<YOUR_VM_INSTANCE>\",\n        \"VM_INSTANCE_TYPE\": \"<YOUR_VM_INSTANCE_TYPE>\",\n        \"VM_INSTANCE_BEARER_TOKEN\": \"<YOUR_VM_BEARER_TOKEN>\",\n        \"VM_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n### Claude Code\n\nRun the command:\n\n```sh\nclaude mcp add victoriametrics -- /path/to/mcp-victoriametrics \\\n  -e VM_INSTANCE_ENTRYPOINT=<YOUR_VM_INSTANCE> \\\n  -e VM_INSTANCE_TYPE=<YOUR_VM_INSTANCE_TYPE>\n  -e VM_INSTANCE_BEARER_TOKEN=<YOUR_VM_BEARER_TOKEN>\n  -e VM_INSTANCE_HEADERS=\"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n```\n\nSee [Claude Code MCP docs](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp) for more info.\n\n### Visual Studio Code\n\nAdd this to your VS Code MCP config file:\n\n```json\n{\n  \"servers\": {\n    \"victoriametrics\": {\n      \"type\": \"stdio\",\n      \"command\": \"/path/to/mcp-victoriametrics\",\n      \"env\": {\n        \"VM_INSTANCE_ENTRYPOINT\": \"<YOUR_VM_INSTANCE>\",\n        \"VM_INSTANCE_TYPE\": \"<YOUR_VM_INSTANCE_TYPE>\",\n        \"VM_INSTANCE_BEARER_TOKEN\": \"<YOUR_VM_BEARER_TOKEN>\",\n        \"VM_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n### Zed\n\nAdd the following to your Zed config file:\n\n```json\n  \"context_servers\": {\n    \"victoriametrics\": {\n      \"command\": {\n        \"path\": \"/path/to/mcp-victoriametrics\",\n        \"args\": [],\n        \"env\": {\n          \"VM_INSTANCE_ENTRYPOINT\": \"<YOUR_VM_INSTANCE>\",\n          \"VM_INSTANCE_TYPE\": \"<YOUR_VM_INSTANCE_TYPE>\",\n          \"VM_INSTANCE_BEARER_TOKEN\": \"<YOUR_VM_BEARER_TOKEN>\",\n          \"VM_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n        }\n      },\n      \"settings\": {}\n    }\n  }\n```\n\nSee [Zed MCP docs](https://zed.dev/docs/ai/mcp) for more info.\n\n### JetBrains IDEs\n\n- Open `Settings` -> `Tools` -> `AI Assistant` -> `Model Context Protocol (MCP)`.\n- Click `Add (+)`\n- Select `As JSON`\n- Put the following to the input field:\n\n```json\n{\n  \"mcpServers\": {\n    \"victoriametrics\": {\n      \"command\": \"/path/to/mcp-victoriametrics\",\n      \"env\": {\n        \"VM_INSTANCE_ENTRYPOINT\": \"<YOUR_VM_INSTANCE>\",\n        \"VM_INSTANCE_TYPE\": \"<YOUR_VM_INSTANCE_TYPE>\",\n        \"VM_INSTANCE_BEARER_TOKEN\": \"<YOUR_VM_BEARER_TOKEN>\",\n        \"VM_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following to your Windsurf MCP config file.\n\n```json\n{\n  \"mcpServers\": {\n    \"victoriametrics\": {\n      \"command\": \"/path/to/mcp-victoriametrics\",\n      \"env\": {\n        \"VM_INSTANCE_ENTRYPOINT\": \"<YOUR_VM_INSTANCE>\",\n        \"VM_INSTANCE_TYPE\": \"<YOUR_VM_INSTANCE_TYPE>\",\n        \"VM_INSTANCE_BEARER_TOKEN\": \"<YOUR_VM_BEARER_TOKEN>\",\n        \"VM_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\nSee [Windsurf MCP docs](https://docs.windsurf.com/windsurf/mcp) for more info.\n\n### Using Docker instead of binary\n\nYou can run VictoriaMetrics MCP server using Docker instead of local binary.\n\nYou should replace run command in configuration examples above in the following way:\n\n```\n{\n  \"mcpServers\": {\n    \"victoriametrics\": {\n      \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\", \"--rm\",\n          \"-e\", \"VM_INSTANCE_ENTRYPOINT\",\n          \"-e\", \"VM_INSTANCE_TYPE\",\n          \"-e\", \"VM_INSTANCE_BEARER_TOKEN\",\n          \"-e\", \"VM_INSTANCE_HEADERS\",\n          \"ghcr.io/victoriametrics-community/mcp-victoriametrics\",\n        ],\n      \"env\": {\n        \"VM_INSTANCE_ENTRYPOINT\": \"<YOUR_VM_INSTANCE>\",\n        \"VM_INSTANCE_TYPE\": \"<YOUR_VM_INSTANCE_TYPE>\",\n        \"VM_INSTANCE_BEARER_TOKEN\": \"<YOUR_VM_BEARER_TOKEN>\",\n        \"VM_INSTANCE_HEADERS\": \"<HEADER>=<HEADER_VALUE>,<HEADER>=<HEADER_VALUE>\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nAfter [installing](#installation) and [configuring](#setup-in-clients) the MCP server, you can start using it with your favorite MCP client.\n\nYou can start dialog with AI assistant from the phrase:\n\n```\nUse MCP VictoriaMetrics in the following answers\n```\n\nBut it's not required, you can just start asking questions and the assistant will automatically use the tools and documentation to provide you with the best answers.\nJust take a look into [Dialog example](#dialog-example) section for better understanding what you can do with it.\n\n### Toolset\n\nMCP VictoriaMetrics provides numerous tools for interacting with your VictoriaMetrics instance.\n\nHere's a list of common available tools:\n\n| Tool                         | Description                                               |\n|------------------------------|-----------------------------------------------------------|\n| `query`                      | Execute instant PromQL/MetricsQL queries                  |\n| `query_range`                | Execute range PromQL/MetricsQL queries over a time period |\n| `metrics`                    | List available metrics                                    |\n| `labels`                     | List available label names                                |\n| `label_values`               | List values for a specific label                          |\n| `series`                     | List available time series                                |\n| `export`                     | Export raw time series data to JSON or CSV                |\n| `rules`                      | View alerting and recording rules                         |\n| `alerts`                     | View current alerts (firing and pending)                  |\n| `flags`                      | View non-default flags of the VictoriaMetrics instance    |\n| `metric_statistics`          | Get metrics usage (in queries) statistics                 |\n| `active_queries`             | View currently executing queries                          |\n| `top_queries`                | View most frequent or slowest queries                     |\n| `tsdb_status`                | View TSDB cardinality statistics                          |\n| `tenants`                    | List available tenants in multi-tenant cluster setup      |\n| `documentation`              | Search in embedded VictoriaMetrics documentation          |\n| `metric_relabel_debug`       | Debug Prometheus-compatible relabeling rules              |\n| `downsampling_filters_debug` | Debug downsampling configuration                          |\n| `retention_filters_debug`    | Debug retention filters configuration                     |\n| `prettify_query`             | Prettify and format PromQL/MetricsQL queries              |\n| `explain_query`              | Parse PromQL/MetricsQL queries and explain how it works   |\n| `test_rules`                 | Unit-test alerting and recording rules using vmalert tool |\n\nHere are some additional tools that are available for [VictoriaMetrics Cloud](https://docs.victoriametrics.com/victoriametrics-cloud/) (with specifying `VMC_API_KEY` parameter) users:\n\n| Tool              | Description                                                                    |\n|-------------------|--------------------------------------------------------------------------------|\n| `deployments`     | List available deployments in VictoriaMetrics Cloud                            |\n| `cloud_providers` | List available cloud providers in VictoriaMetrics Cloud                        |\n| `regions`         | List available cloud providers regions in VictoriaMetrics Cloud                |\n| `tiers`           | List available deployment tiers in VictoriaMetrics Cloud                       |\n| `access_tokens`   | List available deployment access tokens in VictoriaMetrics Cloud               |\n| `rule_filenames`  | List available alerting and recording rule filenames in VictoriaMetrics Cloud  |\n| `rule_file`       | Content of a specific alerting or recording rule file in VictoriaMetrics Cloud |\n\n### Prompts\n\nThe server includes pre-defined prompts for common tasks.\n\nThese are just examples at the moment, the prompt library will be added to in the future:\n\n| Prompt | Description |\n|--------|-------------|\n| `unused_metrics` | Find metrics that are never queried and create relabel configs to stop ingesting them |\n| `documentation` | Search VictoriaMetrics documentation for specific topics |\n| `rarely_used_metrics_with_high_cardinality` | Identify high-cardinality metrics that are rarely queried |\n\n## Dialog example\n\nThis is just example of dialog with Assistant using MCP Server that can help to understand how it can work (but's it's real dialog with using the Victoria Metrics MCP server with Claude Desktop and configured to [public playground instance](https://play.victoriametrics.com)):\n\n- > **You**: What tenants are available in my VM?\n- > **Assistant** *In your VictoriaMetrics instance, the following tenants are available `0:0` (default tenant), `1:0`, `10:0`, `100:0`*\n- > **You**: From now on, please use only default tenant\n- > **Assistant**: *I understand. From now on, I'll use only the default tenant (0:0) for all operations with your VictoriaMetrics instance.*\n- > **You**: Which metrics do we have?\n- > **Assistant**: *There are many metrics available in your VictoriaMetrics instance. The list includes metrics from various systems and components, such as:*\n  >\n  > - *System metrics (`node_`, `machine_`, etc.),*\n  > - *Kubernetes metrics (`kube_`, `container_`, `apiserver_`, etc.),*\n  > - *VictoriaMetrics-specific metrics (`vm_`, `vmagent_`, `vmalert_`, etc.),*\n  > - *Go runtime metrics (`go_`),*\n  > - *Prometheus-related metrics (`scrape_`, `promhttp_`),*\n  > - *Various application metrics (`argocd_`, `alertmanager_`).*\n  >\n  > *This is a comprehensive list of over 2000 metrics that are available for querying. Is there a specific type of metric you're interested in exploring further?*\n- > **You**: Ok, let's work with kubernetes metrics from now on. Please show me which of these metrics never queried?\n- > **Assistant**: *Based on the results, I can see that there are two Kubernetes metrics that have never been queried (query count is 0):*\n  >\n  > - *`kube_persistentvolumeclaim_deletion_timestamp` - This metric provides information about when PersistentVolumeClaims were marked for deletion.*\n  > - *`kube_pod_init_container_status_last_terminated_reason` - This metric shows the reason why init containers in pods were last terminated.*\n  >\n  > *These metrics might be valuable for monitoring and troubleshooting purposes:*\n  >\n  > - *The PVC deletion timestamp metric could help track deletion patterns or identify stuck PVCs.*\n  > - *The init container termination reason metric could help identify common failure patterns in your initialization containers.*\n  >\n  > *Would you like me to explain more about what these metrics track or suggest some useful queries to start monitoring them?*\n- > **You**: Yes, please. And also, can you create relabeling rules to stop ingesting these metrics?\n- > **Assistant**: *I'll create a relabel configuration to exclude these never-queried metrics from collection. This will help reduce storage and processing overhead. Here's the relabel configuration you can add to your VictoriaMetrics scrape configuration:*\n  >\n  > ```yaml\n  > relabel_configs:\n  > - action: drop\n  >   source_labels: [__name__]\n  >   regex: kube_persistentvolumeclaim_deletion_timestamp|kube_pod_init_container_status_last_terminated_reason\n  > ```\n  >\n  > *This configuration will:*\n  > 1. *Look at each metric's name (the `__name__` label)*\n  > 2. *Check if it matches either of the specified metrics*\n  > 3. *Drop the metrics that match, preventing them from being collected and stored*\n  >\n  > *You can add this to your VictoriaMetrics configuration file or to the relevant scrape job configurations in your Prometheus configuration if you're using Prometheus as a scraper for VictoriaMetrics.*\n  > *If you want to test this relabel configuration before implementing it, you can use the metric relabel debug tool to see how it would affect your metrics.*\n- > **You**: Yes, please debug this relabel config on real time series to see how it would affect my metrics\n- > **Assistant**: *Perfect! The relabel config works as expected:*\n  >\n  > - *The first two test cases show that both `kube_persistentvolumeclaim_deletion_timestamp` and `kube_pod_init_container_status_last_terminated_reason` metrics are correctly dropped (resulting labels are empty `{}`).*\n  > - *The third test case confirms that other metrics like `kube_pod_status_phase` are preserved and not affected by our drop rule.*\n  >\n  > *You can confidently add this relabel configuration to your VictoriaMetrics or Prometheus configuration to stop collecting these unused metrics. This will save storage space and reduce processing overhead without affecting your current monitoring capabilities.*\n- > **You**: But i'm using vmagent resource of VictoriaMetrics operator, could you please give me part of specification for my agent with this relabel config?\n- > **Assistant**: *You're right, since you're using the VictoriaMetrics Operator and the VMAgent custom resource, the configuration needs to be in the appropriate format for that CR. Here's how you can add the relabel configuration to your VMAgent specification:*\n  >\n  > ```yaml\n  > apiVersion: operator.victoriametrics.com/v1beta1\n  > kind: VMAgent\n  > metadata:\n  > name: your-vmagent-name\n  > spec:\n  >   # ... your existing configuration ...\n  >   \n  >   # Add this section to configure global metric relabeling\n  >   inlineRelabelConfig:\n  >     - action: drop\n  >       sourceLabels: [\"__name__\"]\n  >       regex: kube_persistentvolumeclaim_deletion_timestamp|kube_pod_init_container_status_last_terminated_reason\n  >   \n  >   # ... rest of your configuration ...\n  > ```\n  >\n  > *Key points:*\n  >\n  > - *In the VMAgent spec, you use `inlineRelabelConfig` for inline relabeling rules*\n  > - *The field is called `sourceLabels` (not `source_labels` as in Prometheus config)*\n  > - *This is a global relabeling that will be applied to all metrics before they're stored*\n- > **You**: I decided not to remove those metrics, but just lower the sample rate for them using Downsampling. What downsampling parameters i have now?\n- > **Assistant**: *I can see that you currently have the following downsampling configuration:*\n  >\n  > ```\n  > -downsampling.period=\"7d:1m,30d:5m,180d:30m\"\n  > ```\n  >\n  > *This means:*\n  >\n  > - *Data newer than 7 days is kept at its original resolution*\n  > - *Data between 7-30 days old is downsampled to 1-minute resolution*\n  > - *Data between 30-180 days old is downsampled to 5-minute resolution*\n  > - *Data older than 180 days is downsampled to 30-minute resolution*\n  >\n  > *Now, if you want to add specific downsampling rules for those rarely-queried metrics, you can use the downsampling filters debug tool to test how new rules would work. Let's test a configuration where we apply more aggressive downsampling to those specific metrics:*\n  >\n  > *...getting series and using downsampling filter debugger...*\n  >\n  > *Great! Now our test is working correctly. Based on the current configuration and our testing, here's a suggested downsampling configuration for your VMAgent CR that would apply more aggressive downsampling to those rarely-queried metrics:*\n  >\n  > ```yaml\n  > apiVersion: operator.victoriametrics.com/v1beta1\n  > kind: VMCluster\n  > metadata:\n  >   name: monitoring-vm-cluster\n  > spec:\n  >   # ... your existing configuration ...\n  >   \n  >   # Update the vmstorage section with the new downsampling parameters\n  >   vmstorage:\n  >     extraArgs:\n  >       downsampling.period: '{__name__=\\\"kube_persistentvolumeclaim_deletion_timestamp\\\"}:1d:10m,{__name__=\\\"kube_pod_init_container_status_last_terminated_reason\\\"}:1d:10m,7d:1m,30d:5m,180d:30m'\n  >   \n  >   # ... rest of your configuration ...\n  > ``` \n  >\n  > *This configuration:*\n  >\n  > *- Applies 10-minute resolution downsampling for the rarely-queried metrics after just 1 day*\n  > *- Maintains your existing downsampling policy for all other metrics*\n  >\n  > *The order of the downsampling rules is important - more specific rules should be listed first.*\n\nDuring this dialog, the assistant was using the following tools:\n\n- `tenants` to get the list of available tenants\n- `documentation` to get information about functionality and data formats\n- `metrics` to get the list of available metrics\n- `metrics_staistics` to get the information about metrics usage\n- `series` to get the time series for debugging\n- `metric_relabel_debug` to debug relabeling rules\n- `flags` to get the information about instance parameters\n- `downsampling_filters_debug` to debug downsampling configuration\n\nBut you can use any other tools and combine them in your own way.\n\n## Monitoring\n\nIn [SSE and HTTP modes](#modes) the MCP Server provides metrics in Prometheus format (see [endpoints](#endpoints)) and you can find [in repo simple grafana dashboard](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/blob/main/dashboard/mcp-victoriametrics-grafana.json) for these metrics. \n\n## Roadmap\n\n- [x] Support \"Prettify query\" tool (done in [`v0.0.5`](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/tag/v0.0.5))\n- [x] Support \"Explain query\" tool (done in [`v0.0.6`](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/tag/v0.0.6))\n- [x] Support CI pipeline for building and pushing multiarch docker images (done in [`v1.0.0`](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/tag/v1.0.0))\n- [ ] Support tool for analysis of [Query execution statistics](https://docs.victoriametrics.com/victoriametrics/query-stats/)\n- [ ] Support vmanomaly\n- [x] Support tool for [unit-testing of alerting and recording rules](https://docs.victoriametrics.com/victoriametrics/vmalert-tool/) (done in [`v0.0.7`](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/tag/v0.0.7))\n- [x] Support optional integration with [VictoriaMetrics Cloud](https://victoriametrics.com/products/cloud/) (via [API keys](https://docs.victoriametrics.com/victoriametrics-cloud/api/)) (done in [`v0.0.9`](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/tag/v0.0.9))\n- [ ] Add some extra knowledge to server in addition to current documentation tool:\n  - [x] [VictoriaMetrics blog](https://victoriametrics.com/blog/) posts (done in [`v1.1.0`](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/tag/v1.1.0))\n  - [ ] Github issues\n  - [ ] Public slack chat history\n  - [ ] CRD schemas\n  - [ ] Alerting and recording rule sets\n- [ ] Implement multitenant version of MCP (that will support several deployments)\n- [ ] Add flags/configs validation tool\n- [ ] Support tools for vmagent API\n- [ ] Support [new vmalert API](https://github.com/VictoriaMetrics/VictoriaMetrics/pull/9046/files)\n- [x] Enabling/disabling tools via configuration (done in [`v0.0.8`](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/releases/tag/v0.0.8))\n- [ ] Tools for Alertmanager APIs [#6](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics/issues/6)\n- [ ] Support for [metrics metadata](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/2974) in case of implementation in VictoriaMetrics\n- [ ] Support authentication\n- [ ] Add static index page with description and links to documentation\n\n## Mentions\n\n[<img src=\"https://github.com/user-attachments/assets/76355b76-8a54-487d-a7d9-0b64886938fb\" alt=\"How to Use an AI Assistant with Your Monitoring System ‚Äì VictoriaMetrics MCP Server\" width=\"520px\" />](https://www.youtube.com/watch?v=1k7xgbRi1k0)\n\n[<img src=\"https://github.com/user-attachments/assets/2b93a545-7834-4020-aa2c-369ddcc90a29\" alt=\"MCP Server Integration & Much More: What's New in VictoriaMetrics Cloud Q2 2025\" width=\"520px\" />](https://victoriametrics.com/blog/q2-2025-whats-new-victoriametrics-cloud/)\n\n## Disclaimer\n\nAI services and agents along with MCP servers like this cannot guarantee the accuracy, completeness and reliability of results.\nYou should double check the results obtained with AI.\n\nThe quality of the MCP Server and its responses depends very much on the capabilities of your client and the quality of the model you are using.\n\n## Contributing\n\nContributions to the MCP VictoriaMetrics project are welcome! \n\nPlease feel free to submit issues, feature requests, or pull requests.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "apis",
        "victoriametrics",
        "application monitoring",
        "victoriametrics provides",
        "analyze application"
      ],
      "category": "monitoring"
    },
    "edgedelta--edgedelta-mcp-server": {
      "owner": "edgedelta",
      "name": "edgedelta-mcp-server",
      "url": "https://github.com/edgedelta/edgedelta-mcp-server",
      "imageUrl": "",
      "description": "[grafana/mcp-grafana](https://github.com/grafana/mcp-grafana) üéñÔ∏è üêç üè† ‚òÅÔ∏è - Search dashboards, investigate incidents and query datasources in your Grafana instance",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-10-03T14:11:38Z",
      "readme_content": "# Edge Delta MCP Server\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/edgedelta/edgedelta-mcp-server)](https://archestra.ai/mcp-catalog/edgedelta__edgedelta-mcp-server)\n\nThe **Edge Delta MCP Server** is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)\nserver that provides seamless integration with Edge Delta APIs, enabling advanced\nautomation and interaction capabilities for developers and tools.\n\n## Use¬†Cases\n\n- Extract and analyse observability data from Edge¬†Delta.\n- Build AI‚Äëpowered tools and applications that interact with Edge¬†Delta‚Äôs platform.\n\n## Prerequisites\n\n1. **Docker¬†Engine¬†‚â•‚ÄØ20.10** installed *and running*.\n2. **Docker¬†Buildx plug‚Äëin** available:\n   - **macOS / Windows**¬†‚Äì included with Docker¬†Desktop.\n   - **Debian¬†/ Ubuntu**\n     ```bash\n     sudo apt-get update && sudo apt-get install -y docker-buildx-plugin\n     ```\n   - **Fedora / RHEL / CentOS**\n     ```bash\n     sudo dnf install -y docker-buildx-plugin   # or yum install ‚Ä¶\n     ```\n   - **Other distros (manual fallback)**\n     ```bash\n     mkdir -p ~/.docker/cli-plugins\n     curl -sSL \\\n       https://github.com/docker/buildx/releases/latest/download/buildx-$(uname -s | tr '[:upper:]' '[:lower:]')-amd64 \\\n       -o ~/.docker/cli-plugins/docker-buildx\n     chmod +x ~/.docker/cli-plugins/docker-buildx\n     ```\n3. An **Edge¬†Delta API token** with the required scope ‚Äì¬†[create one here](https://docs.edgedelta.com/api-tokens/).\n4. Your **Edge¬†Delta organisation ID** ‚Äì¬†[find it here](https://docs.edgedelta.com/my-organization/).\n\n## Build (container image)\n\nFirst‚Äëtime setup (creates a multi‚Äëplatform builder and boots it):\n\n```bash\ndocker buildx create --name edgedelta-builder --use\ndocker buildx inspect --bootstrap\n```\n\nBuild the image and load it into the local Docker daemon:\n\n```bash\ndocker buildx build --load -t mcp/edgedelta .\n```\n\n> ‚ÑπÔ∏è  The `--load` flag streams the image back to your local Docker engine so you can\n> run it directly with `docker run mcp/edgedelta ‚Ä¶`.\n\n## Installation\n\n### Usage with Cursor\n\n```json\n{\n  \"mcpServers\": {\n    \"edgedelta\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e ED_ORG_ID\",\n        \"-e ED_API_TOKEN\",\n        \"ghcr.io/edgedelta/edgedelta-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"ED_API_TOKEN\": \"<YOUR_TOKEN>\",\n        \"ED_ORG_ID\": \"<YOUR_ORG_ID>\"\n      }\n    }\n  }\n}\n```\n\n## Library Usage\n\nThe exported Go API of this module is **experimental** and may change without notice.\nIf you rely on it in production, please open an issue describing your use case so we\ncan stabilise the relevant surface.\n\n## License\n\nLicensed under the terms of the **MIT** licence. See [LICENSE](./LICENSE) for full details.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "grafana",
        "edgedelta",
        "datasources grafana",
        "server grafana",
        "application monitoring"
      ],
      "category": "monitoring"
    },
    "getsentry--sentry-mcp": {
      "owner": "getsentry",
      "name": "sentry-mcp",
      "url": "https://github.com/getsentry/sentry-mcp",
      "imageUrl": "",
      "description": "Sentry.io integration for error tracking and performance monitoring",
      "stars": 369,
      "forks": 44,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-10-04T02:31:09Z",
      "readme_content": "# sentry-mcp\n\n[![codecov](https://codecov.io/gh/getsentry/sentry-mcp/graph/badge.svg?token=khVKvJP5Ig)](https://codecov.io/gh/getsentry/sentry-mcp)\n\nSentry's MCP service is primarily designed for human-in-the-loop coding agents. Our tool selection and priorities are focused on developer workflows and debugging use cases, rather than providing a general-purpose MCP server for all Sentry functionality.\n\nThis remote MCP server acts as middleware to the upstream Sentry API, optimized for coding assistants like Cursor, Claude Code, and similar development tools. It's based on [Cloudflare's work towards remote MCPs](https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/).\n\n## Getting Started\n\nYou'll find everything you need to know by visiting the deployed service in production:\n\n<https://mcp.sentry.dev>\n\nIf you're looking to contribute, learn how it works, or to run this for self-hosted Sentry, continue below.\n\n### Stdio vs Remote\n\nWhile this repository is focused on acting as an MCP service, we also support a `stdio` transport. This is still a work in progress, but is the easiest way to adapt run the MCP against a self-hosted Sentry install.\n\n**Note:** The AI-powered search tools (`search_events` and `search_issues`) require an OpenAI API key. These tools use natural language processing to translate queries into Sentry's query syntax. Without the API key, these specific tools will be unavailable, but all other tools will function normally.\n\nTo utilize the `stdio` transport, you'll need to create an User Auth Token in Sentry with the necessary scopes. As of writing this is:\n\n```\norg:read\nproject:read\nproject:write\nteam:read\nteam:write\nevent:write\n```\n\nLaunch the transport:\n\n```shell\nnpx @sentry/mcp-server@latest --access-token=sentry-user-token\n```\n\nNeed to connect to a self-hosted deployment? Add <code>--host</code> (hostname\nonly, e.g. <code>--host=sentry.example.com</code>) when you run the command.\n\nNote: You can also use environment variables:\n\n```shell\nSENTRY_ACCESS_TOKEN=\n# Optional overrides for self-hosted deployments\nSENTRY_HOST=\nOPENAI_API_KEY=  # Required for AI-powered search tools (search_events, search_issues)\n```\n\nIf you leave the host variable unset, the CLI automatically targets the Sentry\nSaaS service. Only set the override when you operate self-hosted Sentry.\n\n### MCP Inspector\n\nMCP includes an [Inspector](https://modelcontextprotocol.io/docs/tools/inspector), to easily test the service:\n\n```shell\npnpm inspector\n```\n\nEnter the MCP server URL (<http://localhost:5173>) and hit connect. This should trigger the authentication flow for you.\n\nNote: If you have issues with your OAuth flow when accessing the inspector on `127.0.0.1`, try using `localhost` instead by visiting `http://localhost:6274`.\n\n## Local Development\n\nTo contribute changes, you'll need to set up your local environment:\n\n1. **Set up environment files:**\n\n   ```shell\n   make setup-env  # Creates both .env files from examples\n   ```\n\n2. **Create an OAuth App in Sentry** (Settings => API => [Applications](https://sentry.io/settings/account/api/applications/)):\n\n   - Homepage URL: `http://localhost:5173`\n   - Authorized Redirect URIs: `http://localhost:5173/oauth/callback`\n   - Note your Client ID and generate a Client secret\n\n3. **Configure your credentials:**\n\n   - Edit `.env` in the root directory and add your `OPENAI_API_KEY`\n   - Edit `packages/mcp-cloudflare/.env` and add:\n     - `SENTRY_CLIENT_ID=your_development_sentry_client_id`\n     - `SENTRY_CLIENT_SECRET=your_development_sentry_client_secret`\n     - `COOKIE_SECRET=my-super-secret-cookie`\n\n4. **Start the development server:**\n\n   ```shell\n   pnpm dev\n   ```\n\n### Verify\n\nRun the server locally to make it available at `http://localhost:5173`\n\n```shell\npnpm dev\n```\n\nTo test the local server, enter `http://localhost:5173/mcp` into Inspector and hit connect. Once you follow the prompts, you'll be able to \"List Tools\".\n\n### Tests\n\nThere are two test suites included: basic unit tests, and some evaluations.\n\nUnit tests can be run using:\n\n```shell\npnpm test\n```\n\nEvals will require a `.env` file in the project root with some config:\n\n```shell\n# .env (in project root)\nOPENAI_API_KEY=  # Also required for AI-powered search tools in production\n```\n\nNote: The root `.env` file provides defaults for all packages. Individual packages can have their own `.env` files to override these defaults during development.\n\nOnce that's done you can run them using:\n\n```shell\npnpm eval\n```\n\n## Development Notes\n\n### Automated Code Review\n\nThis repository uses automated code review tools (like Cursor BugBot) to help identify potential issues in pull requests. These tools provide helpful feedback and suggestions, but **we do not recommend making these checks required** as the accuracy is still evolving and can produce false positives.\n\nThe automated reviews should be treated as:\n\n- ‚úÖ **Helpful suggestions** to consider during code review\n- ‚úÖ **Starting points** for discussion and improvement\n- ‚ùå **Not blocking requirements** for merging PRs\n- ‚ùå **Not replacements** for human code review\n\nWhen addressing automated feedback, focus on the underlying concerns rather than strictly following every suggestion.\n\n### Contributor Documentation\n\nLooking to contribute or explore the full documentation map? See `CLAUDE.md` (also available as `AGENTS.md`) for contributor workflows and the complete docs index. The `docs/` folder contains the per-topic guides and tool-integrated `.mdc` files.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "io",
        "ai",
        "application monitoring",
        "performance monitoring",
        "error tracking"
      ],
      "category": "monitoring"
    },
    "hyperb1iss--lucidity-mcp": {
      "owner": "hyperb1iss",
      "name": "lucidity-mcp",
      "url": "https://github.com/hyperb1iss/lucidity-mcp",
      "imageUrl": "",
      "description": "Enhance AI-generated code quality through intelligent, prompt-based analysis across 10 critical dimensions from complexity to security vulnerabilities",
      "stars": 67,
      "forks": 20,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-25T14:26:10Z",
      "readme_content": "# ‚ú® Lucidity MCP üîç\n\n<div align=\"center\">\n\n[![Python 3.13+](https://img.shields.io/badge/python-3.13+-9D00FF.svg?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-Apache_2.0-FF00FF.svg?style=for-the-badge)](LICENSE)\n[![Status](https://img.shields.io/badge/status-active_development-39FF14.svg?style=for-the-badge)](docs/plan.md)\n[![Code Style](https://img.shields.io/badge/code_style-ruff-00FFFF.svg?style=for-the-badge)](https://github.com/astral-sh/ruff)\n[![Type Check](https://img.shields.io/badge/type_check-mypy-FFBF00.svg?style=for-the-badge)](https://mypy.readthedocs.io/en/stable/)\n\n**Clarity in Code, Confidence in Creation**\n\n</div>\n\nLucidity is a Model Context Protocol (MCP) server designed to enhance the quality of AI-generated code through intelligent, prompt-based analysis. By providing structured guidance to AI coding assistants, Lucidity helps identify and address common quality issues, resulting in cleaner, more maintainable, and more robust code.\n\nBefore you commit, just ask Lucidity to analyze the changes instead of vibe-coding yourself into a nightmare hellscape! üò± üí• üö´\n\n## üí´ Features\n\n- üîÆ **Comprehensive Issue Detection** - Covers 10 critical quality dimensions from complexity to security vulnerabilities\n- üîÑ **Contextual Analysis** - Compares changes against original code to identify unintended modifications\n- üåê **Language Agnostic** - Works with any programming language the AI assistant understands\n- üéØ **Focused Analysis** - Option to target specific issue types based on project needs\n- üìù **Structured Outputs** - Guides AI to provide actionable feedback with clear recommendations\n- ü§ñ **MCP Integration** - Seamless integration with Claude and other MCP-compatible AI assistants\n- ü™∂ **Lightweight Implementation** - Simple server design with minimal dependencies\n- üß© **Extensible Framework** - Easy to add new issue types or refine analysis criteria\n- üîÄ **Flexible Transport** - Supports both stdio for terminal-based interaction and SSE for network-based communication\n- üîÑ **Git-Aware Analysis** - Analyzes changes directly from git diff, making it ideal for pre-commit reviews\n\n## üöÄ Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/hyperbliss/lucidity-mcp.git\ncd lucidity-mcp\n\n# Set up a virtual environment with UV\nuv venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies with UV\nuv sync\n```\n\n## üìã Prerequisites\n\n- Python 3.13 or higher\n- Git (for analyzing code changes)\n- UV package manager (recommended for dependency management)\n\n## üîÆ Quick Start\n\n### Run the Lucidity server\n\n```bash\n# Start with stdio transport (for terminal use)\nlucidity-mcp\n\n# Start with SSE transport (for network use)\nlucidity-mcp --transport sse --host 127.0.0.1 --port 6969\n\n# Run with debug logging\nlucidity-mcp --debug\n\n# Run with file logging\nlucidity-mcp --log-file lucidity.log\n```\n\n### Using with AI Assistants\n\n1. Start Lucidity in SSE mode:\n\n   ```bash\n   lucidity-mcp --transport sse\n   ```\n\n2. Connect your AI assistant using the MCP protocol URI:\n\n   ```\n   sse://localhost:6969/sse\n   ```\n\n3. The AI can now invoke the `analyze_changes` tool to get code quality feedback!\n\n## üß† Analysis Dimensions\n\nLucidity analyzes code across 10 critical quality dimensions:\n\n1. **Unnecessary Complexity** - Identifies overly complex algorithms, excessive abstractions, and convoluted logic\n2. **Poor Abstractions** - Detects leaky or inappropriate abstractions and unclear separation of concerns\n3. **Unintended Code Deletion** - Catches accidental removal of critical functionality or validation\n4. **Hallucinated Components** - Finds references to non-existent functions, classes, or APIs\n5. **Style Inconsistencies** - Spots deviations from project coding standards and conventions\n6. **Security Vulnerabilities** - Identifies potential security issues in code changes\n7. **Performance Issues** - Detects inefficient algorithms or operations that could impact performance\n8. **Code Duplication** - Finds repeated logic or functionality that should be refactored\n9. **Incomplete Error Handling** - Spots missing or inadequate exception handling\n10. **Test Coverage Gaps** - Identifies missing tests for critical functionality\n\n## üìä Example AI Assistant Queries\n\nWith an AI assistant connected to Lucidity, try these queries:\n\n- \"Analyze the code quality in my latest git changes\"\n- \"Check for security vulnerabilities in my JavaScript changes\"\n- \"Make sure my Python code follows best practices\"\n- \"Identify any performance issues in my recent code changes\"\n- \"Are there any unintended side effects in my recent refactoring?\"\n- \"Help me improve the abstractions in my code\"\n- \"Check if I've accidentally removed any important validation\"\n- \"Find any hallucinated API calls in my latest commit\"\n- \"Is my error handling complete and robust?\"\n- \"Are there any test coverage gaps in my new feature?\"\n\n## üõ†Ô∏è Available MCP Tools\n\n### Tools\n\n- `analyze_changes` - Prepares git changes for analysis through MCP\n  - Parameters:\n    - `workspace_root`: The root directory of the workspace/git repository\n    - `path`: Optional specific file path to analyze\n\n## üíª Development\n\nLucidity uses UV for dependency management and development workflows. UV is a fast, reliable Python package manager and resolver.\n\n```bash\n# Update dependencies\nuv sync\n\n# Run tests\npytest\n\n# Run linting\nruff check .\n\n# Run type checking\nmypy .\n```\n\n## üîß Logging Behavior\n\nLucidity handles logging differently depending on the transport:\n\n- **SSE transport**: Full console logging is enabled\n- **Stdio transport with --log-file**: All logs go to the file, console is disabled\n- **Stdio transport without --log-file**: Only warnings and errors go to stderr, info logs are disabled\n\nThis ensures that stdio communication isn't broken by logs appearing on stdout.\n\n## üéõÔ∏è Command-line Options\n\n```\nusage: lucidity-mcp [-h] [--debug] [--host HOST] [--port PORT] [--transport {stdio,sse}]\n                [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--verbose]\n                [--log-file LOG_FILE]\n\noptions:\n  -h, --help            show this help message and exit\n  --debug               Enable debug logging\n  --host HOST           Host to bind the server to (use 0.0.0.0 for all interfaces)\n  --port PORT           Port to listen on for network connections\n  --transport {stdio,sse}\n                        Transport type to use (stdio for terminal, sse for network)\n  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}\n                        Set the logging level\n  --verbose             Enable verbose logging for HTTP requests\n  --log-file LOG_FILE   Path to log file (required for stdio transport if logs enabled)\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Set up your development environment with UV\n4. Make your changes\n5. Run tests and linting\n6. Commit your changes (`git commit -m 'Add some amazing feature'`)\n7. Push to the branch (`git push origin feature/amazing-feature`)\n8. Open a Pull Request\n\n## üìù License\n\nThis project is licensed under the Apache License 2.0 - see the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nCreated by [Stefanie Jane üå†](https://github.com/hyperb1iss)\n\nIf you find Lucidity useful, [buy me a Monster Ultra Violet ‚ö°Ô∏è](https://ko-fi.com/hyperb1iss)\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "hyperb1iss",
        "ai",
        "vulnerabilities",
        "hyperb1iss lucidity",
        "application monitoring",
        "ai generated"
      ],
      "category": "monitoring"
    },
    "imprvhub--mcp-status-observer": {
      "owner": "imprvhub",
      "name": "mcp-status-observer",
      "url": "https://github.com/imprvhub/mcp-status-observer",
      "imageUrl": "",
      "description": "Model Context Protocol server for monitoring Operational Status of major digital platforms in Claude Desktop.",
      "stars": 5,
      "forks": 5,
      "license": "Mozilla Public License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-14T04:13:39Z",
      "readme_content": "# MCP Status Observer\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/d7d5a94b-3378-479b-b5a3-35efa8904d2e) [![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/imprvhub/mcp-status-observer)](https://archestra.ai/mcp-catalog/imprvhub__mcp-status-observer)\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-status-observer)](https://smithery.ai/server/@imprvhub/mcp-status-observer)\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"padding: 15px; vertical-align: middle; border: none; text-align: center;\">\n  <a href=\"https://mseep.ai/app/imprvhub-mcp-status-observer\">\n    <img src=\"https://mseep.net/pr/imprvhub-mcp-status-observer-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" />\n  </a>\n</td>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">An integration that allows Claude Desktop to monitor and query the operational status of major digital platforms including AI providers, cloud services, and developer tools using the Model Context Protocol (MCP).</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\">\n  <a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-status-observer\">\n    <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-status-observer/badge\" alt=\"Status Observer MCP server\" />\n  </a>\n</td>\n\n</tr>\n</table>\n\n> [!IMPORTANT]\n> This project is continuously updated with new platform integrations. If you're not seeing a service that should be available, or if Claude doesn't recognize a platform, please update by running `npm run build` from a freshly cloned repository. \n> \n> **Last updated**: 2025-09-12T07:22:15Z (UTC) - Added OpenRouter status integration with RSS incident tracking\n\n## Features\n\n- Monitor world's most used digital platforms (GitHub, Slack, Discord, etc.)\n- Track AI providers including OpenRouter, OpenAI, Anthropic, and Gemini\n- Get detailed status information for specific services with incident history\n- Check status of specific components within each platform\n- Real-time updates of service status with impact analysis\n- Comprehensive incident tracking with resolution status and timelines\n- Simple query interface with commands like `status --openrouter`\n\n## Demo\n\n<p>\n  <a href=\"https://www.youtube.com/watch?v=EV1ac0PMzKg\">\n    <img src=\"public/assets/preview.png\" width=\"600\" alt=\"Status Observer MCP Demo\">\n  </a>\n</p>\n\n<details>\n<summary> Timestamps </summary>\n\nClick on any timestamp to jump to that section of the video\n\n[**00:00**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=0s) - **LinkedIn Platform Status Assessment**  \nComprehensive analysis of LinkedIn's operational health, including detailed examination of core services such as LinkedIn.com, LinkedIn Learning, Campaign Manager, Sales Navigator, Recruiter, and Talent solutions. All systems confirmed fully operational with zero service disruptions.\n\n[**00:20**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=20s) - **GitHub Infrastructure Status Overview**  \nDetailed evaluation of GitHub's service availability, covering critical components including Git operations, API requests, Actions, Webhooks, Issues, Pull Requests, Packages, Pages, Codespaces, and Copilot functionality. Complete operational status confirmed across all GitHub services.\n\n[**00:40**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=40s) - **Vercel Platform Reliability Analysis**  \nIn-depth examination of Vercel's global edge network and deployment infrastructure, featuring comprehensive status reporting on core services such as API, Dashboard, Builds, Serverless Functions, Edge Functions, and global CDN locations. All Vercel services verified operational across all regions.\n\n[**01:08**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=68s) - **Cloudflare Network Status Examination**  \nExtensive analysis of Cloudflare's global infrastructure status, detailing service availability across geographic regions and specific service components. Identified performance degradation in multiple regions (Africa, Asia, Europe, Latin America, Middle East, North America) while core services remain functional. Includes detailed assessment of regional data centers under maintenance and technical impact analysis.\n\n[**01:46**](https://www.youtube.com/watch?v=EV1ac0PMzKg&t=106s) - **Global Operational Status Report**  \nConsolidated overview of operational status across all major technology platforms and service providers, highlighting both fully operational services (GitHub, Vercel, Netlify, Asana, Atlassian, OpenRouter, etc.) and services experiencing degraded performance (Cloudflare, Twilio). Includes strategic recommendations for organizations with dependencies on affected services.\n</details>\n\n## Requirements\n\n- Node.js 16 or higher\n- Claude Desktop\n- Internet connection to access status APIs\n\n## Installation\n\n### Installing Manually\n1. Clone or download this repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-status-observer\ncd mcp-status-observer\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Running the MCP Server\n\nThere are two ways to run the MCP server:\n\n### Option 1: Running manually\n\n1. Open a terminal or command prompt\n2. Navigate to the project directory\n3. Run the server directly:\n\n```bash\nnode build/index.js\n```\n\nKeep this terminal window open while using Claude Desktop. The server will run until you close the terminal.\n\n### Option 2: Auto-starting with Claude Desktop (recommended for regular use)\n\nThe Claude Desktop can automatically start the MCP server when needed. To set this up:\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the Status Observer MCP configuration. If the file doesn't exist, create it:\n\n```json\n{\n  \"mcpServers\": {\n    \"statusObserver\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_DIRECTORY/mcp-status-observer/build/index.js\"]\n    }\n  }\n}\n```\n\n**Important**: Replace `ABSOLUTE_PATH_TO_DIRECTORY` with the **complete absolute path** where you installed the MCP\n  - macOS/Linux example: `/Users/username/mcp-status-observer`\n  - Windows example: `C:\\\\Users\\\\username\\\\mcp-status-observer`\n\nIf you already have other MCPs configured, simply add the \"statusObserver\" section inside the \"mcpServers\" object. Here's an example of a configuration with multiple MCPs:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp1\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"otherMcp2\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"statusObserver\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"ABSOLUTE_PATH_TO_DIRECTORY/mcp-status-observer/build/index.js\"\n      ]\n    }\n  }\n}\n```\n\nThe MCP server will automatically start when Claude Desktop needs it, based on the configuration in your `claude_desktop_config.json` file.\n\n## Usage\n\n1. Restart Claude Desktop after modifying the configuration\n2. In Claude, use the `status` command to interact with the Status Observer MCP Server\n3. The MCP server runs as a subprocess managed by Claude Desktop\n\n## Available Commands\n\nThe Status Observer MCP provides a single tool named `status` with several commands:\n\n| Command | Description | Parameters | Example |\n|---------|-------------|------------|---------|\n| `list` | List all available platforms | None | `status list` |\n| `--[platform]` | Get status for a specific platform | Platform name | `status --openrouter` |\n| `--all` | Get status for all platforms | None | `status --all` |\n\n## Supported Platforms\n\nThe Status Observer monitors 22 major digital platforms across various categories:\n\n### AI & Machine Learning (4)\n- **OpenRouter** - AI model routing and access platform\n- **OpenAI** - Leading AI services provider (ChatGPT, DALL-E, API)\n- **Anthropic** - AI assistant provider (Claude)\n- **Gemini** - Google's multimodal AI platform\n\n### Cloud Infrastructure (4)\n- **Google Cloud Platform** - Comprehensive cloud computing services\n- **DigitalOcean** - Developer-focused cloud infrastructure\n- **Vercel** - Frontend deployment and edge platform\n- **Netlify** - Web development and deployment platform\n\n### Developer Tools & Platforms (5)\n- **Docker** - Container platform and services\n- **GitHub** - Version control and collaboration platform\n- **npm** - JavaScript package manager and registry\n- **Atlassian** - Developer collaboration tools (Jira, Bitbucket, Confluence)\n- **Supabase** - Open source backend platform (PostgreSQL, auth, storage)\n\n### Productivity & Collaboration (5)\n- **LinkedIn** - Professional networking platform\n- **Slack** - Business communication and collaboration\n- **Asana** - Team workflow and project management\n- **Dropbox** - Cloud file storage and collaboration\n- **X (Twitter)** - Social media and real-time communication\n\n### Web Infrastructure & Security (3)\n- **Cloudflare** - Web infrastructure, CDN, and security\n- **Discord** - Developer community and communication platform\n- **Reddit** - Social news and developer community platform\n\n### Analytics & Business Tools (1)\n- **Amplitude** - Product analytics platform\n\n## Example Usage\n\nHere are various examples of how to use the Status Observer with Claude:\n\n### Direct Commands:\n\n```\n# AI Platforms\nstatus --openrouter\nstatus --openai\nstatus --anthropic\nstatus --gemini\n\n# Cloud Infrastructure\nstatus --gcp\nstatus --vercel\nstatus --digitalocean\nstatus --netlify\n\n# Developer Tools\nstatus --docker\nstatus --github\nstatus --atlassian\nstatus --supabase\nstatus --npm\n\n# Productivity & Social\nstatus --linkedin\nstatus --slack\nstatus --x\nstatus --dropbox\n\n# Web Infrastructure\nstatus --cloudflare\nstatus --discord\n\n# All platforms\nstatus --all\nstatus list\n```\n\n### Preview\n![OpenRouter Status Monitoring Preview](https://github.com/imprvhub/mcp-status-observer/raw/main/public/assets/openrouter.png)\n![GCP Status Monitoring Preview](https://github.com/imprvhub/mcp-status-observer/raw/main/public/assets/gcp.png)\n\n### Natural Language Prompts:\n\nYou can also interact with the MCP using natural language. Claude will interpret these requests and use the appropriate commands:\n\n- \"Could you check if OpenRouter is having any API issues right now?\"\n- \"What's the status of OpenAI's ChatGPT service?\"\n- \"Has there been any recent incidents with Claude or the Anthropic API?\"\n- \"Is Google Cloud Platform experiencing any outages in my region?\"\n- \"Check if Docker Hub is operational for automated builds\"\n- \"What's the current status of LinkedIn's Sales Navigator?\"\n- \"Can you tell me if Google's Gemini AI is experiencing any service disruptions?\"\n- \"Show me the status of all AI platforms including OpenRouter and OpenAI\"\n- \"Are there any active incidents affecting GitHub Actions or Git operations?\"\n- \"Check the overall health of Vercel and Netlify for my deployment pipeline\"\n- \"Has Supabase had any recent database or authentication issues?\"\n- \"What's the status of all major platforms right now?\"\n\n\n## Troubleshooting\n\n### \"Server disconnected\" error\nIf you see the error \"MCP Status Observer: Server disconnected\" in Claude Desktop:\n\n1. **Verify the server is running**:\n   - Open a terminal and manually run `node build/index.js` from the project directory\n   - If the server starts successfully, use Claude while keeping this terminal open\n\n2. **Check your configuration**:\n   - Ensure the absolute path in `claude_desktop_config.json` is correct for your system\n   - Double-check that you've used double backslashes (`\\\\`) for Windows paths\n   - Verify you're using the complete path from the root of your filesystem\n\n### Tools not appearing in Claude\nIf the Status Observer tools don't appear in Claude:\n- Make sure you've restarted Claude Desktop after configuration\n- Check the Claude Desktop logs for any MCP communication errors\n- Ensure the MCP server process is running (run it manually to confirm)\n- Verify that the MCP server is correctly registered in the Claude Desktop MCP registry\n\n### Checking if the server is running\nTo check if the server is running:\n\n- **Windows**: Open Task Manager, go to the \"Details\" tab, and look for \"node.exe\"\n- **macOS/Linux**: Open Terminal and run `ps aux | grep node`\n\nIf you don't see the server running, start it manually or use the auto-start method.\n\n## Contributing\n\n### Adding New Status APIs\n\nContributors can easily add support for additional platforms by modifying the `initializePlatforms` method in `src/index.ts`. The process is straightforward:\n\n1. Identify a platform's status API endpoint\n2. Add a new entry using the `addPlatform` method with the following parameters:\n   - `id`: A unique identifier for the platform (lowercase, no spaces)\n   - `name`: The display name of the platform\n   - `url`: The status API endpoint URL\n   - `description`: A brief description of the platform\n\nExample:\n```typescript\nthis.addPlatform('newservice', 'New Service', 'https://status.newservice.com/api/v2/summary.json', 'Description of the service');\n```\n\n### Custom API Integration\n\nFor platforms with non-standard status pages (like OpenRouter, OpenAI, Anthropic), you can create custom handlers:\n\n1. Add the platform to `initializePlatforms()`\n2. Create a TypeScript interface for the response format\n3. Add a specific handler method like `getOpenRouterStatus()`\n4. Update the main `getPlatformStatus()` method to route to your handler\n5. Add quick status support in `getQuickPlatformStatus()`\n\nExample structure for custom handlers:\n```typescript\nprivate async getCustomPlatformStatus(platform: PlatformStatus): Promise<string> {\n  // Custom parsing logic for your platform\n  // Return formatted status text\n}\n```\n\n### Platform Categories\n\nWhen adding new platforms, consider organizing them into logical categories:\n- **AI/ML**: OpenRouter, OpenAI, Anthropic, Gemini\n- **Cloud Infrastructure**: GCP, AWS, Azure, DigitalOcean\n- **Developer Tools**: GitHub, GitLab, Docker, npm\n- **Productivity**: Slack, Microsoft 365, Google Workspace\n- **Web Infrastructure**: Cloudflare, Fastly, Akamai\n\n## License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-claude-hackernews/blob/main/LICENSE) file for details.\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)\n- [MCP Series](https://github.com/mcp-series)\n\n## Changelog\n\n- **2025-09-12**: Added OpenRouter integration with RSS incident tracking and detailed impact analysis\n- **2025-04-26**: Added Docker status integration with comprehensive component monitoring\n- **2025-03-15**: Enhanced GCP regional status reporting with incident correlation\n- **2025-02-28**: Added Anthropic and Gemini AI platform monitoring\n- **2025-01-20**: Initial release with core platform support (GitHub, Vercel, Cloudflare, etc.)\n\n---\n\n*Built for the developer community by [imprvhub](https://github.com/imprvhub)*",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "application",
        "protocol",
        "application monitoring",
        "server monitoring",
        "monitoring operational"
      ],
      "category": "monitoring"
    },
    "inspektor-gadget--ig-mcp-server": {
      "owner": "inspektor-gadget",
      "name": "ig-mcp-server",
      "url": "https://github.com/inspektor-gadget/ig-mcp-server",
      "imageUrl": "",
      "description": "Debug your Container and Kubernetes workloads with an AI interface powered by eBPF.",
      "stars": 15,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-02T09:49:39Z",
      "readme_content": "[![GitHub Release](https://img.shields.io/github/v/release/inspektor-gadget/ig-mcp-server)](https://github.com/inspektor-gadget/ig-mcp-server/releases)\n[![License](https://img.shields.io/github/license/inspektor-gadget/ig-mcp-server)](LICENSE)\n[![Slack](https://img.shields.io/badge/slack-%23inspektor--gadget-brightgreen)](https://kubernetes.slack.com/channels/inspektor-gadget)\n[![Go Report Card](https://goreportcard.com/badge/github.com/inspektor-gadget/ig-mcp-server)](https://goreportcard.com/report/github.com/inspektor-gadget/ig-mcp-server)\n[![Examples](https://img.shields.io/badge/examples-view-orange)](examples/README.md)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/inspektor-gadget/ig-mcp-server)\n\n# Inspektor Gadget MCP Server\n\nAI-powered debugging and inspection for Kubernetes clusters using Inspektor Gadget.\n\nhttps://github.com/user-attachments/assets/86367982-c0aa-455c-ac9e-ca43348899df\n\n## Features\n\n- AI-powered interface for Kubernetes troubleshooting and monitoring\n- One-click Inspektor Gadget deployment and removal\n- Intelligent output summarization and analysis\n- Automatic gadget discovery from Artifact Hub\n\n## Quick Start\n\n1. Ensure you have Docker and a valid `kubeconfig` file\n2. Configure the MCP server in VS Code (see [INSTALL.md](INSTALL.md))\n3. Start using AI commands in VS Code Copilot Chat\n4. Try: \"Show me DNS traffic\" or \"Deploy Inspektor Gadget\"\n5. Head to [examples](examples/README.md) for detailed examples.\n\n## Installation\n\nYou can use the following commands to quickly configure the Inspektor Gadget MCP server using either Docker or a binary in your VS Code settings.\n\n### Docker\n\n<summary>\n  <details>\n    <summary>Install Inspektor Gadget MCP Server - Artifact Hub Discovery</summary>\n    <pre><code>code --add-mcp '{\n  \"name\": \"inspektor-gadget\",\n  \"command\": \"docker\",\n  \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"--volume\",\n    \"ig-mcp-cache:/root/.cache/ig-mcp-server\",\n    \"--mount\",\n    \"type=bind,src=${env:HOME}/.kube/config,dst=/kubeconfig\",\n    \"ghcr.io/inspektor-gadget/ig-mcp-server:latest\",\n    \"-gadget-discoverer=artifacthub\"\n  ]\n}'</code></pre>\n  </details>\n<details>\n    <summary>Install Inspektor Gadget MCP Server - Specific Gadgets</summary>\n    <pre><code>code --add-mcp '{\n  \"name\": \"inspektor-gadget\",\n  \"command\": \"docker\",\n  \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"--volume\",\n    \"ig-mcp-cache:/root/.cache/ig-mcp-server\",\n    \"--mount\",\n    \"type=bind,src=${env:HOME}/.kube/config,dst=/kubeconfig\",\n    \"ghcr.io/inspektor-gadget/ig-mcp-server:latest\",\n    \"-gadget-images=trace_dns:latest,trace_tcp:latest,snapshot_process:latest,snapshot_socket:latest\"\n  ]\n}'</code></pre>\n  </details>\n</summary>\n\n### Binary\n\nYou can head to the [Releases](https://github.com/inspektor-gadget/ig-mcp-server/releases) page and download the latest binary for your platform:\n\n<summary>\n  <details>\n    <summary>Linux</summary>\n    <pre><code>MCP_VERSION=$(curl -s https://api.github.com/repos/inspektor-gadget/ig-mcp-server/releases/latest | jq -r .tag_name)\nMCP_ARCH=amd64\ncurl -sL https://github.com/inspektor-gadget/ig-mcp-server/releases/download/${MCP_VERSION}/ig-mcp-server-linux-${MCP_ARCH}.tar.gz | sudo tar -C /usr/local/bin -xzf - ig-mcp-server\n</code></pre>\n  </details>\n  <details>\n    <summary>macOS</summary>\n    <pre><code>MCP_VERSION=$(curl -s https://api.github.com/repos/inspektor-gadget/ig-mcp-server/releases/latest | jq -r .tag_name)\nMCP_ARCH=arm64\ncurl -sL https://github.com/inspektor-gadget/ig-mcp-server/releases/download/${MCP_VERSION}/ig-mcp-server-darwin-${MCP_ARCH}.tar.gz | sudo tar -C /usr/local/bin -xzf - ig-mcp-server\n</code></pre>\n  </details>\n  <details>\n    <summary>Windows</summary>\n    <pre><code>$MCP_VERSION = (curl.exe -s https://api.github.com/repos/inspektor-gadget/ig-mcp-server/releases/latest | ConvertFrom-Json).tag_name\n$MCP_ARCH = \"amd64\"\ncurl.exe -L \"https://github.com/inspektor-gadget/ig-mcp-server/releases/download/$MCP_VERSION/ig-mcp-server-windows-$MCP_ARCH.tar.gz\" -o \"ig-mcp-server.tar.gz\"\n$destPath = \"C:\\Program Files\\ig-mcp-server\"\nif (-Not (Test-Path $destPath -PathType Container)) { mkdir $destPath}\ntar.exe -xzf \"ig-mcp-server.tar.gz\" -C \"$destPath\"\nrm ig-mcp-server.tar.gz\nWrite-Host \"‚úÖ Extracted to $destPath\"\nWrite-Host \"üëâ Please add '$destPath' to your PATH environment variable manually.\"\n</code></pre>\n  </details>\n</summary>\n\nAfter downloading, you can run the following command to add it to your VS Code MCP configuration.\n\n<summary>\n  <details>\n    <summary>Install Inspektor Gadget MCP Server - Artifact Hub Discovery</summary>\n    <pre><code>code --add-mcp '{\n  \"name\": \"inspektor-gadget\",\n  \"command\": \"ig-mcp-server\",\n  \"args\": [\n    \"-gadget-discoverer=artifacthub\"\n  ]\n}'</code></pre>\n  </details>\n<details>\n    <summary>Install Inspektor Gadget MCP Server - Specific Gadgets</summary>\n    <pre><code>code --add-mcp '{\n    \"name\": \"inspektor-gadget\",\n    \"command\": \"ig-mcp-server\",\n    \"args\": [\n      \"-gadget-images=trace_dns:latest,trace_tcp:latest\"\n    ]\n}'</code></pre>\n    </details>\n</summary>\n\n## Available Tools\n\n### Inspektor Gadget Lifecycle\n\n- **ig_deploy**: Manage deployment of Inspektor Gadget on target Kubernetes cluster\n\n### Gadget Lifecycle\n\n- **ig_gadgets**: Manage the lifecycle of running gadgets (stop/list/get-results)\n\n### Dynamic Tools\n\nEach gadget is registered as its own MCP tool (e.g., `gadget_trace_dns`, `gadget_trace_tcp`, etc.) and supports running gadgets in foreground mode, which is useful for debugging/development and also in background mode for observability.\n\nAlso, You can control which gadgets are available by configuring the MCP server with the `-gadget-discoverer` or `-gadget-images` options, allowing you to limit the tools to only those you need.\n\n#### Gadget Discovery\n\nGadget discovery allows controlling which gadgets are available for use. You can choose between two methods:\n\n- **Automatic**: Uses Artifact Hub (`-gadget-discoverer=artifacthub`)\n- **Manual**: Specify gadgets directly (`-gadget-images=trace_dns:latest`)\n\n![Gadget Tools](media/gadget-tools.png)\n\nSee [INSTALL.md](INSTALL.md) for configuration options.\n\n## Security Notes\n\n- Requires read-only access to your kubeconfig file\n- Needs network access for Artifact Hub discovery\n- See [security guide](SECURITY.md) for setting up server with minimal permissions\n\n## Resources\n\n- üìñ [Documentation](https://inspektor-gadget.io/docs/)\n- üîç [Examples](examples/README.md)\n- üêõ [Issues](https://github.com/inspektor-gadget/ig-mcp-server/issues)\n- üí¨ [Slack](https://kubernetes.slack.com/channels/inspektor-gadget)\n- üåê [Website](https://inspektor-gadget.io/)\n- üìã [Troubleshooting](TROUBLESHOOTING.md)\n- üîí [Security Guide](SECURITY.md)\n\n## Related Projects\n\n- [Inspektor Gadget](https://github.com/inspektor-gadget/inspektor-gadget) - Kubernetes debugging tool\n- [MCP Specification](https://spec.modelcontextprotocol.io/) - Model Context Protocol\n\n## License\n\nApache License 2.0 - see [LICENSE](LICENSE) for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "kubernetes",
        "monitoring",
        "ai interface",
        "application monitoring",
        "container kubernetes"
      ],
      "category": "monitoring"
    },
    "inventer-dev--mcp-internet-speed-test": {
      "owner": "inventer-dev",
      "name": "mcp-internet-speed-test",
      "url": "https://github.com/inventer-dev/mcp-internet-speed-test",
      "imageUrl": "",
      "description": "Internet speed testing with network performance metrics including download/upload speed, latency, jitter analysis, and CDN server detection with geographic mapping",
      "stars": 10,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-28T17:18:29Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@inventer-dev/mcp-internet-speed-test)](https://smithery.ai/server/@inventer-dev/mcp-internet-speed-test)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/inventer-dev-mcp-internet-speed-test-badge.png)](https://mseep.ai/app/inventer-dev-mcp-internet-speed-test)\n\n<a href=\"https://glama.ai/mcp/servers/@inventer-dev/mcp-internet-speed-test\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@inventer-dev/mcp-internet-speed-test/badge\" alt=\"mcp-internet-speed-test MCP server\" />\n</a>\n\n# MCP Internet Speed Test\n\nAn implementation of a Model Context Protocol (MCP) for internet speed testing. It allows AI models and agents to measure, analyze, and report network performance metrics through a standardized interface.\n\n**üì¶ Available on PyPI:** https://pypi.org/project/mcp-internet-speed-test/\n\n**üöÄ Quick Start:**\n```bash\npip install mcp-internet-speed-test\nmcp-internet-speed-test\n```\n\n## What is MCP?\n\nThe Model Context Protocol (MCP) provides a standardized way for Large Language Models (LLMs) to interact with external tools and data sources. Think of it as the \"USB-C for AI applications\" - a common interface that allows AI systems to access real-world capabilities and information.\n\n## Features\n\n- **Smart Incremental Testing**: Uses SpeedOf.Me methodology with 8-second threshold for optimal accuracy\n- **Download Speed Testing**: Measures bandwidth using files from 128KB to 100MB from GitHub repository\n- **Upload Speed Testing**: Tests upload bandwidth using generated data from 128KB to 100MB\n- **Latency Testing**: Measures network latency with detailed server location information\n- **Jitter Analysis**: Calculates network stability using multiple latency samples (default: 5)\n- **Multi-CDN Support**: Detects and provides info for Fastly, Cloudflare, and AWS CloudFront\n- **Geographic Location**: Maps POP codes to physical locations (50+ locations worldwide)\n- **Cache Analysis**: Detects HIT/MISS status and cache headers\n- **Server Metadata**: Extracts detailed CDN headers including `x-served-by`, `via`, `x-cache`\n- **Comprehensive Testing**: Single function to run all tests with complete metrics\n\n## Installation\n\n### Prerequisites\n\n- Python 3.12 or higher (required for async support)\n- pip or [uv](https://github.com/astral-sh/uv) package manager\n\n### Option 1: Install from PyPI with pip (Recommended)\n\n```bash\n# Install the package globally\npip install mcp-internet-speed-test\n\n# Run the MCP server\nmcp-internet-speed-test\n```\n\n### Option 2: Install from PyPI with uv\n\n```bash\n# Install the package globally\nuv add mcp-internet-speed-test\n\n# Or run directly without installing\nuvx mcp-internet-speed-test\n```\n\n### Option 3: Using docker\n\n```bash\n# Build the Docker image\ndocker build -t mcp-internet-speed-test .\n\n# Run the MCP server in a Docker container\ndocker run -it --rm -v $(pwd):/app -w /app mcp-internet-speed-test\n```\n\n### Option 4: Development/Local Installation\n\nIf you want to contribute or modify the code:\n\n```bash\n# Clone the repository\ngit clone https://github.com/inventer-dev/mcp-internet-speed-test.git\ncd mcp-internet-speed-test\n\n# Install in development mode\npip install -e .\n\n# Or using uv\nuv sync\nuv run python -m mcp_internet_speed_test.main\n```\n\n### Dependencies\n\nThe package automatically installs these dependencies:\n- `mcp[cli]>=1.6.0`: MCP server framework with CLI integration\n- `httpx>=0.27.0`: Async HTTP client for speed tests\n\n\n## Configuration\n\nTo use this MCP server with Claude Desktop or other MCP clients, add it to your MCP configuration file.\n\n### Claude Desktop Configuration\n\nEdit your Claude Desktop MCP configuration file:\n\n#### Option 1: Using pip installed package (Recommended)\n\n```json\n{\n    \"mcpServers\": {\n        \"mcp-internet-speed-test\": {\n            \"command\": \"mcp-internet-speed-test\"\n        }\n    }\n}\n```\n\n#### Option 2: Using uvx\n\n```json\n{\n    \"mcpServers\": {\n        \"mcp-internet-speed-test\": {\n            \"command\": \"uvx\",\n            \"args\": [\"mcp-internet-speed-test\"]\n        }\n    }\n}\n```\n\n## API Tools\n\nThe MCP Internet Speed Test provides the following tools:\n\n### Testing Functions\n1. `measure_download_speed`: Measures download bandwidth (in Mbps) with server location info\n2. `measure_upload_speed`: Measures upload bandwidth (in Mbps) with server location info\n3. `measure_latency`: Measures network latency (in ms) with server location info\n4. `measure_jitter`: Measures network jitter by analyzing latency variations with server info\n5. `get_server_info`: Get detailed CDN server information for any URL without running speed tests\n6. `run_complete_test`: Comprehensive test with all metrics and server metadata\n\n## CDN Server Detection\n\nThis speed test now provides detailed information about the CDN servers serving your tests:\n\n### What You Get\n- **CDN Provider**: Identifies if you're connecting to Fastly, Cloudflare, or Amazon CloudFront\n- **Geographic Location**: Shows the physical location of the server (e.g., \"Mexico City, Mexico\")\n- **POP Code**: Three-letter code identifying the Point of Presence (e.g., \"MEX\", \"QRO\", \"DFW\")\n- **Cache Status**: Whether content is served from cache (HIT) or fetched from origin (MISS)\n- **Server Headers**: Full HTTP headers including `x-served-by`, `via`, and `x-cache`\n\n### Technical Implementation\n\n#### Smart Testing Methodology\n- **Incremental Approach**: Starts with small files (128KB) and progressively increases\n- **Time-Based Optimization**: Uses 8-second base threshold + 4-second additional buffer\n- **Accuracy Focus**: Selects optimal file size that provides reliable measurements\n- **Multi-Provider Support**: Tests against geographically distributed endpoints\n\n#### CDN Detection Capabilities\n- **Fastly**: Detects POP codes and maps to 50+ global locations\n- **Cloudflare**: Identifies data centers and geographic regions\n- **AWS CloudFront**: Recognizes edge locations across continents\n- **Header Analysis**: Parses `x-served-by`, `via`, `x-cache`, and custom CDN headers\n\n### Why This Matters\n- **Network Diagnostics**: Understand which server is actually serving your tests\n- **Performance Analysis**: Correlate speed results with server proximity\n- **CDN Optimization**: Identify if your ISP's routing is optimal\n- **Geographic Awareness**: Know if tests are running from your expected region\n- **Troubleshooting**: Identify routing issues and CDN misconfigurations\n\n### Example Server Info Output\n```json\n{\n  \"cdn_provider\": \"Fastly\",\n  \"pop_code\": \"MEX\",\n  \"pop_location\": \"Mexico City, Mexico\",\n  \"served_by\": \"cache-mex4329-MEX\",\n  \"cache_status\": \"HIT\",\n  \"x_cache\": \"HIT, HIT\"\n}\n```\n\n### Technical Configuration\n\n#### Default Test Files Repository\n```\nGitHub Repository: inventer-dev/speed-test-files\nBranch: main\nFile Sizes: 128KB, 256KB, 512KB, 1MB, 2MB, 5MB, 10MB, 20MB, 40MB, 50MB, 100MB\n```\n\n#### Upload Endpoints Priority\n1. **Cloudflare Workers** (httpi.dev) - Global distribution, highest priority\n2. **HTTPBin** (httpbin.org) - AWS-based, secondary endpoint\n\n#### Supported CDN Locations (150+ POPs)\n\n**Fastly POPs**: MEX, QRO, DFW, LAX, NYC, MIA, LHR, FRA, AMS, CDG, NRT, SIN, SYD, GRU, SCL, BOG, MAD, MIL...\n\n**Cloudflare Centers**: DFW, LAX, SJC, SEA, ORD, MCI, IAD, ATL, MIA, YYZ, LHR, FRA, AMS, CDG, ARN, STO...\n\n**AWS CloudFront**: ATL, BOS, ORD, CMH, DFW, DEN, IAD, LAX, MIA, MSP, JFK, SEA, SJC, AMS, ATH, TXL...\n\n#### Performance Thresholds\n- **Base Test Duration**: 8.0 seconds\n- **Additional Buffer**: 4.0 seconds\n- **Maximum File Size**: Configurable (default: 100MB)\n- **Jitter Samples**: 5 measurements (configurable)\n\n## Troubleshooting\n\n### Common Issues\n\n#### MCP Server Connection\n1. **Path Configuration**: Ensure absolute path is used in MCP configuration\n2. **Directory Permissions**: Verify read/execute permissions for the project directory\n3. **Python Version**: Requires Python 3.12+ with async support\n4. **Dependencies**: Install `fastmcp` and `httpx` packages\n\n#### Speed Test Issues\n1. **GitHub Repository Access**: Ensure `inventer-dev/speed-test-files` is accessible\n2. **Firewall/Proxy**: Check if corporate firewalls block test endpoints\n3. **CDN Routing**: Some ISPs may route differently to CDNs\n4. **Network Stability**: Jitter tests require stable connections\n\n#### Performance Considerations\n- **File Size Limits**: Large files (>50MB) may timeout on slow connections\n- **Upload Endpoints**: If primary endpoint fails, fallback is automatic\n- **Geographic Accuracy**: POP detection depends on CDN header consistency\n\n## Development\n\n### Project Structure\n```\nmcp-internet-speed-test/\n‚îú‚îÄ‚îÄ mcp_internet_speed_test/  # Main package directory\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # Package initialization\n‚îÇ   ‚îî‚îÄ‚îÄ main.py          # MCP server implementation\n‚îú‚îÄ‚îÄ README.md           # This documentation\n‚îú‚îÄ‚îÄ Dockerfile          # Container configuration\n‚îî‚îÄ‚îÄ pyproject.toml      # Python project configuration\n```\n\n### Key Components\n\n#### Configuration Constants\n- `GITHUB_RAW_URL`: Base URL for test files repository\n- `UPLOAD_ENDPOINTS`: Prioritized list of upload test endpoints\n- `SIZE_PROGRESSION`: Ordered list of file sizes for incremental testing\n- `*_POP_LOCATIONS`: Mappings of CDN codes to geographic locations\n\n#### Core Functions\n- `extract_server_info()`: Parses HTTP headers to identify CDN providers\n- `measure_*()`: Individual test functions for different metrics\n- `run_complete_test()`: Orchestrates comprehensive testing suite\n\n### Configuration Customization\n\nYou can customize the following in `mcp_internet_speed_test/main.py` if you clone the repository:\n```python\n# GitHub repository settings\nGITHUB_USERNAME = \"your-username\"\nGITHUB_REPO = \"your-speed-test-files\"\nGITHUB_BRANCH = \"main\"\n\n# Test duration thresholds\nBASE_TEST_DURATION = 8.0  # seconds\nADDITIONAL_TEST_DURATION = 4.0  # seconds\n\n# Default endpoints\nDEFAULT_UPLOAD_URL = \"your-upload-endpoint\"\nDEFAULT_LATENCY_URL = \"your-latency-endpoint\"\n```\n\n### Contributing\n\nThis is an experimental project and contributions are welcome:\n\n1. **Issues**: Report bugs or request features\n2. **Pull Requests**: Submit code improvements\n3. **Documentation**: Help improve this README\n4. **Testing**: Test with different network conditions and CDNs\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\n- MCP Framework maintainers for standardizing AI tool interactions\n- The Model Context Protocol community for documentation and examples",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "latency",
        "ai",
        "test internet",
        "speed testing",
        "application monitoring"
      ],
      "category": "monitoring"
    },
    "last9--last9-mcp-server": {
      "owner": "last9",
      "name": "last9-mcp-server",
      "url": "https://github.com/last9/last9-mcp-server",
      "imageUrl": "",
      "description": "Seamlessly bring real-time production context‚Äîlogs, metrics, and traces‚Äîinto your local environment to auto-fix code faster",
      "stars": 46,
      "forks": 8,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-02T21:44:46Z",
      "readme_content": "# Last9 MCP Server\n\n![last9 mcp demo](mcp-demo.gif)\n\nA [Model Context Protocol](https://modelcontextprotocol.io/) server\nimplementation for [Last9](https://last9.io/mcp/) that enables AI agents to\nseamlessly bring real-time production context ‚Äî logs, metrics, and traces ‚Äî into\nyour local environment to auto-fix code faster.\n\n- [View demo](https://www.youtube.com/watch?v=AQH5xq6qzjI)\n- Read our\n  [announcement blog post](https://last9.io/blog/launching-last9-mcp-server/)\n\n## Status\n\nWorks with Claude desktop app, or Cursor, Windsurf, and VSCode (Github Copilot)\nIDEs. Implements the following MCP\n[tools](https://modelcontextprotocol.io/docs/concepts/tools):\n\n**Observability & APM Tools:**\n\n- `get_exceptions`: Get the list of exceptions.\n- `get_service_summary`: Get service summary with throughput, error rate, and response time.\n- `get_service_environments`: Get available environments for services.\n- `get_service_performance_details`: Get detailed performance metrics for a service.\n- `get_service_operations_summary`: Get operations summary for a service.\n- `get_service_dependency_graph`: Get service dependency graph showing incoming/outgoing dependencies.\n\n**Prometheus/PromQL Tools:**\n\n- `prometheus_range_query`: Execute PromQL range queries for metrics data.\n- `prometheus_instant_query`: Execute PromQL instant queries for metrics data.\n- `prometheus_label_values`: Get label values for PromQL queries.\n- `prometheus_labels`: Get available labels for PromQL queries.\n\n**Logs Management:**\n\n- `get_logs`: Get logs filtered by service name and/or severity level.\n- `get_drop_rules`: Get drop rules for logs that determine what logs get\n  filtered out at [Last9 Control Plane](https://last9.io/control-plane)\n- `add_drop_rule`: Create a drop rule for logs at\n  [Last9 Control Plane](https://last9.io/control-plane)\n- `get_service_logs`: Get raw log entries for a specific service over a time range. Can apply filters on severity and body.\n- `get_log_attributes`: Get available log attributes (labels) for a specified time window.\n\n**Traces Management:**\n\n- `get_service_traces`: Query traces for a specific service with filtering options for span kinds, status codes, and other trace attributes.\n- `get_trace_attributes`: Get available trace attributes (series) for a specified time window.\n\n**Change Events:**\n\n- `get_change_events`: Get change events from the last9_change_events prometheus metric over a given time range.\n\n**Alert Management:**\n\n- `get_alert_config`: Get alert configurations (alert rules) from Last9.\n- `get_alerts`: Get currently active alerts from Last9 monitoring system.\n\n## Tools Documentation\n\n### get_exceptions\n\nRetrieves server-side exceptions over a specified time range.\n\nParameters:\n\n- `limit` (integer, optional): Maximum number of exceptions to return.\n  Default: 20.\n- `lookback_minutes` (integer, recommended): Number of minutes to look back from\n  now. Default: 60. Examples: 60, 30, 15.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD\n  HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD\n  HH:MM:SS). Leave empty to default to current time.\n- `span_name` (string, optional): Name of the span to filter by.\n\n### get_service_summary\n\nGet service summary over a given time range. Includes service name, environment, throughput, error rate, and response time. All values are p95 quantiles over the time range.\n\nParameters:\n\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to end_time_iso - 1 hour.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### get_service_environments\n\nGet available environments for services. Returns an array of environments that can be used with other APM tools.\n\nParameters:\n\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to end_time_iso - 1 hour.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\nNote: All other APM tools that retrieve service information (like `get_service_performance_details`, `get_service_dependency_graph`, `get_service_operations_summary`, `get_service_summary`) require an `env` parameter. This parameter must be one of the environments returned by this tool. If this tool returns an empty array, use an empty string `\"\"` for the env parameter.\n\n### get_service_performance_details\n\nGet detailed performance metrics for a specific service over a given time range.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get performance details for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### get_service_operations_summary\n\nGet a summary of operations inside a service over a given time range. Returns operations like HTTP endpoints, database queries, messaging producer and HTTP client calls.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get operations summary for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### get_service_dependency_graph\n\nGet details of the throughput, response times and error rates of incoming, outgoing and infrastructure components of a service. Useful for analyzing cascading effects of errors and performance issues.\n\nParameters:\n\n- `service_name` (string, optional): Name of the service to get the dependency graph for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `env` (string, optional): Environment to filter by. Defaults to 'prod'.\n\n### prometheus_range_query\n\nPerform a Prometheus range query to get metrics data over a specified time range. Recommended to check available labels first using `prometheus_labels` tool.\n\nParameters:\n\n- `query` (string, required): The range query to execute.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### prometheus_instant_query\n\nPerform a Prometheus instant query to get metrics data at a specific point in time. Typically should use rollup functions like sum_over_time, avg_over_time, quantile_over_time over a time window.\n\nParameters:\n\n- `query` (string, required): The instant query to execute.\n- `time_iso` (string, optional): Time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### prometheus_label_values\n\nReturn the label values for a particular label and PromQL filter query. Similar to Prometheus /label_values call.\n\nParameters:\n\n- `match_query` (string, required): A valid PromQL filter query.\n- `label` (string, required): The label to get values for.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### prometheus_labels\n\nReturn the labels for a given PromQL match query. Similar to Prometheus /labels call.\n\nParameters:\n\n- `match_query` (string, required): A valid PromQL filter query.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - 60 minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\n### get_logs\n\nGets logs filtered by service name and/or severity level within a specified time range. This tool now uses the advanced v2 logs API with physical index optimization for better performance.\n\n**Note**: This tool now requires a `service_name` parameter and internally uses the same advanced infrastructure as `get_service_logs`.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get logs for.\n- `severity` (string, optional): Severity of the logs to get (automatically converted to severity_filters format).\n- `lookback_minutes` (integer, recommended): Number of minutes to look back from now. Default: 60. Examples: 60, 30, 15.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `limit` (integer, optional): Maximum number of logs to return. Default: 20.\n- `env` (string, optional): Environment to filter by. Use \"get_service_environments\" tool to get available environments.\n\n### get_drop_rules\n\nGets drop rules for logs, which determine what logs get filtered out from\nreaching Last9.\n\n### add_drop_rule\n\nAdds a new drop rule to filter out specific logs at\n[Last9 Control Plane](https://last9.io/control-plane)\n\nParameters:\n\n- `name` (string, required): Name of the drop rule.\n- `filters` (array, required): List of filter conditions to apply. Each filter\n  has:\n  - `key` (string, required): The key to filter on. Only attributes and\n    resource.attributes keys are supported. For resource attributes, use format:\n    resource.attributes[key_name] and for log attributes, use format:\n    attributes[key_name] Double quotes in key names must be escaped.\n  - `value` (string, required): The value to filter against.\n  - `operator` (string, required): The operator used for filtering. Valid\n    values:\n    - \"equals\"\n    - \"not_equals\"\n  - `conjunction` (string, required): The logical conjunction between filters.\n    Valid values:\n    - \"and\"\n\n### get_alert_config\n\nGet alert configurations (alert rules) from Last9. Returns all configured alert rules including their conditions, labels, and annotations.\n\nParameters:\n\nNone - This tool retrieves all available alert configurations.\n\nReturns information about:\n\n- Alert rule ID and name\n- Primary indicator being monitored\n- Current state and severity\n- Algorithm used for alerting\n- Entity ID and organization details\n- Properties and configuration\n- Creation and update timestamps\n- Group timeseries notification settings\n\n### get_alerts\n\nGet currently active alerts from Last9 monitoring system. Returns all alerts that are currently firing or have fired recently within the specified time window.\n\nParameters:\n\n- `timestamp` (integer, optional): Unix timestamp for the query time. Leave empty to default to current time.\n- `window` (integer, optional): Time window in seconds to look back for alerts. Defaults to 900 seconds (15 minutes). Range: 60-86400 seconds.\n\nReturns information about:\n\n- Alert rule details (ID, name, group, type)\n- Current state and severity\n- Last fired timestamp and duration\n- Rule properties and configuration\n- Alert instances with current values\n- Metric degradation information\n- Group labels and annotations for each instance\n\n### get_service_logs\n\nGet raw log entries for a specific service over a time range. This tool retrieves actual log entries including log messages, timestamps, severity levels, and other metadata. Useful for debugging issues, monitoring service behavior, and analyzing specific log patterns.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get logs for.\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now. Default: 60 minutes. Examples: 60, 30, 15.\n- `limit` (integer, optional): Maximum number of log entries to return. Default: 20.\n- `env` (string, optional): Environment to filter by. Use \"get_service_environments\" tool to get available environments.\n- `severity_filters` (array, optional): Array of severity patterns to filter logs (e.g., [\"error\", \"warn\"]). Uses OR logic.\n- `body_filters` (array, optional): Array of message content patterns to filter logs (e.g., [\"timeout\", \"failed\"]). Uses OR logic.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\nFiltering behavior:\n- Multiple filter types are combined with AND logic (service AND severity AND body)\n- Each filter array uses OR logic (matches any pattern in the array)\n\nExamples:\n- service_name=\"api\" + severity_filters=[\"error\"] + body_filters=[\"timeout\"] ‚Üí finds error logs containing \"timeout\"\n- service_name=\"web\" + body_filters=[\"timeout\", \"failed\", \"error 500\"] ‚Üí finds logs containing any of these patterns\n\n### get_log_attributes\n\nGet available log attributes (labels) for a specified time window. This tool retrieves all attribute names that exist in logs during the specified time range, which can be used for filtering and querying logs.\n\nParameters:\n\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now for the time window. Default: 15. Examples: 15, 30, 60.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `region` (string, optional): AWS region to query. Leave empty to use default from configuration. Examples: ap-south-1, us-east-1, eu-west-1.\n\nReturns:\n- List of log attributes grouped into two categories:\n  - Log Attributes: Standard log fields like service, severity, body, level, etc.\n  - Resource Attributes: Resource-related fields prefixed with \"resource_\" like resource_k8s.pod.name, resource_service.name, etc.\n\n### get_service_traces\n\nQuery traces for a specific service with filtering options for span kinds, status codes, and other trace attributes. This tool retrieves distributed tracing data for debugging performance issues, understanding request flows, and analyzing service interactions.\n\nParameters:\n\n- `service_name` (string, required): Name of the service to get traces for.\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now. Default: 60 minutes. Examples: 60, 30, 15.\n- `limit` (integer, optional): Maximum number of traces to return. Default: 10.\n- `env` (string, optional): Environment to filter by. Use \"get_service_environments\" tool to get available environments.\n- `span_kind` (array, optional): Filter by span types (server, client, internal, consumer, producer).\n- `span_name` (string, optional): Filter by specific span name.\n- `status_code` (array, optional): Filter by trace status (ok, error, unset, success).\n- `order` (string, optional): Field to order traces by. Default: \"Duration\". Options: Duration, Timestamp.\n- `direction` (string, optional): Sort direction. Default: \"backward\". Options: forward, backward.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n\nFiltering options:\n- Combine multiple filters to narrow down specific traces of interest\n- Use time range filters with lookback_minutes or explicit start/end times\n\nExamples:\n- service_name=\"api\" + span_kind=[\"server\"] + status_code=[\"error\"] ‚Üí finds failed server-side traces\n- service_name=\"payment\" + span_name=\"process_payment\" + lookback_minutes=30 ‚Üí finds payment processing traces from last 30 minutes\n\n### get_trace_attributes\n\nGet available trace attributes (series) for a specified time window. This tool retrieves all attribute names that exist in traces during the specified time range, which can be used for filtering and querying traces.\n\nParameters:\n\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now for the time window. Default: 15. Examples: 15, 30, 60.\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to use lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `region` (string, optional): AWS region to query. Leave empty to use default from configuration. Examples: ap-south-1, us-east-1, eu-west-1.\n\nReturns:\n- An alphabetically sorted list of all available trace attributes (e.g., http.method, http.status_code, db.name, resource_service.name, duration, etc.)\n\n### get_change_events\n\nGet change events from the last9_change_events prometheus metric over a given time range. Returns change events that occurred in the specified time window, including deployments, configuration changes, and other system modifications.\n\nParameters:\n\n- `start_time_iso` (string, optional): Start time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to now - lookback_minutes.\n- `end_time_iso` (string, optional): End time in ISO format (YYYY-MM-DD HH:MM:SS). Leave empty to default to current time.\n- `lookback_minutes` (integer, optional): Number of minutes to look back from now. Default: 60 minutes. Examples: 60, 30, 15.\n- `service` (string, optional): Name of the service to filter change events for.\n- `environment` (string, optional): Environment to filter by.\n- `event_name` (string, optional): Name of the change event to filter by (use available_event_names to see valid values).\n\nReturns:\n- `available_event_names`: List of all available event types that can be used for filtering\n- `change_events`: Array of timeseries data with metric labels and timestamp-value pairs\n- `count`: Total number of change events returned\n- `time_range`: Start and end time of the query window\n\nEach change event includes:\n- `metric`: Map of metric labels (service_name, env, event_type, message, etc.)\n- `values`: Array of timestamp-value pairs representing the timeseries data\n\nCommon event types include: deployment, config_change, rollback, scale_up/scale_down, restart, upgrade/downgrade, maintenance, backup/restore, health_check, certificate, database.\n\nBest practices:\n1. First call without event_name to get available_event_names\n2. Use exact event name from available_event_names for the event_name parameter\n3. Combine with other filters (service, environment, time) for precise results\n\n## Installation\n\nYou can install and run the Last9 Observability MCP server in several ways:\n\n### Local Installation\n\nFor local development and traditional STDIO usage:\n\n#### Homebrew\n\n```bash\n# Add the Last9 tap\nbrew tap last9/tap\n\n# Install the Last9 MCP CLI\nbrew install last9-mcp\n```\n\n#### NPM\n\n```bash\n# Install globally\nnpm install -g @last9/mcp-server\n\n# Or run directly with npx\nnpx @last9/mcp-server\n```\n\n## Configuration\n\n### Environment Variables\n\nThe Last9 MCP server requires the following environment variables:\n\n- `LAST9_BASE_URL`: (required) Last9 API URL from\n  [OTel integration](https://app.last9.io/integrations?integration=OpenTelemetry)\n- `LAST9_AUTH_TOKEN`: (required) Authentication token for Last9 MCP server from\n  [OTel integration](https://app.last9.io/integrations?integration=OpenTelemetry)\n- `LAST9_REFRESH_TOKEN`: (required) Refresh Token with Write permissions, needed\n  for accessing control plane APIs from\n  [API Access](https://app.last9.io/settings/api-access)\n\n## Usage\n\n## Usage with Claude Desktop\n\nConfigure the Claude app to use the MCP server:\n\n1. Open the¬†Claude Desktop¬†app, go to¬†Settings, then¬†Developer\n2. Click¬†Edit Config\n3. Open the¬†`claude_desktop_config.json`¬†file\n4. Copy and paste the server config to your existing file, then save\n5. Restart Claude\n\n### If installed via Homebrew:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"/opt/homebrew/bin/last9-mcp\",\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@last9/mcp-server\"],\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n## Usage with Cursor\n\nConfigure Cursor to use the MCP server:\n\n1. Open Cursor, go to¬†Settings, then¬†Cursor Settings\n2. Select¬†MCP¬†on the left\n3. Click Add \"New Global MCP Server\" at the top right\n4. Copy and paste the server config to your existing file, then save\n5. Restart Cursor\n\n### If installed via Homebrew:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"/opt/homebrew/bin/last9-mcp\",\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@last9/mcp-server\"],\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n## Usage with Windsurf\n\nConfigure Windsurf to use the MCP server:\n\n1. Open¬†Windsurf, go to¬†Settings, then¬†Developer\n2. Click¬†Edit Config\n3. Open the¬†`windsurf_config.json`¬†file\n4. Copy and paste the server config to your existing file, then save\n5. Restart Windsurf\n\n### If installed via Homebrew:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"/opt/homebrew/bin/last9-mcp\",\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcpServers\": {\n    \"last9\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@last9/mcp-server\"],\n      \"env\": {\n        \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n        \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n        \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n      }\n    }\n  }\n}\n```\n\n## Usage with VS Code\n\n> Note: MCP support in VS Code is available starting v1.99 and is currently in\n> preview. For advanced configuration options and alternative setup methods,\n> [view the VS Code MCP documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n1. Open VS Code, go to Settings, select the User tab, then Features, then Chat\n2. Click \"Edit settings.json\"\n3. Copy and paste the server config to your existing file, then save\n4. Restart VS Code\n\n### If installed via Homebrew:\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"last9\": {\n        \"type\": \"stdio\",\n        \"command\": \"/opt/homebrew/bin/last9-mcp\",\n        \"env\": {\n          \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n          \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n          \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n        }\n      }\n    }\n  }\n}\n```\n\n### If installed via NPM:\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"last9\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@last9/mcp-server\"],\n        \"env\": {\n          \"LAST9_BASE_URL\": \"<last9_otlp_host>\",\n          \"LAST9_AUTH_TOKEN\": \"<last9_otlp_auth_token>\",\n          \"LAST9_REFRESH_TOKEN\": \"<last9_write_refresh_token>\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Development\n\nFor local development and testing, you can run the MCP server in HTTP mode which makes it easier to debug requests and responses.\n\n### Running in HTTP Mode\n\nSet the `HTTP_MODE` environment variable to enable HTTP server mode:\n\n```bash\n# Export required environment variables\nexport LAST9_API_TOKEN=\"your_api_token\"\nexport LAST9_BASE_URL=\"https://your-last9-endpoint\"  # Your Last9 endpoint\nexport HTTP_MODE=true\nexport HTTP_PORT=8080  # Optional, defaults to 8080\n\n# Run the server\n./last9-mcp-server\n```\n\nThe server will start on `http://localhost:8080/mcp` and you can test it with curl:\n\n### Testing with curl\n\n```bash\n# Test get_service_logs\ncurl -X POST http://localhost:8080/mcp \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Mcp-Session-Id: session_$(date +%s)000000000\" \\\n    -d '{\n      \"jsonrpc\": \"2.0\",\n      \"id\": 1,\n      \"method\": \"tools/call\",\n      \"params\": {\n        \"name\": \"get_service_logs\",\n        \"arguments\": {\n          \"service_name\": \"your-service-name\",\n          \"lookback_minutes\": 30,\n          \"limit\": 10\n        }\n      }\n    }'\n\n# Test get_service_traces\ncurl -X POST http://localhost:8080/mcp \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Mcp-Session-Id: session_$(date +%s)000000000\" \\\n    -d '{\n      \"jsonrpc\": \"2.0\",\n      \"id\": 2,\n      \"method\": \"tools/call\",\n      \"params\": {\n        \"name\": \"get_service_traces\",\n        \"arguments\": {\n          \"service_name\": \"your-service-name\",\n          \"lookback_minutes\": 60,\n          \"limit\": 5\n        }\n      }\n    }'\n\n# List available tools\ncurl -X POST http://localhost:8080/mcp \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Mcp-Session-Id: session_$(date +%s)000000000\" \\\n    -d '{\n      \"jsonrpc\": \"2.0\",\n      \"id\": 3,\n      \"method\": \"tools/list\",\n      \"params\": {}\n    }'\n```\n\n### Building from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/last9/last9-mcp-server.git\ncd last9-mcp-server\n\n# Build the binary\ngo build -o last9-mcp-server\n\n# Run in development mode\nHTTP_MODE=true ./last9-mcp-server\n```\n\n**Note**: HTTP mode is for development and testing only. When integrating with Claude Desktop or other MCP clients, use the default STDIO mode (without `HTTP_MODE=true`).\n\n## Badges\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/last9-last9-mcp-server-badge.png)](https://mseep.ai/app/last9-last9-mcp-server)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "ai",
        "logs",
        "application monitoring",
        "monitoring data",
        "monitoring access"
      ],
      "category": "monitoring"
    },
    "mpeirone--zabbix-mcp-server": {
      "owner": "mpeirone",
      "name": "zabbix-mcp-server",
      "url": "https://github.com/mpeirone/zabbix-mcp-server",
      "imageUrl": "",
      "description": "Zabbix integration for hosts, items, triggers, templates, problems, data and more.",
      "stars": 80,
      "forks": 23,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-10-03T07:32:40Z",
      "readme_content": "# Zabbix MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n\nA comprehensive Model Context Protocol (MCP) server for Zabbix integration using FastMCP and python-zabbix-utils. This server provides complete access to Zabbix API functionality through MCP-compatible tools.\n\n<a href=\"https://glama.ai/mcp/servers/@mpeirone/zabbix-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mpeirone/zabbix-mcp-server/badge\" alt=\"zabbix-mcp-server MCP server\" />\n</a>\n\n## Features\n\n### üè† Host Management\n- `host_get` - Retrieve hosts with advanced filtering\n- `host_create` - Create new hosts with interfaces and templates\n- `host_update` - Update existing host configurations\n- `host_delete` - Remove hosts from monitoring\n\n### üë• Host Group Management\n- `hostgroup_get` - Retrieve host groups\n- `hostgroup_create` - Create new host groups\n- `hostgroup_update` - Modify existing host groups\n- `hostgroup_delete` - Remove host groups\n\n### üìä Item Management\n- `item_get` - Retrieve monitoring items with filtering\n- `item_create` - Create new monitoring items\n- `item_update` - Update existing items\n- `item_delete` - Remove monitoring items\n\n### ‚ö†Ô∏è Trigger Management\n- `trigger_get` - Retrieve triggers and alerts\n- `trigger_create` - Create new triggers\n- `trigger_update` - Modify existing triggers\n- `trigger_delete` - Remove triggers\n\n### üìã Template Management\n- `template_get` - Retrieve monitoring templates\n- `template_create` - Create new templates\n- `template_update` - Update existing templates\n- `template_delete` - Remove templates\n\n### üö® Problem & Event Management\n- `problem_get` - Retrieve current problems and issues\n- `event_get` - Get historical events\n- `event_acknowledge` - Acknowledge events and problems\n\n### üìà Data Retrieval\n- `history_get` - Access historical monitoring data\n- `trend_get` - Retrieve trend data and statistics\n\n### üë§ User Management\n- `user_get` - Retrieve user accounts\n- `user_create` - Create new users\n- `user_update` - Update user information\n- `user_delete` - Remove user accounts\n\n### üîß Maintenance Management\n- `maintenance_get` - Retrieve maintenance periods\n- `maintenance_create` - Schedule maintenance windows\n- `maintenance_update` - Modify maintenance periods\n- `maintenance_delete` - Remove maintenance schedules\n\n### üìä Additional Features\n- `graph_get` - Retrieve graph configurations\n- `discoveryrule_get` - Get discovery rules\n- `itemprototype_get` - Retrieve item prototypes\n- `configuration_export` - Export Zabbix configurations\n- `configuration_import` - Import configurations\n- `apiinfo_version` - Get API version information\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [uv](https://docs.astral.sh/uv/) package manager\n- Access to a Zabbix server with API enabled\n\n### Quick Start\n\n1. **Clone the repository:**\n   ```bash\n   git clone https://github.com/mpeirone/zabbix-mcp-server.git\n   cd zabbix-mcp-server\n   ```\n\n2. **Install dependencies:**\n   ```bash\n   uv sync\n   ```\n\n3. **Configure environment variables:**\n   ```bash\n   cp config/.env.example .env\n   # Edit .env with your Zabbix server details\n   ```\n\n4. **Test the installation:**\n   ```bash\n   uv run python scripts/test_server.py\n   ```\n\n## Configuration\n\n### Required Environment Variables\n\n- `ZABBIX_URL` - Your Zabbix server API endpoint (e.g., `https://zabbix.example.com`)\n\n### Authentication (choose one method)\n\n**Method 1: API Token (Recommended)**\n- `ZABBIX_TOKEN` - Your Zabbix API token\n\n**Method 2: Username/Password**\n- `ZABBIX_USER` - Your Zabbix username\n- `ZABBIX_PASSWORD` - Your Zabbix password\n\n### Optional Configuration\n\n- `READ_ONLY` - Set to `true`, `1`, or `yes` to enable read-only mode (only GET operations allowed)\n\n## Usage\n\n### Running the Server\n\n**With startup script (recommended):**\n```bash\nuv run python scripts/start_server.py\n```\n\n**Direct execution:**\n```bash\nuv run python src/zabbix_mcp_server.py\n```\n\n### Testing\n\n**Run test suite:**\n```bash\nuv run python scripts/test_server.py\n```\n\n### Read-Only Mode\n\nWhen `READ_ONLY=true`, the server will only expose GET operations (retrieve data) and block all create, update, and delete operations. This is useful for:\n\n- üìä Monitoring dashboards\n- üîç Read-only integrations\n- üîí Security-conscious environments\n- üõ°Ô∏è Preventing accidental modifications\n\n### Example Tool Calls\n\n**Get all hosts:**\n```python\nhost_get()\n```\n\n**Get hosts in specific group:**\n```python\nhost_get(groupids=[\"1\"])\n```\n\n**Create a new host:**\n```python\nhost_create(\n    host=\"server-01\",\n    groups=[{\"groupid\": \"1\"}],\n    interfaces=[{\n        \"type\": 1,\n        \"main\": 1,\n        \"useip\": 1,\n        \"ip\": \"192.168.1.100\",\n        \"dns\": \"\",\n        \"port\": \"10050\"\n    }]\n)\n```\n\n**Get recent problems:**\n```python\nproblem_get(recent=True, limit=10)\n```\n\n**Get history data:**\n```python\nhistory_get(\n    itemids=[\"12345\"],\n    time_from=1640995200,\n    limit=100\n)\n```\n\n## MCP Integration\n\nThis server is designed to work with MCP-compatible clients like Claude Desktop. See [MCP_SETUP.md](MCP_SETUP.md) for detailed integration instructions.\n\n## Docker Support\n\n### Using Docker Compose\n\n1. **Configure environment:**\n   ```bash\n   cp config/.env.example .env\n   # Edit .env with your settings\n   ```\n\n2. **Run with Docker Compose:**\n   ```bash\n   docker compose up -d\n   ```\n\n### Building Docker Image\n\n```bash\ndocker build -t zabbix-mcp-server .\n```\n\n## Development\n\n### Project Structure\n\n```\nzabbix-mcp-server/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ zabbix_mcp_server.py    # Main server implementation\n‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îú‚îÄ‚îÄ start_server.py         # Startup script with validation\n‚îÇ   ‚îî‚îÄ‚îÄ test_server.py          # Test script\n‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îú‚îÄ‚îÄ .env.example           # Environment configuration template\n‚îÇ   ‚îî‚îÄ‚îÄ mcp.json               # MCP client configuration example\n‚îú‚îÄ‚îÄ pyproject.toml             # Python project configuration\n‚îú‚îÄ‚îÄ requirements.txt           # Dependencies\n‚îú‚îÄ‚îÄ Dockerfile                 # Docker configuration\n‚îú‚îÄ‚îÄ docker-compose.yml         # Docker Compose setup\n‚îú‚îÄ‚îÄ README.md                  # This file\n‚îú‚îÄ‚îÄ MCP_SETUP.md              # MCP integration guide\n‚îú‚îÄ‚îÄ CONTRIBUTING.md           # Contribution guidelines\n‚îú‚îÄ‚îÄ CHANGELOG.md              # Version history\n‚îî‚îÄ‚îÄ LICENSE                   # MIT license\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n### Running Tests\n\n```bash\n# Test server functionality\nuv run python scripts/test_server.py\n\n# Test with Docker\ndocker-compose exec zabbix-mcp python scripts/test_server.py\n```\n\n## Error Handling\n\nThe server includes comprehensive error handling:\n\n- ‚úÖ Authentication errors are clearly reported\n- üîí Read-only mode violations are blocked with descriptive messages\n- ‚úîÔ∏è Invalid parameters are validated\n- üåê Network and API errors are properly formatted\n- üìù Detailed logging for troubleshooting\n\n## Security Considerations\n\n- üîë Use API tokens instead of username/password when possible\n- üîí Enable read-only mode for monitoring-only use cases\n- üõ°Ô∏è Secure your environment variables\n- üîê Use HTTPS for Zabbix server connections\n- üîÑ Regularly rotate API tokens\n- üìÅ Store configuration files securely\n\n## Troubleshooting\n\n### Common Issues\n\n**Connection Failed:**\n- Verify `ZABBIX_URL` is correct and accessible\n- Check authentication credentials\n- Ensure Zabbix API is enabled\n\n**Permission Denied:**\n- Verify user has sufficient Zabbix permissions\n- Check if read-only mode is enabled when trying to modify data\n\n**Tool Not Found:**\n- Ensure all dependencies are installed: `uv sync`\n- Verify Python version compatibility (3.10+)\n\n### Debug Mode\n\nSet environment variable for detailed logging:\n```bash\nexport DEBUG=1\nuv run python scripts/start_server.py\n```\n\n## Dependencies\n\n- [FastMCP](https://github.com/jlowin/fastmcp) - MCP server framework\n- [python-zabbix-utils](https://github.com/zabbix/python-zabbix-utils) - Official Zabbix Python library\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- [Zabbix](https://www.zabbix.com/) for the monitoring platform\n- [Model Context Protocol](https://modelcontextprotocol.io/) for the integration standard\n- [FastMCP](https://github.com/jlowin/fastmcp) for the server framework\n\n## Support\n\n- üìñ [Documentation](README.md)\n- üêõ [Issue Tracker](https://github.com/mpeirone/zabbix-mcp-server/issues)\n- üí¨ [Discussions](https://github.com/mpeirone/zabbix-mcp-server/discussions)\n\n---\n\n**Made with ‚ù§Ô∏è for the Zabbix and MCP communities**",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "zabbix",
        "ai",
        "server zabbix",
        "application monitoring",
        "zabbix integration"
      ],
      "category": "monitoring"
    },
    "netdata--netdata#Netdata": {
      "owner": "netdata",
      "name": "netdata#Netdata",
      "url": "https://github.com/netdata/netdata/blob/master/src/web/mcp/README.md",
      "imageUrl": "",
      "description": "Discovery, exploration, reporting and root cause analysis using all observability data, including metrics, logs, systems, containers, processes, and network connections",
      "stars": 76293,
      "forks": 6193,
      "license": "GNU General Public License v3.0",
      "language": "C",
      "updated_at": "2025-10-04T07:00:19Z",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "netdata",
        "monitoring",
        "ai",
        "netdata discovery",
        "netdata netdata",
        "application monitoring"
      ],
      "category": "monitoring"
    },
    "seekrays--mcp-monitor": {
      "owner": "seekrays",
      "name": "mcp-monitor",
      "url": "https://github.com/seekrays/mcp-monitor",
      "imageUrl": "",
      "description": "A system monitoring tool that exposes system metrics via the Model Context Protocol (MCP). This tool allows LLMs to retrieve real-time system information through an MCP-compatible interface.Ôºàsupport CPU„ÄÅMemory„ÄÅDisk„ÄÅNetwork„ÄÅHost„ÄÅProcessÔºâ",
      "stars": 71,
      "forks": 15,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-10-02T04:26:49Z",
      "readme_content": "# MCP System Monitor\n![Go](https://github.com/seekrays/mcp-monitor/actions/workflows/go.yml/badge.svg)\n![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/seekrays/mcp-monitor?sort=semver)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-blue?style=flat&logo=discord)](https://discord.gg/kbMJ9Qpf)\n\nA system monitoring tool that exposes system metrics via the Model Context Protocol (MCP). This tool allows LLMs to retrieve real-time system information through an MCP-compatible interface.\n\n![](./doc/snapshot-1.png)\n\n## Features\n\nThis tool provides the following monitoring capabilities:\n\n- **CPU Information**: Usage percentage, core count, and detailed CPU info\n- **Memory Information**: Virtual and swap memory usage\n- **Disk Information**: Disk usage, partitions, and I/O statistics\n- **Network Information**: Network interfaces, connections, and traffic statistics\n- **Host Information**: System details, uptime, boot time, and users\n- **Process Information**: Process listing, sorting, and detailed per-process statistics\n\n\n## Available Tools\n\n### 1. CPU Information\n\n```\nTool: get_cpu_info\nDescription: Get CPU information and usage\nParameters:\n  - per_cpu (boolean, default: false): Whether to return data for each core\n```\n\n### 2. Memory Information\n\n```\nTool: get_memory_info\nDescription: Get system memory usage information\nParameters: None\n```\n\n### 3. Disk Information\n\n```\nTool: get_disk_info\nDescription: Get disk usage information\nParameters:\n  - path (string, default: \"/\"): Specify the disk path to query\n  - all_partitions (boolean, default: false): Whether to return information for all partitions\n```\n\n### 4. Network Information\n\n```\nTool: get_network_info\nDescription: Get network interface and traffic information\nParameters:\n  - interface (string, optional): Specify the network interface name to query\n```\n\n### 5. Host Information\n\n```\nTool: get_host_info\nDescription: Get host system information\nParameters: None\n```\n\n### 6. Process Information\n\n```\nTool: get_process_info\nDescription: Get process information\nParameters:\n  - pid (number, optional): Process ID to get detailed information for a specific process\n  - limit (number, default: 10): Limit the number of processes returned\n  - sort_by (string, default: \"cpu\"): Sort field (cpu, memory, pid, name)\n```\n\n\n## Installation\n\n```bash\ngit clone https://github.com/seekrays/mcp-monitor.git\ncd mcp-monitor\nmake build\n```\n\n## Usage\n\nRun the compiled binary:\n\n```bash\n./mcp-monitor\n```\n\nThe server starts in stdio mode, ready to communicate with an MCP-compatible LLM client.\n\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "ai",
        "metrics",
        "application monitoring",
        "monitoring tool",
        "monitoring access"
      ],
      "category": "monitoring"
    },
    "tumf--grafana-loki-mcp": {
      "owner": "tumf",
      "name": "grafana-loki-mcp",
      "url": "https://github.com/tumf/grafana-loki-mcp",
      "imageUrl": "",
      "description": "An MCP server that allows querying Loki logs through the Grafana API.",
      "stars": 17,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T05:23:29Z",
      "readme_content": "# Grafana-Loki MCP Server\n\n[![Test](https://github.com/tumf/grafana-loki-mcp/actions/workflows/test.yml/badge.svg)](https://github.com/tumf/grafana-loki-mcp/actions/workflows/test.yml)\n[![PyPI version](https://badge.fury.io/py/grafana-loki-mcp.svg)](https://badge.fury.io/py/grafana-loki-mcp)\n[![codecov](https://codecov.io/gh/tumf/grafana-loki-mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tumf/grafana-loki-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA [FastMCP](https://github.com/jlowin/fastmcp) server that allows querying Loki logs from Grafana.\n\n## MCP Server Settings\n\n```json\n{\n  \"mcpServers\": {\n    \"loki\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"grafana-loki-mcp\",\n        \"-u\",\n        \"GRAFANA_URL\",\n        \"-k\",\n        \"GRAFANA_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\n- `GRAFANA_URL`: URL of your Grafana instance\n- `GRAFANA_API_KEY`: Grafana API key with appropriate permissions\n\n## Features\n\n- Query Loki logs through Grafana API\n- Get Loki labels and label values\n- Format query results in different formats (text, JSON, markdown)\n- Support for both stdio and SSE transport protocols\n\n## Requirements\n\n- Python 3.10+\n- FastMCP\n- Requests\n\n## Installation\n\n### Using pip\n\n```bash\npip install grafana-loki-mcp\n```\n\n### Development Setup\n\n1. Clone this repository\n2. Install dependencies using uv:\n\n```bash\n# Install uv\npip install uv\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\nuv pip install -e \".[dev]\"\n```\n\n## Usage\n\n### Environment Variables\n\nSet the following environment variables:\n\n- `GRAFANA_URL`: URL of your Grafana instance\n- `GRAFANA_API_KEY`: Grafana API key with appropriate permissions\n\n### Command Line Arguments\n\nYou can also provide these values as command line arguments:\n\n```bash\ngrafana-loki-mcp -u https://your-grafana-instance.com -k your-api-key\n```\n\nAdditional options:\n- `--transport`: Transport protocol to use (`stdio` or `sse`, default: `stdio`)\n\n### Running the Server\n\n```bash\n# Using environment variables\nexport GRAFANA_URL=https://your-grafana-instance.com\nexport GRAFANA_API_KEY=your-api-key\ngrafana-loki-mcp\n\n# Using command line arguments\ngrafana-loki-mcp -u https://your-grafana-instance.com -k your-api-key\n\n# Using SSE transport\ngrafana-loki-mcp --transport sse\n```\n\n## Development\n\n### Testing\n\nRun the test suite:\n\n```bash\npytest\n```\n\nRun with coverage:\n\n```bash\npytest --cov=. --cov-report=term\n```\n\n### Linting and Formatting\n\n```bash\n# Run ruff linter\nruff check .\n\n# Run black formatter\nblack .\n\n# Run type checking\nmypy .\n```\n\n## Available Tools\n\n### query_loki\n\nQuery Loki logs through Grafana.\n\nParameters:\n- `query`: Loki query string\n- `start`: Start time (ISO format, Unix timestamp, or Grafana-style relative time like 'now-1h', default: 1 hour ago)\n- `end`: End time (ISO format, Unix timestamp, or Grafana-style relative time like 'now', default: now)\n- `limit`: Maximum number of log lines to return (default: 100)\n- `direction`: Query direction ('forward' or 'backward', default: 'backward')\n- `max_per_line`: Maximum characters per log line (0 for unlimited, default: 100)\n\n### get_loki_labels\n\nGet all label names from Loki.\n\n### get_loki_label_values\n\nGet values for a specific label from Loki.\n\nParameters:\n- `label`: Label name\n\n### format_loki_results\n\nFormat Loki query results in a more readable format.\n\nParameters:\n- `results`: Loki query results from query_loki\n- `format_type`: Output format ('text', 'json', or 'markdown', default: 'text')\n- `max_per_line`: Maximum characters per log line (0 for unlimited, default: 0)\n\n## Example Usage\n\n```python\n# Example client code\nfrom mcp.client import Client\n\nasync with Client() as client:\n    # Query Loki logs with max_per_line limit\n    results = await client.call_tool(\n        \"query_loki\",\n        {\n            \"query\": '{app=\"my-app\"} |= \"error\"',\n            \"limit\": 50,\n            \"max_per_line\": 100,  # Limit log lines to 100 characters\n            \"start\": \"now-6h\",    # Grafana-style relative time: 6 hours ago\n            \"end\": \"now\"          # Current time\n        }\n    )\n\n    # Format the results\n    formatted = await client.call_tool(\n        \"format_loki_results\",\n        {\n            \"results\": results,\n            \"format_type\": \"markdown\",\n            \"max_per_line\": 100  # Can also limit at formatting time\n        }\n    )\n\n    print(formatted)\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "api",
        "logs",
        "application monitoring",
        "grafana api",
        "monitoring access"
      ],
      "category": "monitoring"
    },
    "yshngg--pmcp": {
      "owner": "yshngg",
      "name": "pmcp",
      "url": "https://github.com/yshngg/pmcp",
      "imageUrl": "",
      "description": "A Prometheus Model Context Protocol Server.",
      "stars": 3,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-09-30T01:59:52Z",
      "readme_content": "# PMCP - Prometheus Model Context Protocol Server\n\n[![codecov](https://codecov.io/gh/yshngg/pmcp/graph/badge.svg?token=C64XY9GFP3)](https://codecov.io/gh/yshngg/pmcp)\n[![Go Report Card](https://goreportcard.com/badge/github.com/yshngg/pmcp)](https://goreportcard.com/report/github.com/yshngg/pmcp)\n\n**üöÄ A Golang-based Model Context Protocol (MCP) server implementation for Prometheus that enables natural language interactions with Prometheus metrics and queries.**\n\n**Built with Go**, PMCP provides a robust, type-safe interface that maintains full consistency with the Prometheus HTTP API, allowing you to query and manage your Prometheus instance through natural language conversations with MCP-compatible clients.\n\n---\n\n## Table of Contents\n\n1. [Features](#features)\n2. [Architecture](#architecture)\n3. [Requirements](#requirements)\n4. [Installation](#installation)\n5. [Usage](#usage)\n   * [Command Line Flags](#command-line-flags)\n6. [API Compatibility](#api-compatibility)\n7. [Binding Blocks](#binding-blocks)\n   * [Tools](#tools)\n   * [Prompts](#prompts)\n8. [Contributing](#contributing)\n9. [License](#license)\n10. [Acknowledgments](#acknowledgments)\n\n---\n\n## Features\n\n* **üî• Golang Implementation**: Built with Go 1.23+ for performance, reliability, and type safety\n* **üìä Complete Prometheus API Coverage**: Full compatibility with Prometheus HTTP API v1\n* **‚ö° Instant Query**: Execute Prometheus queries at a specific point in time\n* **üìà Range Query**: Retrieve historical metric data over defined time ranges\n* **üîç Metadata Query**: Discover time series, label names, and label values\n* **üéØ Target & Rule Management**: Monitor targets, rules, and alerting configurations\n* **üõ†Ô∏è TSDB Administration**: Advanced database operations including snapshots and series deletion\n* **üåê Multiple Transport Options**: Support for HTTP, Server-Sent Events (SSE), and stdio\n* **ü§ñ MCP Integration**: Seamless communication with MCP-compatible clients like Claude Desktop\n\n---\n\n## Architecture\n\nPMCP is designed as a **Golang microservice** that acts as a bridge between MCP clients and Prometheus servers. It provides:\n\n* **Type-safe API bindings** using Go structs that mirror Prometheus API responses\n* **Modular package structure** for maintainability and extensibility  \n* **Comprehensive error handling** with proper Go error propagation\n* **Clean separation of concerns** between transport, API client, and business logic\n\n---\n\n## Requirements\n\n* **Go 1.23.0** or higher\n* A running **Prometheus server** (v2.x)\n* Compatible MCP client (Claude Desktop, custom implementations, etc.)\n\n---\n\n## Installation\n\n### Using Docker (Recommended)\n\nPull the pre-built image from GitHub Container Registry:\n\n```bash\n# Pull the latest image\ndocker pull ghcr.io/yshngg/pmcp:latest\n\n# Run with stdio transport (for desktop clients)\ndocker run --rm ghcr.io/yshngg/pmcp:latest --prom-addr=\"http://host.docker.internal:9090\"\n\n# Run with HTTP transport\ndocker run --rm -p 8080:8080 ghcr.io/yshngg/pmcp:latest --prom-addr=\"http://host.docker.internal:9090\" --transport=http --mcp-addr=\"0.0.0.0:8080\"\n```\n\nAlternatively, build locally:\n\n```bash\ndocker build -t pmcp .\ndocker run -p 8080:8080 pmcp --prom-addr=\"http://prometheus:9090\" --transport=http\n```\n\n### Download Pre-built Binary\n\nDownload the latest release from GitHub:\n\n1. Go to [PMCP Releases](https://github.com/yshngg/pmcp/releases)\n2. Download the appropriate binary for your platform from the **Assets** section\n3. Extract and run:\n\n```bash\n# Linux/macOS example\ntar -xzf pmcp-<version>.linux-amd64.tar.gz\n./pmcp --prom-addr=\"http://localhost:9090\"\n\n# Windows example\nunzip pmcp-<version>.windows-amd64.zip\npmcp.exe --prom-addr=\"http://localhost:9090\"\n```\n\n### Building from Source\n\n```bash\ngit clone https://github.com/yshngg/pmcp.git\ncd pmcp\nmake build\n# Binary will be available as ./pmcp\n```\n\n### Using Go Install\n\nInstall the `pmcp` binary directly from source:\n\n```bash\ngo install github.com/yshngg/pmcp@latest\n```\n\nEnsure `$GOPATH/bin` is in your `$PATH`.\n\n---\n\n## Usage\n\nRun the server by specifying your Prometheus address and preferred transport:\n\n```bash\n# Default (stdio transport) - ideal for desktop clients\npmcp --prom-addr=\"http://localhost:9090\"\n\n# HTTP transport - for web-based integrations\npmcp --prom-addr=\"http://localhost:9090\" --transport=http --mcp-addr=\"localhost:8080\"\n\n# SSE transport - for real-time streaming (deprecated, use HTTP)\npmcp --prom-addr=\"http://localhost:9090\" --transport=sse --mcp-addr=\"localhost:8080\"\n```\n\n### Command Line Flags\n\n| Flag         | Description                                       | Default                 |\n| ------------ | ------------------------------------------------- | ----------------------- |\n| `-help`      | Show help information.                            | N/A                     |\n| `-mcp-addr`  | Address for the MCP server to listen on.          | `localhost:8080`        |\n| `-prom-addr` | Prometheus server URL.                            | `http://localhost:9090` |\n| `-transport` | Communication transport (`stdio`, `http`, `sse`). | `stdio`                 |\n| `-version`   | Print version and exit.                           | N/A                     |\n\n---\n\n## API Compatibility\n\nPMCP maintains **100% compatibility** with the Prometheus HTTP API v1. Every tool and endpoint corresponds directly to the official Prometheus API:\n\n### Query & Data Retrieval\n\n| PMCP Tool | Prometheus Endpoint | HTTP Method | Purpose |\n|-----------|-------------------|-------------|---------|\n| Instant Query | `/api/v1/query` | GET/POST | Execute instant queries |\n| Range Query | `/api/v1/query_range` | GET/POST | Execute range queries |\n\n### Metadata & Discovery\n\n| PMCP Tool | Prometheus Endpoint | HTTP Method | Purpose |\n|-----------|-------------------|-------------|---------|\n| Find Series by Labels | `/api/v1/series` | GET/POST | Find matching time series |\n| List Label Names | `/api/v1/labels` | GET/POST | List all label names |\n| List Label Values | `/api/v1/label/:name/values` | GET | List values for a specific label |\n| Target Discovery | `/api/v1/targets` | GET | Get target information |\n| Target Metadata Query | `/api/v1/targets/metadata` | GET | Get metadata from targets |\n| Metric Metadata Query | `/api/v1/metadata` | GET | Get metric metadata |\n\n### Rules & Alerts\n\n| PMCP Tool | Prometheus Endpoint | HTTP Method | Purpose |\n|-----------|-------------------|-------------|---------|\n| Alert Query | `/api/v1/alerts` | GET | Get all active alerts |\n| Rule Query | `/api/v1/rules` | GET | Get recording/alerting rules |\n| Alertmanager Discovery | `/api/v1/alertmanagers` | GET | Get alertmanager information |\n\n### Status & Configuration\n\n| PMCP Tool | Prometheus Endpoint | HTTP Method | Purpose |\n|-----------|-------------------|-------------|---------|\n| Config | `/api/v1/status/config` | GET | Get current configuration |\n| Flags | `/api/v1/status/flags` | GET | Get runtime flags |\n| Build Information | `/api/v1/status/buildinfo` | GET | Get build information |\n| Runtime Information | `/api/v1/status/runtimeinfo` | GET | Get runtime information |\n| TSDB Stats | `/api/v1/status/tsdb` | GET | Get TSDB statistics |\n| WAL Replay Stats | `/api/v1/status/walreplay` | GET | Get WAL replay status |\n\n### TSDB Administration\n\n| PMCP Tool | Prometheus Endpoint | HTTP Method | Purpose |\n|-----------|-------------------|-------------|---------|\n| TSDB Snapshot | `/api/v1/admin/tsdb/snapshot` | POST/PUT | Create TSDB snapshot |\n| Delete Series | `/api/v1/admin/tsdb/delete_series` | POST/PUT | Delete time series data |\n| Clean Tombstones | `/api/v1/admin/tsdb/clean_tombstones` | POST/PUT | Clean deleted data |\n\n### Management APIs\n\n| PMCP Tool | Prometheus Endpoint | HTTP Method | Purpose |\n|-----------|-------------------|-------------|---------|\n| Health Check | `/-/healthy` | GET/HEAD | Check Prometheus health |\n| Readiness Check | `/-/ready` | GET/HEAD | Check if ready to serve |\n| Reload | `/-/reload` | PUT/POST | Reload configuration |\n| Quit | `/-/quit` | PUT/POST | Graceful shutdown |\n\n**All query parameters, response formats, and error codes match the official Prometheus API specification.**\n\n---\n\n## Binding Blocks\n\n### Tools\n\n**Expression Queries** (Core Prometheus functionality):\n\n* **Instant Query**: Evaluate an instant query at a single point in time\n* **Range Query**: Evaluate an expression query over a range of time\n\n**Metadata Queries** (Series and label discovery):\n\n* **Find Series by Labels**: Return the list of time series that match a certain label set\n* **List Label Names**: Return a list of label names\n* **List Label Values**: Return a list of label values for a provided label name\n* **Target Metadata Query**: Return metadata about metrics currently scraped from targets\n* **Metric Metadata Query**: Return metadata about metrics currently scraped from targets (without target information)\n\n**Discovery & Monitoring**:\n\n* **Target Discovery**: Return an overview of the current state of the Prometheus target discovery\n* **Alert Query**: Return a list of all active alerts\n* **Rule Query**: Return a list of alerting and recording rules that are currently loaded\n* **Alertmanager Discovery**: Return an overview of the current state of the Prometheus alertmanager discovery\n\n**Status & Configuration**:\n\n* **Config**: Return currently loaded configuration file\n* **Flags**: Return flag values that Prometheus was configured with\n* **Runtime Information**: Return various runtime information properties about the Prometheus server\n* **Build Information**: Return various build information properties about the Prometheus server\n* **TSDB Stats**: Return various cardinality statistics about the Prometheus TSDB\n* **WAL Replay Stats**: Return information about the WAL replay\n\n**TSDB Admin APIs** (Advanced operations):\n\n* **TSDB Snapshot**: Create a snapshot of all current data into snapshots/`<datetime>`-`<rand>`\n* **Delete Series**: Delete data for a selection of series in a time range\n* **Clean Tombstones**: Remove the deleted data from disk and cleans up the existing tombstones\n\n**Management APIs**:\n\n* **Health Check**: Check Prometheus health\n* **Readiness Check**: Check if Prometheus is ready to serve traffic (i.e. respond to queries)\n* **Reload**: Trigger a reload of the Prometheus configuration and rule files\n* **Quit**: Trigger a graceful shutdown of Prometheus\n\n### Prompts\n\n* **All Available Metrics**: Return a list of every metric exposed by the Prometheus instance\n\n---\n\n## Contributing\n\nContributions are welcome! This is a **Golang project**, so please ensure:\n\n* Follow Go best practices and conventions\n* Add appropriate tests for new functionality\n* Maintain API compatibility with Prometheus\n* Update documentation as needed\n\nPlease submit a pull request or open an issue to discuss improvements.\n\n### Development Setup\n\n```bash\ngit clone https://github.com/yshngg/pmcp.git\ncd pmcp\ngo mod download\nmake build\n```\n\n---\n\n## License\n\nThis project is licensed under the Apache License 2.0. See the [LICENSE](LICENSE) file for details.\n\n---\n\n## Acknowledgments\n\n* **Built with Go** using the official [Prometheus Go client library](https://github.com/prometheus/client_golang)\n* Powered by [Model Context Protocol Go SDK](https://github.com/modelcontextprotocol/go-sdk)\n* Inspired by [Prometheus](https://prometheus.io/) - the de facto standard for monitoring and alerting\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "monitoring",
        "pmcp",
        "protocol",
        "pmcp prometheus",
        "application monitoring",
        "monitoring access"
      ],
      "category": "monitoring"
    }
  }
}